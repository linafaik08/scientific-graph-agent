{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a003e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDemo notebook for Scientific Graph Agent.\\n\\nThis script demonstrates:\\n1. Basic usage of the agent graph\\n2. Memory functionality across multiple queries\\n3. Graph visualization\\n4. How the conditional loop works\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Demo notebook for Scientific Graph Agent.\n",
    "\n",
    "This script demonstrates:\n",
    "1. Basic usage of the agent graph\n",
    "2. Memory functionality across multiple queries\n",
    "3. Graph visualization\n",
    "4. How the conditional loop works\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7de14f",
   "metadata": {},
   "source": [
    "# üß™ Scientific Graph Agent - Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e943173",
   "metadata": {},
   "source": [
    "This notebook demonstrates a simple agent graph for scientific paper exploration using LangGraph + ArXiv API with memory support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e2a928a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ced08c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL = \"gpt-5-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa1950f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables loaded\n"
     ]
    }
   ],
   "source": [
    "# Verify that API keys are set\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"‚ö†Ô∏è  OPENAI_API_KEY not found in .env file\")\n",
    "\n",
    "print(\"‚úÖ Environment variables loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be7942fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/linafaik/Documents/projects\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc756288",
   "metadata": {},
   "source": [
    "## Graph Agent Initialization\n",
    "\n",
    "The shared state has been refactored into `InputState`, `OutputState`, and `InternalState` (see `src/agent_graph/state.py`).\n",
    "We now import the graph constructor directly to reflect that structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e7e4d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Graph compiled with memory enabled\n",
      "‚úÖ Agent graph created\n"
     ]
    }
   ],
   "source": [
    "from src.agent_graph.graph import create_graph\n",
    "\n",
    "# Create graph with custom parameters (state handled via InternalState reducers)\n",
    "graph = create_graph(with_checkpointer=True)\n",
    "\n",
    "print(\"‚úÖ Agent graph created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a01e19",
   "metadata": {},
   "source": [
    "### Graph Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1e2eae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph structure:\n",
      "START ‚Üí clarifier ‚Üí researcher ‚Üí summarizer ‚Üí END\n",
      "           ‚Üë            ‚Üì\n",
      "           ‚îî‚îÄ(if <3 papers)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "    \n",
    "# Generate graph visualization as PNG\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "print(\"Graph structure:\")\n",
    "print(\"START ‚Üí clarifier ‚Üí researcher ‚Üí summarizer ‚Üí END\")\n",
    "print(\"           ‚Üë            ‚Üì\")\n",
    "print(\"           ‚îî‚îÄ(if <3 papers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1090516",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce34676",
   "metadata": {},
   "source": [
    "### Test 1: Simple Research Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "193d6be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 21:29:02,736 - INFO - üéØ Clarifying query: 'What are the key innovations in transformer architectures?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting agent graph execution...\n",
      "\n",
      "state\n",
      "{'query': 'What are the key innovations in transformer architectures?', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2, 'papers': [], 'messages': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 21:29:07,801 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-02 21:29:07,815 - INFO - ‚ú® Refined query: 'transformer architecture innovations attention variants positional encodings'\n",
      "2025-11-02 21:29:07,819 - INFO - Searching ArXiv: 'transformer architecture innovations attention variants positional encodings' (iteration 0)\n",
      "2025-11-02 21:29:08,827 - INFO - Found 5 papers\n",
      "2025-11-02 21:29:08,828 - INFO - Scoring paper relevance...\n",
      "2025-11-02 21:29:12,333 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-02 21:29:12,340 - INFO -   üìÑ AttentionSmithy: A Modular Framework for Rapid Transformer D... - Score: 60\n",
      "2025-11-02 21:29:16,708 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-02 21:29:16,723 - INFO -   üìÑ Stable, Fast and Accurate: Kernelized Attention with Relativ... - Score: 65\n",
      "2025-11-02 21:29:21,520 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-02 21:29:21,538 - INFO -   üìÑ Context-aware Rotary Position Embedding... - Score: 45\n",
      "2025-11-02 21:29:25,308 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-02 21:29:25,316 - INFO -   üìÑ Positional encoding is not the same as context: A study on p... - Score: 30\n",
      "2025-11-02 21:29:30,633 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-02 21:29:30,640 - INFO -   üìÑ Revisiting Transformers with Insights from Image Filtering... - Score: 55\n",
      "2025-11-02 21:29:30,645 - INFO - üìù Synthesizing 5 papers...\n",
      "2025-11-02 21:29:47,030 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-02 21:29:47,064 - INFO - ‚úÖ Summary generated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary': '## R√©sum√©\\n‚Ä¢ Kernelized and linearized attention improve scalability by approximating the softmax-based attention kernel, trading exactness for sub-quadratic or linear complexity while retaining accuracy; combining these kernels with relative positional encoding helps keep long-range structure without quadratic cost [Paper 1].  \\n‚Ä¢ Positional encodings remain central but are evolving: Context-aware extensions to Rotary Positional Embeddings introduce input-dependent frequency patterns to better capture local sequence context and reduce the mismatch between static encodings and dynamic inputs [Paper 4].  \\n‚Ä¢ New theoretical lenses (e.g., image-filtering perspectives) make attention more interpretable by showing when self-attention behaves like learned filtering versus when it provides distinct global interactions, guiding principled architectural modifications and better inductive biases [Paper 3].  \\n‚Ä¢ Engineering and adoption accelerate when transformer components are modular: frameworks that decompose attention, feed‚Äëforward, normalization and positional modules lower the barrier for domain-driven innovation and rapid experimentation [Paper 2].  \\n‚Ä¢ Task-specific studies warn against treating positional encoding as equivalent to contextual information; empirical work in sequential recommendation shows common encodings can misrepresent temporal dynamics, motivating task-aware encoding design or alternative conditioning mechanisms [Paper 5].  \\n\\n## R√©f√©rences\\n[Paper 1] Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding - Shengjie Luo, Shanda Li, Tianle Cai (2021) - http://arxiv.org/abs/2106.12566v2\\n\\n[Paper 2] AttentionSmithy: A Modular Framework for Rapid Transformer Development and Customization - Caleb Cranney, Jesse G. Meyer (2025) - http://arxiv.org/abs/2502.09503v2\\n\\n[Paper 3] Revisiting Transformers with Insights from Image Filtering - Laziz U. Abdullaev, Maksim Tkachenko, Tan M. Nguyen (2025) - http://arxiv.org/abs/2506.10371v1\\n\\n[Paper 4] Context-aware Rotary Position Embedding - Ali Veisi, Delaram Fartoot, Hamidreza Amirzadeh (2025) - http://arxiv.org/abs/2507.23083v1\\n\\n[Paper 5] Positional encoding is not the same as context: A study on positional encoding for sequential recommendation - Alejo Lopez-Avila, Jinhua Du, Abbas Shimary (2024) - http://arxiv.org/abs/2405.10436v2',\n",
       " 'papers': [{'id': 'http://arxiv.org/abs/2106.12566v2',\n",
       "   'title': 'Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding',\n",
       "   'authors': ['Shengjie Luo',\n",
       "    'Shanda Li',\n",
       "    'Tianle Cai',\n",
       "    'Di He',\n",
       "    'Dinglan Peng',\n",
       "    'Shuxin Zheng',\n",
       "    'Guolin Ke',\n",
       "    'Liwei Wang',\n",
       "    'Tie-Yan Liu'],\n",
       "   'summary': 'The attention module, which is a crucial component in Transformer, cannot\\nscale efficiently to long sequences due to its quadratic complexity. Many works\\nfocus on approximating the dot-then-exponentiate softmax function in the\\noriginal attention, leading to sub-quadratic or even linear-complexity\\nTransformer architectures. However, we show that these methods cannot be\\napplied to more powerful attention modules that go beyond the\\ndot-then-exponentiate style, e.g., Transformers with relative positional\\nencoding (RPE). Since in many state-of-the-art models, relative positional\\nencoding is used as default, designing efficient Transformers that can\\nincorporate RPE is appealing. In this paper, we propose a novel way to\\naccelerate attention calculation for Transformers with RPE on top of the\\nkernelized attention. Based upon the observation that relative positional\\nencoding forms a Toeplitz matrix, we mathematically show that kernelized\\nattention with RPE can be calculated efficiently using Fast Fourier Transform\\n(FFT). With FFT, our method achieves $\\\\mathcal{O}(n\\\\log n)$ time complexity.\\nInterestingly, we further demonstrate that properly using relative positional\\nencoding can mitigate the training instability problem of vanilla kernelized\\nattention. On a wide range of tasks, we empirically show that our models can be\\ntrained from scratch without any optimization issues. The learned model\\nperforms better than many efficient Transformer variants and is faster than\\nstandard Transformer in the long-sequence regime.',\n",
       "   'url': 'http://arxiv.org/abs/2106.12566v2',\n",
       "   'published': '2021-06-23',\n",
       "   'relevance_score': 65},\n",
       "  {'id': 'http://arxiv.org/abs/2502.09503v2',\n",
       "   'title': 'AttentionSmithy: A Modular Framework for Rapid Transformer Development and Customization',\n",
       "   'authors': ['Caleb Cranney', 'Jesse G. Meyer'],\n",
       "   'summary': \"Transformer architectures have transformed AI applications but remain complex\\nto customize for domain experts lacking low-level implementation expertise. We\\nintroduce AttentionSmithy, a modular software package that simplifies\\ntransformer innovation by breaking down key components into reusable building\\nblocks: attention modules, feed-forward networks, normalization layers, and\\npositional encodings. Users can rapidly prototype and evaluate transformer\\nvariants without extensive coding. Our framework supports four positional\\nencoding strategies and integrates with neural architecture search for\\nautomated design. We validate AttentionSmithy by replicating the original\\ntransformer under resource constraints and optimizing translation performance\\nby combining positional encodings. Additionally, we demonstrate its\\nadaptability in gene-specific modeling, achieving over 95% accuracy in cell\\ntype classification. These case studies highlight AttentionSmithy's potential\\nto accelerate research across diverse fields by removing framework\\nimplementation barriers.\",\n",
       "   'url': 'http://arxiv.org/abs/2502.09503v2',\n",
       "   'published': '2025-02-13',\n",
       "   'relevance_score': 60},\n",
       "  {'id': 'http://arxiv.org/abs/2506.10371v1',\n",
       "   'title': 'Revisiting Transformers with Insights from Image Filtering',\n",
       "   'authors': ['Laziz U. Abdullaev', 'Maksim Tkachenko', 'Tan M. Nguyen'],\n",
       "   'summary': 'The self-attention mechanism, a cornerstone of Transformer-based\\nstate-of-the-art deep learning architectures, is largely heuristic-driven and\\nfundamentally challenging to interpret. Establishing a robust theoretical\\nfoundation to explain its remarkable success and limitations has therefore\\nbecome an increasingly prominent focus in recent research. Some notable\\ndirections have explored understanding self-attention through the lens of image\\ndenoising and nonparametric regression. While promising, existing frameworks\\nstill lack a deeper mechanistic interpretation of various architectural\\ncomponents that enhance self-attention, both in its original formulation and\\nsubsequent variants. In this work, we aim to advance this understanding by\\ndeveloping a unifying image processing framework, capable of explaining not\\nonly the self-attention computation itself but also the role of components such\\nas positional encoding and residual connections, including numerous later\\nvariants. We also pinpoint potential distinctions between the two concepts\\nbuilding upon our framework, and make effort to close this gap. We introduce\\ntwo independent architectural modifications within transformers. While our\\nprimary objective is interpretability, we empirically observe that image\\nprocessing-inspired modifications can also lead to notably improved accuracy\\nand robustness against data contamination and adversaries across language and\\nvision tasks as well as better long sequence understanding.',\n",
       "   'url': 'http://arxiv.org/abs/2506.10371v1',\n",
       "   'published': '2025-06-12',\n",
       "   'relevance_score': 55},\n",
       "  {'id': 'http://arxiv.org/abs/2507.23083v1',\n",
       "   'title': 'Context-aware Rotary Position Embedding',\n",
       "   'authors': ['Ali Veisi', 'Delaram Fartoot', 'Hamidreza Amirzadeh'],\n",
       "   'summary': 'Positional encoding is a vital component of Transformer architectures,\\nenabling models to incorporate sequence order into self-attention mechanisms.\\nRotary Positional Embeddings (RoPE) have become a widely adopted solution due\\nto their compatibility with relative position encoding and computational\\nefficiency. However, RoPE relies on static, input-independent sinusoidal\\nfrequency patterns, limiting its ability to model context-sensitive\\nrelationships. In this work, we propose CARoPE (Context-Aware Rotary Positional\\nEmbedding), a novel generalization of RoPE that dynamically generates\\nhead-specific frequency patterns conditioned on token embeddings. This design\\nintroduces token- and context-sensitive positional representations while\\npreserving RoPE efficiency and architectural simplicity. CARoPE computes\\ninput-dependent phase shifts using a bounded transformation of token embeddings\\nand integrates them into the rotary mechanism across attention heads. We\\nevaluate CARoPE on the FineWeb-Edu-10B dataset using GPT-2 variants trained on\\nnext-token prediction tasks. Experimental results show that CARoPE consistently\\noutperforms RoPE and other common positional encoding baselines, achieving\\nsignificantly lower perplexity, even at longer context lengths. Additionally,\\nCARoPE enables faster training throughput without sacrificing model stability.\\nThese findings demonstrate that CARoPE offers a scalable, expressive, and\\nefficient upgrade to existing positional encoding strategies in Transformer\\nmodels.',\n",
       "   'url': 'http://arxiv.org/abs/2507.23083v1',\n",
       "   'published': '2025-07-30',\n",
       "   'relevance_score': 45},\n",
       "  {'id': 'http://arxiv.org/abs/2405.10436v2',\n",
       "   'title': 'Positional encoding is not the same as context: A study on positional encoding for sequential recommendation',\n",
       "   'authors': ['Alejo Lopez-Avila', 'Jinhua Du', 'Abbas Shimary', 'Ze Li'],\n",
       "   'summary': \"The rapid growth of streaming media and e-commerce has driven advancements in\\nrecommendation systems, particularly Sequential Recommendation Systems (SRS).\\nThese systems employ users' interaction histories to predict future\\npreferences. While recent research has focused on architectural innovations\\nlike transformer blocks and feature extraction, positional encodings, crucial\\nfor capturing temporal patterns, have received less attention. These encodings\\nare often conflated with contextual, such as the temporal footprint, which\\nprevious works tend to treat as interchangeable with positional information.\\nThis paper highlights the critical distinction between temporal footprint and\\npositional encodings, demonstrating that the latter offers unique relational\\ncues between items, which the temporal footprint alone cannot provide. Through\\nextensive experimentation on eight Amazon datasets and subsets, we assess the\\nimpact of various encodings on performance metrics and training stability. We\\nintroduce new positional encodings and investigate integration strategies that\\nimprove both metrics and stability, surpassing state-of-the-art results at the\\ntime of this work's initial preprint. Importantly, we demonstrate that\\nselecting the appropriate encoding is not only key to better performance but\\nalso essential for building robust, reliable SRS models.\",\n",
       "   'url': 'http://arxiv.org/abs/2405.10436v2',\n",
       "   'published': '2024-05-16',\n",
       "   'relevance_score': 30}],\n",
       " 'messages': [SystemMessage(content='User inquired about the key innovations in transformer architectures, which suggests an interest in advancements in this field.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Refined query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Clarifier'),\n",
       "  AIMessage(content='Found 5 papers on ArXiv for query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Researcher'),\n",
       "  AIMessage(content='## R√©sum√©\\n‚Ä¢ Kernelized and linearized attention improve scalability by approximating the softmax-based attention kernel, trading exactness for sub-quadratic or linear complexity while retaining accuracy; combining these kernels with relative positional encoding helps keep long-range structure without quadratic cost [Paper 1].  \\n‚Ä¢ Positional encodings remain central but are evolving: Context-aware extensions to Rotary Positional Embeddings introduce input-dependent frequency patterns to better capture local sequence context and reduce the mismatch between static encodings and dynamic inputs [Paper 4].  \\n‚Ä¢ New theoretical lenses (e.g., image-filtering perspectives) make attention more interpretable by showing when self-attention behaves like learned filtering versus when it provides distinct global interactions, guiding principled architectural modifications and better inductive biases [Paper 3].  \\n‚Ä¢ Engineering and adoption accelerate when transformer components are modular: frameworks that decompose attention, feed‚Äëforward, normalization and positional modules lower the barrier for domain-driven innovation and rapid experimentation [Paper 2].  \\n‚Ä¢ Task-specific studies warn against treating positional encoding as equivalent to contextual information; empirical work in sequential recommendation shows common encodings can misrepresent temporal dynamics, motivating task-aware encoding design or alternative conditioning mechanisms [Paper 5].  \\n\\n## R√©f√©rences\\n[Paper 1] Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding - Shengjie Luo, Shanda Li, Tianle Cai (2021) - http://arxiv.org/abs/2106.12566v2\\n\\n[Paper 2] AttentionSmithy: A Modular Framework for Rapid Transformer Development and Customization - Caleb Cranney, Jesse G. Meyer (2025) - http://arxiv.org/abs/2502.09503v2\\n\\n[Paper 3] Revisiting Transformers with Insights from Image Filtering - Laziz U. Abdullaev, Maksim Tkachenko, Tan M. Nguyen (2025) - http://arxiv.org/abs/2506.10371v1\\n\\n[Paper 4] Context-aware Rotary Position Embedding - Ali Veisi, Delaram Fartoot, Hamidreza Amirzadeh (2025) - http://arxiv.org/abs/2507.23083v1\\n\\n[Paper 5] Positional encoding is not the same as context: A study on positional encoding for sequential recommendation - Alejo Lopez-Avila, Jinhua Du, Abbas Shimary (2024) - http://arxiv.org/abs/2405.10436v2', additional_kwargs={}, response_metadata={}, name='Summarizer')]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Configuration for this run (needed for memory/checkpointing)\n",
    "config = {\"configurable\": {\"thread_id\": \"demo-thread-1\"}}\n",
    "\n",
    "question = \"What are the key innovations in transformer architectures?\"\n",
    "\n",
    "# Provide required fields directly in state (no nested 'config' dict)\n",
    "initial_state = {\n",
    "    \"query\": question,\n",
    "    \"llm_model\": LLM_MODEL,\n",
    "    \"llm_temperature\": 0,\n",
    "    \"max_papers\": 5,\n",
    "    \"max_iterations\": 2,\n",
    "}\n",
    "\n",
    "print(\"Starting agent graph execution...\\n\")\n",
    "\n",
    "# Invoke the graph\n",
    "result = graph.invoke(initial_state, config=config)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8117bbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL RESULT\n",
      "================================================================================\n",
      "- Papers found: 5\n",
      "\n",
      "## R√©sum√©\n",
      "‚Ä¢ The RWKV architecture introduces a linear attention mechanism that enhances training efficiency akin to Transformers while improving inference efficiency similar to RNNs, marking a significant advancement in language modeling [Paper 1].  \n",
      "‚Ä¢ The \\METHOD architecture addresses unique challenges in healthcare by modularizing transformer components to better handle irregular patient timelines and complex contextual relationships, showcasing adaptability in specialized domains [Paper 2].  \n",
      "‚Ä¢ GLOD combines Swin Transformers with novel UpConvMixer blocks for high-resolution satellite imagery object detection, achieving superior performance compared to traditional CNN-based methods, thus illustrating the potential of hybrid architectures [Paper 3].  \n",
      "‚Ä¢ The ADROIT6G framework emphasizes the need for open and programmable architectures in 6G networks, highlighting the role of advanced transformer models in meeting stringent performance and reliability requirements for future applications [Paper 4].  \n",
      "\n",
      "## R√©f√©rences\n",
      "[Paper 1] The Evolution of RWKV: Advancements in Efficient Language Modeling - Akul Datta (2024) - http://arxiv.org/abs/2411.02795v1  \n",
      "[Paper 2] METHOD: Modular Efficient Transformer for Health Outcome Discovery - Linglong Qian, Zina Ibrahim (2025) - http://arxiv.org/abs/2505.17054v1  \n",
      "[Paper 3] Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery - Nicolas Drapier, Aladine Chetouani, Aur√©lien Chateigner (2025) - http://arxiv.org/abs/2507.11040v1  \n",
      "[Paper 4] ADROIT6G DAI-driven Open and Programmable Architecture for 6G Networks - Christophoros Christophorou, Iacovos Ioannou, Vasos Vassiliou (2024) - http://arxiv.org/abs/2403.05277v1  \n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"- Papers found: {len(result['papers'])}\")\n",
    "print(f\"\\n{result['summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e846657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 21:12:25,063 - INFO - üéØ Clarifying query: 'Can you give more detail regarding your first point?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Asking follow-up question...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 21:12:25,790 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-02 21:12:25,811 - INFO - ‚ú® Refined query: 'Refined query: RWKV architecture advancements in language modeling'\n",
      "2025-11-02 21:12:27,353 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-02 21:12:27,386 - INFO - Searching ArXiv: 'Refined query: RWKV architecture advancements in language modeling' (iteration 0)\n",
      "2025-11-02 21:12:28,462 - INFO - Found 5 papers\n",
      "2025-11-02 21:12:28,463 - INFO - Scoring paper relevance...\n",
      "2025-11-02 21:12:29,019 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-02 21:12:29,025 - INFO -   üìÑ Effectiveness of Deep Networks in NLP using BiDAF as an exam... - Score: 75\n",
      "2025-11-02 21:12:29,522 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-02 21:12:29,546 - INFO -   üìÑ RIRO: Reshaping Inputs, Refining Outputs Unlocking the Poten... - Score: 75\n",
      "2025-11-02 21:12:30,149 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-02 21:12:30,190 - INFO -   üìÑ Enhancing Neural Spoken Language Recognition: An Exploration... - Score: 65\n",
      "2025-11-02 21:12:30,851 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-02 21:12:30,872 - INFO -   üìÑ Iterative Prompt Refinement for Radiation Oncology Symptom E... - Score: 75\n",
      "2025-11-02 21:12:31,377 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-02 21:12:31,384 - INFO -   üìÑ Software Architecture Meets LLMs: A Systematic Literature Re... - Score: 85\n",
      "2025-11-02 21:12:33,471 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-02 21:12:33,483 - INFO - üìù Synthesizing 10 papers...\n",
      "2025-11-02 21:12:41,253 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-02 21:12:41,263 - INFO - ‚úÖ Summary generated\n",
      "2025-11-02 21:12:42,891 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FOLLOW-UP RESULT\n",
      "================================================================================\n",
      "- Papers found: 5\n",
      "\n",
      "## R√©sum√©\n",
      "‚Ä¢ The RWKV architecture introduces a linear attention mechanism that enhances training efficiency akin to Transformers while improving inference efficiency similar to RNNs, marking a significant advancement in language modeling [Paper 1].  \n",
      "‚Ä¢ The \\METHOD architecture addresses unique challenges in healthcare by modularizing transformer components to better handle irregular patient timelines and complex contextual relationships, showcasing adaptability in specialized domains [Paper 2].  \n",
      "‚Ä¢ GLOD combines Swin Transformers with novel UpConvMixer blocks for high-resolution satellite imagery object detection, achieving superior performance compared to traditional CNN-based methods, thus illustrating the potential of hybrid architectures [Paper 3].  \n",
      "‚Ä¢ The ADROIT6G framework emphasizes the need for open and programmable architectures in 6G networks, highlighting the role of advanced transformer models in meeting stringent performance and reliability requirements for future applications [Paper 4].  \n",
      "\n",
      "## R√©f√©rences\n",
      "[Paper 1] The Evolution of RWKV: Advancements in Efficient Language Modeling - Akul Datta (2024) - http://arxiv.org/abs/2411.02795v1  \n",
      "[Paper 2] METHOD: Modular Efficient Transformer for Health Outcome Discovery - Linglong Qian, Zina Ibrahim (2025) - http://arxiv.org/abs/2505.17054v1  \n",
      "[Paper 3] Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery - Nicolas Drapier, Aladine Chetouani, Aur√©lien Chateigner (2025) - http://arxiv.org/abs/2507.11040v1  \n",
      "[Paper 4] ADROIT6G DAI-driven Open and Programmable Architecture for 6G Networks - Christophoros Christophorou, Iacovos Ioannou, Vasos Vassiliou (2024) - http://arxiv.org/abs/2403.05277v1  \n"
     ]
    }
   ],
   "source": [
    "follow_up_question = \"Can you give more detail regarding your first point?\"\n",
    "\n",
    "# IMPORTANT: Only pass the new query, not the full config\n",
    "# The checkpointer will restore previous state (messages, papers, etc.)\n",
    "follow_up_state = {\n",
    "    \"query\": follow_up_question,\n",
    "}\n",
    "\n",
    "print(\"\\n\\nAsking follow-up question...\\n\")\n",
    "\n",
    "# Use the SAME config with the same thread_id\n",
    "# The checkpointer loads previous messages, papers, config automatically\n",
    "follow_up_result = graph.invoke(follow_up_state, config=config)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FOLLOW-UP RESULT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"- Papers found: {len(result['papers'])}\")\n",
    "print(f\"\\n{result['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa35b6",
   "metadata": {},
   "source": [
    "### Test 2: Query with Few Results (Triggers Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83bf138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting agent graph execution...\n",
      "\n",
      "üéØ Clarifying query: 'quantum machine learning for protein folding using NISQ devices'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 17:56:50,186 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-21 17:56:50,195 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=NISQ+variational+quantum+algorithms+protein+folding&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ú® Refined query: 'NISQ variational quantum algorithms protein folding'\n",
      "üîç Searching ArXiv: 'NISQ variational quantum algorithms protein folding' (iteration 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 17:56:51,175 - INFO - Got first page: 100 of 835056 total results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Found 5 papers\n",
      "üìù Synthesizing 5 papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 17:57:14,021 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summary generated\n",
      "\n",
      "================================================================================\n",
      "FINAL RESULT\n",
      "================================================================================\n",
      "\n",
      "- Original query: quantum machine learning for protein folding using NISQ devices\n",
      "- Refined query: NISQ variational quantum algorithms protein folding\n",
      "- Papers found: 5\n",
      "- Iterations: 2\n",
      "\n",
      "## R√©sum√©\n",
      "‚Ä¢ NISQ-oriented algorithms adapt protein folding to near-term gate-based devices by using hybrid classical‚Äìquantum workflows and compressed evolutions (digitized counterdiabatic, variational ans√§tze). These approaches aim to reduce circuit depth and runtime while keeping the search for low-energy conformations feasible on limited hardware [Paper 1, Paper 2].  \n",
      "‚Ä¢ Problem encoding and lattice models are central: mapping amino-acid chains to lattice representations (cubic, tetrahedral) and compact encodings markedly lowers qubit and gate counts, at the cost of model fidelity. Efficient encodings also enable exploring free-energy landscapes more directly on quantum hardware [Paper 3, Paper 4].  \n",
      "‚Ä¢ Variational and QAOA-like methods (including alternating‚Äëoperator ans√§tze) have shown promising numerical results for small peptides, demonstrating that approximate ground states and conformational sampling can be obtained on simulators and small devices. However, these studies repeatedly report scaling and noise-sensitivity challenges for larger proteins [Paper 5, Paper 3].  \n",
      "‚Ä¢ Resource-analysis and algorithmic compression (e.g., digitized‚Äëcounterdiabatic schemes and other resource‚Äëefficient designs) indicate realistic near‚Äëterm targets: small to medium peptides, hybrid optimization loops, and extensive classical preprocessing/postprocessing remain necessary. Continued improvements in error mitigation, encoding, and hardware will be required before tackling biologically sized folding problems [Paper 2, Paper 1, Paper 5, Paper 4].\n",
      "\n",
      "## R√©f√©rences\n",
      "[Paper 1] Digitized-Counterdiabatic Quantum Algorithm for Protein Folding - Pranav Chandarana, Narendra N. Hegade, Iraitz Montalban (2022) - http://arxiv.org/abs/2212.13511v1\n",
      "\n",
      "[Paper 2] Resource-Efficient Quantum Algorithm for Protein Folding - Anton Robert, Panagiotis Kl. Barkoutsos, Stefan Woerner (2019) - http://arxiv.org/abs/1908.02163v1\n",
      "\n",
      "[Paper 3] A quantum alternating operator ansatz with hard and soft constraints for lattice protein folding - Mark Fingerhuth, Tom√°≈° Babej, Christopher Ing (2018) - http://arxiv.org/abs/1810.13411v1\n",
      "\n",
      "[Paper 4] Capturing Protein Free Energy Landscape using Efficient Quantum Encoding - Ashwini Kannan, Jaya Vasavi Pamidimukkala, Avinash Dakshinamoorthy (2025) - http://arxiv.org/abs/2510.15316v1\n",
      "\n",
      "[Paper 5] Peptide conformational sampling using the Quantum Approximate Optimization Algorithm - Sami Boulebnane, Xavier Lucas, Agnes Meyder (2022) - http://arxiv.org/abs/2204.01821v1\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Configuration for this run (needed for memory/checkpointing)\n",
    "config = {\"configurable\": {\"thread_id\": \"demo-thread-1\"}}\n",
    "\n",
    "question = \"quantum machine learning for protein folding using NISQ devices\"\n",
    "\n",
    "initial_state = {\n",
    "    \"query\": question,\n",
    "    \"llm_model\": LLM_MODEL,\n",
    "    \"llm_temperature\": 0,\n",
    "    \"max_papers\": 5,\n",
    "    \"max_iterations\": 2,\n",
    "}\n",
    "\n",
    "print(\"Starting agent graph execution...\\n\")\n",
    "\n",
    "# Invoke the graph\n",
    "result = await graph.invoke(initial_state, config=config)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n- Original query: {result['query']}\")\n",
    "print(f\"- Refined query: {result['refined_query']}\")\n",
    "print(f\"- Papers found: {len(result['papers'])}\")\n",
    "print(f\"- Iterations: {result['iteration']}\")\n",
    "print(f\"\\n{result['summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c0f64a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StateSnapshot(values={'query': 'What are the key innovations in transformer architectures?', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2, 'summary': '## R√©sum√©\\n‚Ä¢ Kernelized and linearized attention improve scalability by approximating the softmax-based attention kernel, trading exactness for sub-quadratic or linear complexity while retaining accuracy; combining these kernels with relative positional encoding helps keep long-range structure without quadratic cost [Paper 1].  \\n‚Ä¢ Positional encodings remain central but are evolving: Context-aware extensions to Rotary Positional Embeddings introduce input-dependent frequency patterns to better capture local sequence context and reduce the mismatch between static encodings and dynamic inputs [Paper 4].  \\n‚Ä¢ New theoretical lenses (e.g., image-filtering perspectives) make attention more interpretable by showing when self-attention behaves like learned filtering versus when it provides distinct global interactions, guiding principled architectural modifications and better inductive biases [Paper 3].  \\n‚Ä¢ Engineering and adoption accelerate when transformer components are modular: frameworks that decompose attention, feed‚Äëforward, normalization and positional modules lower the barrier for domain-driven innovation and rapid experimentation [Paper 2].  \\n‚Ä¢ Task-specific studies warn against treating positional encoding as equivalent to contextual information; empirical work in sequential recommendation shows common encodings can misrepresent temporal dynamics, motivating task-aware encoding design or alternative conditioning mechanisms [Paper 5].  \\n\\n## R√©f√©rences\\n[Paper 1] Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding - Shengjie Luo, Shanda Li, Tianle Cai (2021) - http://arxiv.org/abs/2106.12566v2\\n\\n[Paper 2] AttentionSmithy: A Modular Framework for Rapid Transformer Development and Customization - Caleb Cranney, Jesse G. Meyer (2025) - http://arxiv.org/abs/2502.09503v2\\n\\n[Paper 3] Revisiting Transformers with Insights from Image Filtering - Laziz U. Abdullaev, Maksim Tkachenko, Tan M. Nguyen (2025) - http://arxiv.org/abs/2506.10371v1\\n\\n[Paper 4] Context-aware Rotary Position Embedding - Ali Veisi, Delaram Fartoot, Hamidreza Amirzadeh (2025) - http://arxiv.org/abs/2507.23083v1\\n\\n[Paper 5] Positional encoding is not the same as context: A study on positional encoding for sequential recommendation - Alejo Lopez-Avila, Jinhua Du, Abbas Shimary (2024) - http://arxiv.org/abs/2405.10436v2', 'papers': [{'id': 'http://arxiv.org/abs/2106.12566v2', 'title': 'Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding', 'authors': ['Shengjie Luo', 'Shanda Li', 'Tianle Cai', 'Di He', 'Dinglan Peng', 'Shuxin Zheng', 'Guolin Ke', 'Liwei Wang', 'Tie-Yan Liu'], 'summary': 'The attention module, which is a crucial component in Transformer, cannot\\nscale efficiently to long sequences due to its quadratic complexity. Many works\\nfocus on approximating the dot-then-exponentiate softmax function in the\\noriginal attention, leading to sub-quadratic or even linear-complexity\\nTransformer architectures. However, we show that these methods cannot be\\napplied to more powerful attention modules that go beyond the\\ndot-then-exponentiate style, e.g., Transformers with relative positional\\nencoding (RPE). Since in many state-of-the-art models, relative positional\\nencoding is used as default, designing efficient Transformers that can\\nincorporate RPE is appealing. In this paper, we propose a novel way to\\naccelerate attention calculation for Transformers with RPE on top of the\\nkernelized attention. Based upon the observation that relative positional\\nencoding forms a Toeplitz matrix, we mathematically show that kernelized\\nattention with RPE can be calculated efficiently using Fast Fourier Transform\\n(FFT). With FFT, our method achieves $\\\\mathcal{O}(n\\\\log n)$ time complexity.\\nInterestingly, we further demonstrate that properly using relative positional\\nencoding can mitigate the training instability problem of vanilla kernelized\\nattention. On a wide range of tasks, we empirically show that our models can be\\ntrained from scratch without any optimization issues. The learned model\\nperforms better than many efficient Transformer variants and is faster than\\nstandard Transformer in the long-sequence regime.', 'url': 'http://arxiv.org/abs/2106.12566v2', 'published': '2021-06-23', 'relevance_score': 65}, {'id': 'http://arxiv.org/abs/2502.09503v2', 'title': 'AttentionSmithy: A Modular Framework for Rapid Transformer Development and Customization', 'authors': ['Caleb Cranney', 'Jesse G. Meyer'], 'summary': \"Transformer architectures have transformed AI applications but remain complex\\nto customize for domain experts lacking low-level implementation expertise. We\\nintroduce AttentionSmithy, a modular software package that simplifies\\ntransformer innovation by breaking down key components into reusable building\\nblocks: attention modules, feed-forward networks, normalization layers, and\\npositional encodings. Users can rapidly prototype and evaluate transformer\\nvariants without extensive coding. Our framework supports four positional\\nencoding strategies and integrates with neural architecture search for\\nautomated design. We validate AttentionSmithy by replicating the original\\ntransformer under resource constraints and optimizing translation performance\\nby combining positional encodings. Additionally, we demonstrate its\\nadaptability in gene-specific modeling, achieving over 95% accuracy in cell\\ntype classification. These case studies highlight AttentionSmithy's potential\\nto accelerate research across diverse fields by removing framework\\nimplementation barriers.\", 'url': 'http://arxiv.org/abs/2502.09503v2', 'published': '2025-02-13', 'relevance_score': 60}, {'id': 'http://arxiv.org/abs/2506.10371v1', 'title': 'Revisiting Transformers with Insights from Image Filtering', 'authors': ['Laziz U. Abdullaev', 'Maksim Tkachenko', 'Tan M. Nguyen'], 'summary': 'The self-attention mechanism, a cornerstone of Transformer-based\\nstate-of-the-art deep learning architectures, is largely heuristic-driven and\\nfundamentally challenging to interpret. Establishing a robust theoretical\\nfoundation to explain its remarkable success and limitations has therefore\\nbecome an increasingly prominent focus in recent research. Some notable\\ndirections have explored understanding self-attention through the lens of image\\ndenoising and nonparametric regression. While promising, existing frameworks\\nstill lack a deeper mechanistic interpretation of various architectural\\ncomponents that enhance self-attention, both in its original formulation and\\nsubsequent variants. In this work, we aim to advance this understanding by\\ndeveloping a unifying image processing framework, capable of explaining not\\nonly the self-attention computation itself but also the role of components such\\nas positional encoding and residual connections, including numerous later\\nvariants. We also pinpoint potential distinctions between the two concepts\\nbuilding upon our framework, and make effort to close this gap. We introduce\\ntwo independent architectural modifications within transformers. While our\\nprimary objective is interpretability, we empirically observe that image\\nprocessing-inspired modifications can also lead to notably improved accuracy\\nand robustness against data contamination and adversaries across language and\\nvision tasks as well as better long sequence understanding.', 'url': 'http://arxiv.org/abs/2506.10371v1', 'published': '2025-06-12', 'relevance_score': 55}, {'id': 'http://arxiv.org/abs/2507.23083v1', 'title': 'Context-aware Rotary Position Embedding', 'authors': ['Ali Veisi', 'Delaram Fartoot', 'Hamidreza Amirzadeh'], 'summary': 'Positional encoding is a vital component of Transformer architectures,\\nenabling models to incorporate sequence order into self-attention mechanisms.\\nRotary Positional Embeddings (RoPE) have become a widely adopted solution due\\nto their compatibility with relative position encoding and computational\\nefficiency. However, RoPE relies on static, input-independent sinusoidal\\nfrequency patterns, limiting its ability to model context-sensitive\\nrelationships. In this work, we propose CARoPE (Context-Aware Rotary Positional\\nEmbedding), a novel generalization of RoPE that dynamically generates\\nhead-specific frequency patterns conditioned on token embeddings. This design\\nintroduces token- and context-sensitive positional representations while\\npreserving RoPE efficiency and architectural simplicity. CARoPE computes\\ninput-dependent phase shifts using a bounded transformation of token embeddings\\nand integrates them into the rotary mechanism across attention heads. We\\nevaluate CARoPE on the FineWeb-Edu-10B dataset using GPT-2 variants trained on\\nnext-token prediction tasks. Experimental results show that CARoPE consistently\\noutperforms RoPE and other common positional encoding baselines, achieving\\nsignificantly lower perplexity, even at longer context lengths. Additionally,\\nCARoPE enables faster training throughput without sacrificing model stability.\\nThese findings demonstrate that CARoPE offers a scalable, expressive, and\\nefficient upgrade to existing positional encoding strategies in Transformer\\nmodels.', 'url': 'http://arxiv.org/abs/2507.23083v1', 'published': '2025-07-30', 'relevance_score': 45}, {'id': 'http://arxiv.org/abs/2405.10436v2', 'title': 'Positional encoding is not the same as context: A study on positional encoding for sequential recommendation', 'authors': ['Alejo Lopez-Avila', 'Jinhua Du', 'Abbas Shimary', 'Ze Li'], 'summary': \"The rapid growth of streaming media and e-commerce has driven advancements in\\nrecommendation systems, particularly Sequential Recommendation Systems (SRS).\\nThese systems employ users' interaction histories to predict future\\npreferences. While recent research has focused on architectural innovations\\nlike transformer blocks and feature extraction, positional encodings, crucial\\nfor capturing temporal patterns, have received less attention. These encodings\\nare often conflated with contextual, such as the temporal footprint, which\\nprevious works tend to treat as interchangeable with positional information.\\nThis paper highlights the critical distinction between temporal footprint and\\npositional encodings, demonstrating that the latter offers unique relational\\ncues between items, which the temporal footprint alone cannot provide. Through\\nextensive experimentation on eight Amazon datasets and subsets, we assess the\\nimpact of various encodings on performance metrics and training stability. We\\nintroduce new positional encodings and investigate integration strategies that\\nimprove both metrics and stability, surpassing state-of-the-art results at the\\ntime of this work's initial preprint. Importantly, we demonstrate that\\nselecting the appropriate encoding is not only key to better performance but\\nalso essential for building robust, reliable SRS models.\", 'url': 'http://arxiv.org/abs/2405.10436v2', 'published': '2024-05-16', 'relevance_score': 30}], 'messages': [SystemMessage(content='User inquired about the key innovations in transformer architectures, which suggests an interest in advancements in this field.', additional_kwargs={}, response_metadata={}), AIMessage(content='Refined query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Clarifier'), AIMessage(content='Found 5 papers on ArXiv for query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Researcher'), AIMessage(content='## R√©sum√©\\n‚Ä¢ Kernelized and linearized attention improve scalability by approximating the softmax-based attention kernel, trading exactness for sub-quadratic or linear complexity while retaining accuracy; combining these kernels with relative positional encoding helps keep long-range structure without quadratic cost [Paper 1].  \\n‚Ä¢ Positional encodings remain central but are evolving: Context-aware extensions to Rotary Positional Embeddings introduce input-dependent frequency patterns to better capture local sequence context and reduce the mismatch between static encodings and dynamic inputs [Paper 4].  \\n‚Ä¢ New theoretical lenses (e.g., image-filtering perspectives) make attention more interpretable by showing when self-attention behaves like learned filtering versus when it provides distinct global interactions, guiding principled architectural modifications and better inductive biases [Paper 3].  \\n‚Ä¢ Engineering and adoption accelerate when transformer components are modular: frameworks that decompose attention, feed‚Äëforward, normalization and positional modules lower the barrier for domain-driven innovation and rapid experimentation [Paper 2].  \\n‚Ä¢ Task-specific studies warn against treating positional encoding as equivalent to contextual information; empirical work in sequential recommendation shows common encodings can misrepresent temporal dynamics, motivating task-aware encoding design or alternative conditioning mechanisms [Paper 5].  \\n\\n## R√©f√©rences\\n[Paper 1] Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding - Shengjie Luo, Shanda Li, Tianle Cai (2021) - http://arxiv.org/abs/2106.12566v2\\n\\n[Paper 2] AttentionSmithy: A Modular Framework for Rapid Transformer Development and Customization - Caleb Cranney, Jesse G. Meyer (2025) - http://arxiv.org/abs/2502.09503v2\\n\\n[Paper 3] Revisiting Transformers with Insights from Image Filtering - Laziz U. Abdullaev, Maksim Tkachenko, Tan M. Nguyen (2025) - http://arxiv.org/abs/2506.10371v1\\n\\n[Paper 4] Context-aware Rotary Position Embedding - Ali Veisi, Delaram Fartoot, Hamidreza Amirzadeh (2025) - http://arxiv.org/abs/2507.23083v1\\n\\n[Paper 5] Positional encoding is not the same as context: A study on positional encoding for sequential recommendation - Alejo Lopez-Avila, Jinhua Du, Abbas Shimary (2024) - http://arxiv.org/abs/2405.10436v2', additional_kwargs={}, response_metadata={}, name='Summarizer')], 'refined_query': 'transformer architecture innovations attention variants positional encodings', 'iteration': 1}, next=(), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b82aa-c810-6854-8003-edaa360d77e9'}}, metadata={'source': 'loop', 'step': 3, 'parents': {}}, created_at='2025-11-02T20:29:47.075981+00:00', parent_config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b82aa-2b58-6642-8002-15631b450f65'}}, tasks=(), interrupts=()),\n",
       " StateSnapshot(values={'query': 'What are the key innovations in transformer architectures?', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2, 'papers': [{'id': 'http://arxiv.org/abs/2106.12566v2', 'title': 'Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding', 'authors': ['Shengjie Luo', 'Shanda Li', 'Tianle Cai', 'Di He', 'Dinglan Peng', 'Shuxin Zheng', 'Guolin Ke', 'Liwei Wang', 'Tie-Yan Liu'], 'summary': 'The attention module, which is a crucial component in Transformer, cannot\\nscale efficiently to long sequences due to its quadratic complexity. Many works\\nfocus on approximating the dot-then-exponentiate softmax function in the\\noriginal attention, leading to sub-quadratic or even linear-complexity\\nTransformer architectures. However, we show that these methods cannot be\\napplied to more powerful attention modules that go beyond the\\ndot-then-exponentiate style, e.g., Transformers with relative positional\\nencoding (RPE). Since in many state-of-the-art models, relative positional\\nencoding is used as default, designing efficient Transformers that can\\nincorporate RPE is appealing. In this paper, we propose a novel way to\\naccelerate attention calculation for Transformers with RPE on top of the\\nkernelized attention. Based upon the observation that relative positional\\nencoding forms a Toeplitz matrix, we mathematically show that kernelized\\nattention with RPE can be calculated efficiently using Fast Fourier Transform\\n(FFT). With FFT, our method achieves $\\\\mathcal{O}(n\\\\log n)$ time complexity.\\nInterestingly, we further demonstrate that properly using relative positional\\nencoding can mitigate the training instability problem of vanilla kernelized\\nattention. On a wide range of tasks, we empirically show that our models can be\\ntrained from scratch without any optimization issues. The learned model\\nperforms better than many efficient Transformer variants and is faster than\\nstandard Transformer in the long-sequence regime.', 'url': 'http://arxiv.org/abs/2106.12566v2', 'published': '2021-06-23', 'relevance_score': 65}, {'id': 'http://arxiv.org/abs/2502.09503v2', 'title': 'AttentionSmithy: A Modular Framework for Rapid Transformer Development and Customization', 'authors': ['Caleb Cranney', 'Jesse G. Meyer'], 'summary': \"Transformer architectures have transformed AI applications but remain complex\\nto customize for domain experts lacking low-level implementation expertise. We\\nintroduce AttentionSmithy, a modular software package that simplifies\\ntransformer innovation by breaking down key components into reusable building\\nblocks: attention modules, feed-forward networks, normalization layers, and\\npositional encodings. Users can rapidly prototype and evaluate transformer\\nvariants without extensive coding. Our framework supports four positional\\nencoding strategies and integrates with neural architecture search for\\nautomated design. We validate AttentionSmithy by replicating the original\\ntransformer under resource constraints and optimizing translation performance\\nby combining positional encodings. Additionally, we demonstrate its\\nadaptability in gene-specific modeling, achieving over 95% accuracy in cell\\ntype classification. These case studies highlight AttentionSmithy's potential\\nto accelerate research across diverse fields by removing framework\\nimplementation barriers.\", 'url': 'http://arxiv.org/abs/2502.09503v2', 'published': '2025-02-13', 'relevance_score': 60}, {'id': 'http://arxiv.org/abs/2506.10371v1', 'title': 'Revisiting Transformers with Insights from Image Filtering', 'authors': ['Laziz U. Abdullaev', 'Maksim Tkachenko', 'Tan M. Nguyen'], 'summary': 'The self-attention mechanism, a cornerstone of Transformer-based\\nstate-of-the-art deep learning architectures, is largely heuristic-driven and\\nfundamentally challenging to interpret. Establishing a robust theoretical\\nfoundation to explain its remarkable success and limitations has therefore\\nbecome an increasingly prominent focus in recent research. Some notable\\ndirections have explored understanding self-attention through the lens of image\\ndenoising and nonparametric regression. While promising, existing frameworks\\nstill lack a deeper mechanistic interpretation of various architectural\\ncomponents that enhance self-attention, both in its original formulation and\\nsubsequent variants. In this work, we aim to advance this understanding by\\ndeveloping a unifying image processing framework, capable of explaining not\\nonly the self-attention computation itself but also the role of components such\\nas positional encoding and residual connections, including numerous later\\nvariants. We also pinpoint potential distinctions between the two concepts\\nbuilding upon our framework, and make effort to close this gap. We introduce\\ntwo independent architectural modifications within transformers. While our\\nprimary objective is interpretability, we empirically observe that image\\nprocessing-inspired modifications can also lead to notably improved accuracy\\nand robustness against data contamination and adversaries across language and\\nvision tasks as well as better long sequence understanding.', 'url': 'http://arxiv.org/abs/2506.10371v1', 'published': '2025-06-12', 'relevance_score': 55}, {'id': 'http://arxiv.org/abs/2507.23083v1', 'title': 'Context-aware Rotary Position Embedding', 'authors': ['Ali Veisi', 'Delaram Fartoot', 'Hamidreza Amirzadeh'], 'summary': 'Positional encoding is a vital component of Transformer architectures,\\nenabling models to incorporate sequence order into self-attention mechanisms.\\nRotary Positional Embeddings (RoPE) have become a widely adopted solution due\\nto their compatibility with relative position encoding and computational\\nefficiency. However, RoPE relies on static, input-independent sinusoidal\\nfrequency patterns, limiting its ability to model context-sensitive\\nrelationships. In this work, we propose CARoPE (Context-Aware Rotary Positional\\nEmbedding), a novel generalization of RoPE that dynamically generates\\nhead-specific frequency patterns conditioned on token embeddings. This design\\nintroduces token- and context-sensitive positional representations while\\npreserving RoPE efficiency and architectural simplicity. CARoPE computes\\ninput-dependent phase shifts using a bounded transformation of token embeddings\\nand integrates them into the rotary mechanism across attention heads. We\\nevaluate CARoPE on the FineWeb-Edu-10B dataset using GPT-2 variants trained on\\nnext-token prediction tasks. Experimental results show that CARoPE consistently\\noutperforms RoPE and other common positional encoding baselines, achieving\\nsignificantly lower perplexity, even at longer context lengths. Additionally,\\nCARoPE enables faster training throughput without sacrificing model stability.\\nThese findings demonstrate that CARoPE offers a scalable, expressive, and\\nefficient upgrade to existing positional encoding strategies in Transformer\\nmodels.', 'url': 'http://arxiv.org/abs/2507.23083v1', 'published': '2025-07-30', 'relevance_score': 45}, {'id': 'http://arxiv.org/abs/2405.10436v2', 'title': 'Positional encoding is not the same as context: A study on positional encoding for sequential recommendation', 'authors': ['Alejo Lopez-Avila', 'Jinhua Du', 'Abbas Shimary', 'Ze Li'], 'summary': \"The rapid growth of streaming media and e-commerce has driven advancements in\\nrecommendation systems, particularly Sequential Recommendation Systems (SRS).\\nThese systems employ users' interaction histories to predict future\\npreferences. While recent research has focused on architectural innovations\\nlike transformer blocks and feature extraction, positional encodings, crucial\\nfor capturing temporal patterns, have received less attention. These encodings\\nare often conflated with contextual, such as the temporal footprint, which\\nprevious works tend to treat as interchangeable with positional information.\\nThis paper highlights the critical distinction between temporal footprint and\\npositional encodings, demonstrating that the latter offers unique relational\\ncues between items, which the temporal footprint alone cannot provide. Through\\nextensive experimentation on eight Amazon datasets and subsets, we assess the\\nimpact of various encodings on performance metrics and training stability. We\\nintroduce new positional encodings and investigate integration strategies that\\nimprove both metrics and stability, surpassing state-of-the-art results at the\\ntime of this work's initial preprint. Importantly, we demonstrate that\\nselecting the appropriate encoding is not only key to better performance but\\nalso essential for building robust, reliable SRS models.\", 'url': 'http://arxiv.org/abs/2405.10436v2', 'published': '2024-05-16', 'relevance_score': 30}], 'messages': [HumanMessage(content='What are the key innovations in transformer architectures?', additional_kwargs={}, response_metadata={}, name='User'), AIMessage(content='Refined query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Clarifier'), AIMessage(content='Found 5 papers on ArXiv for query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Researcher')], 'refined_query': 'transformer architecture innovations attention variants positional encodings', 'iteration': 1}, next=('summarizer',), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b82aa-2b58-6642-8002-15631b450f65'}}, metadata={'source': 'loop', 'step': 2, 'parents': {}}, created_at='2025-11-02T20:29:30.642777+00:00', parent_config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b82a9-51ab-6692-8001-c4cc81da0a2c'}}, tasks=(PregelTask(id='b695b72c-09ff-65dd-8808-c63f5b6ca19e', name='summarizer', path=('__pregel_pull', 'summarizer'), error=None, interrupts=(), state=None, result={'summary': '## R√©sum√©\\n‚Ä¢ Kernelized and linearized attention improve scalability by approximating the softmax-based attention kernel, trading exactness for sub-quadratic or linear complexity while retaining accuracy; combining these kernels with relative positional encoding helps keep long-range structure without quadratic cost [Paper 1].  \\n‚Ä¢ Positional encodings remain central but are evolving: Context-aware extensions to Rotary Positional Embeddings introduce input-dependent frequency patterns to better capture local sequence context and reduce the mismatch between static encodings and dynamic inputs [Paper 4].  \\n‚Ä¢ New theoretical lenses (e.g., image-filtering perspectives) make attention more interpretable by showing when self-attention behaves like learned filtering versus when it provides distinct global interactions, guiding principled architectural modifications and better inductive biases [Paper 3].  \\n‚Ä¢ Engineering and adoption accelerate when transformer components are modular: frameworks that decompose attention, feed‚Äëforward, normalization and positional modules lower the barrier for domain-driven innovation and rapid experimentation [Paper 2].  \\n‚Ä¢ Task-specific studies warn against treating positional encoding as equivalent to contextual information; empirical work in sequential recommendation shows common encodings can misrepresent temporal dynamics, motivating task-aware encoding design or alternative conditioning mechanisms [Paper 5].  \\n\\n## R√©f√©rences\\n[Paper 1] Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding - Shengjie Luo, Shanda Li, Tianle Cai (2021) - http://arxiv.org/abs/2106.12566v2\\n\\n[Paper 2] AttentionSmithy: A Modular Framework for Rapid Transformer Development and Customization - Caleb Cranney, Jesse G. Meyer (2025) - http://arxiv.org/abs/2502.09503v2\\n\\n[Paper 3] Revisiting Transformers with Insights from Image Filtering - Laziz U. Abdullaev, Maksim Tkachenko, Tan M. Nguyen (2025) - http://arxiv.org/abs/2506.10371v1\\n\\n[Paper 4] Context-aware Rotary Position Embedding - Ali Veisi, Delaram Fartoot, Hamidreza Amirzadeh (2025) - http://arxiv.org/abs/2507.23083v1\\n\\n[Paper 5] Positional encoding is not the same as context: A study on positional encoding for sequential recommendation - Alejo Lopez-Avila, Jinhua Du, Abbas Shimary (2024) - http://arxiv.org/abs/2405.10436v2', 'messages': [AIMessage(content='## R√©sum√©\\n‚Ä¢ Kernelized and linearized attention improve scalability by approximating the softmax-based attention kernel, trading exactness for sub-quadratic or linear complexity while retaining accuracy; combining these kernels with relative positional encoding helps keep long-range structure without quadratic cost [Paper 1].  \\n‚Ä¢ Positional encodings remain central but are evolving: Context-aware extensions to Rotary Positional Embeddings introduce input-dependent frequency patterns to better capture local sequence context and reduce the mismatch between static encodings and dynamic inputs [Paper 4].  \\n‚Ä¢ New theoretical lenses (e.g., image-filtering perspectives) make attention more interpretable by showing when self-attention behaves like learned filtering versus when it provides distinct global interactions, guiding principled architectural modifications and better inductive biases [Paper 3].  \\n‚Ä¢ Engineering and adoption accelerate when transformer components are modular: frameworks that decompose attention, feed‚Äëforward, normalization and positional modules lower the barrier for domain-driven innovation and rapid experimentation [Paper 2].  \\n‚Ä¢ Task-specific studies warn against treating positional encoding as equivalent to contextual information; empirical work in sequential recommendation shows common encodings can misrepresent temporal dynamics, motivating task-aware encoding design or alternative conditioning mechanisms [Paper 5].  \\n\\n## R√©f√©rences\\n[Paper 1] Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding - Shengjie Luo, Shanda Li, Tianle Cai (2021) - http://arxiv.org/abs/2106.12566v2\\n\\n[Paper 2] AttentionSmithy: A Modular Framework for Rapid Transformer Development and Customization - Caleb Cranney, Jesse G. Meyer (2025) - http://arxiv.org/abs/2502.09503v2\\n\\n[Paper 3] Revisiting Transformers with Insights from Image Filtering - Laziz U. Abdullaev, Maksim Tkachenko, Tan M. Nguyen (2025) - http://arxiv.org/abs/2506.10371v1\\n\\n[Paper 4] Context-aware Rotary Position Embedding - Ali Veisi, Delaram Fartoot, Hamidreza Amirzadeh (2025) - http://arxiv.org/abs/2507.23083v1\\n\\n[Paper 5] Positional encoding is not the same as context: A study on positional encoding for sequential recommendation - Alejo Lopez-Avila, Jinhua Du, Abbas Shimary (2024) - http://arxiv.org/abs/2405.10436v2', additional_kwargs={}, response_metadata={}, name='Summarizer')]}),), interrupts=()),\n",
       " StateSnapshot(values={'query': 'What are the key innovations in transformer architectures?', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2, 'papers': [], 'messages': [HumanMessage(content='What are the key innovations in transformer architectures?', additional_kwargs={}, response_metadata={}, name='User'), AIMessage(content='Refined query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Clarifier')], 'refined_query': 'transformer architecture innovations attention variants positional encodings', 'iteration': 0}, next=('researcher',), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b82a9-51ab-6692-8001-c4cc81da0a2c'}}, metadata={'source': 'loop', 'step': 1, 'parents': {}}, created_at='2025-11-02T20:29:07.817826+00:00', parent_config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b82a9-2132-6b14-8000-685e1fa890d8'}}, tasks=(PregelTask(id='fb740216-6c15-46e7-4341-7548472f0b17', name='researcher', path=('__pregel_pull', 'researcher'), error=None, interrupts=(), state=None, result={'papers': [{'id': 'http://arxiv.org/abs/2502.09503v2', 'title': 'AttentionSmithy: A Modular Framework for Rapid Transformer Development and Customization', 'authors': ['Caleb Cranney', 'Jesse G. Meyer'], 'summary': \"Transformer architectures have transformed AI applications but remain complex\\nto customize for domain experts lacking low-level implementation expertise. We\\nintroduce AttentionSmithy, a modular software package that simplifies\\ntransformer innovation by breaking down key components into reusable building\\nblocks: attention modules, feed-forward networks, normalization layers, and\\npositional encodings. Users can rapidly prototype and evaluate transformer\\nvariants without extensive coding. Our framework supports four positional\\nencoding strategies and integrates with neural architecture search for\\nautomated design. We validate AttentionSmithy by replicating the original\\ntransformer under resource constraints and optimizing translation performance\\nby combining positional encodings. Additionally, we demonstrate its\\nadaptability in gene-specific modeling, achieving over 95% accuracy in cell\\ntype classification. These case studies highlight AttentionSmithy's potential\\nto accelerate research across diverse fields by removing framework\\nimplementation barriers.\", 'url': 'http://arxiv.org/abs/2502.09503v2', 'published': '2025-02-13', 'relevance_score': 60}, {'id': 'http://arxiv.org/abs/2106.12566v2', 'title': 'Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding', 'authors': ['Shengjie Luo', 'Shanda Li', 'Tianle Cai', 'Di He', 'Dinglan Peng', 'Shuxin Zheng', 'Guolin Ke', 'Liwei Wang', 'Tie-Yan Liu'], 'summary': 'The attention module, which is a crucial component in Transformer, cannot\\nscale efficiently to long sequences due to its quadratic complexity. Many works\\nfocus on approximating the dot-then-exponentiate softmax function in the\\noriginal attention, leading to sub-quadratic or even linear-complexity\\nTransformer architectures. However, we show that these methods cannot be\\napplied to more powerful attention modules that go beyond the\\ndot-then-exponentiate style, e.g., Transformers with relative positional\\nencoding (RPE). Since in many state-of-the-art models, relative positional\\nencoding is used as default, designing efficient Transformers that can\\nincorporate RPE is appealing. In this paper, we propose a novel way to\\naccelerate attention calculation for Transformers with RPE on top of the\\nkernelized attention. Based upon the observation that relative positional\\nencoding forms a Toeplitz matrix, we mathematically show that kernelized\\nattention with RPE can be calculated efficiently using Fast Fourier Transform\\n(FFT). With FFT, our method achieves $\\\\mathcal{O}(n\\\\log n)$ time complexity.\\nInterestingly, we further demonstrate that properly using relative positional\\nencoding can mitigate the training instability problem of vanilla kernelized\\nattention. On a wide range of tasks, we empirically show that our models can be\\ntrained from scratch without any optimization issues. The learned model\\nperforms better than many efficient Transformer variants and is faster than\\nstandard Transformer in the long-sequence regime.', 'url': 'http://arxiv.org/abs/2106.12566v2', 'published': '2021-06-23', 'relevance_score': 65}, {'id': 'http://arxiv.org/abs/2507.23083v1', 'title': 'Context-aware Rotary Position Embedding', 'authors': ['Ali Veisi', 'Delaram Fartoot', 'Hamidreza Amirzadeh'], 'summary': 'Positional encoding is a vital component of Transformer architectures,\\nenabling models to incorporate sequence order into self-attention mechanisms.\\nRotary Positional Embeddings (RoPE) have become a widely adopted solution due\\nto their compatibility with relative position encoding and computational\\nefficiency. However, RoPE relies on static, input-independent sinusoidal\\nfrequency patterns, limiting its ability to model context-sensitive\\nrelationships. In this work, we propose CARoPE (Context-Aware Rotary Positional\\nEmbedding), a novel generalization of RoPE that dynamically generates\\nhead-specific frequency patterns conditioned on token embeddings. This design\\nintroduces token- and context-sensitive positional representations while\\npreserving RoPE efficiency and architectural simplicity. CARoPE computes\\ninput-dependent phase shifts using a bounded transformation of token embeddings\\nand integrates them into the rotary mechanism across attention heads. We\\nevaluate CARoPE on the FineWeb-Edu-10B dataset using GPT-2 variants trained on\\nnext-token prediction tasks. Experimental results show that CARoPE consistently\\noutperforms RoPE and other common positional encoding baselines, achieving\\nsignificantly lower perplexity, even at longer context lengths. Additionally,\\nCARoPE enables faster training throughput without sacrificing model stability.\\nThese findings demonstrate that CARoPE offers a scalable, expressive, and\\nefficient upgrade to existing positional encoding strategies in Transformer\\nmodels.', 'url': 'http://arxiv.org/abs/2507.23083v1', 'published': '2025-07-30', 'relevance_score': 45}, {'id': 'http://arxiv.org/abs/2405.10436v2', 'title': 'Positional encoding is not the same as context: A study on positional encoding for sequential recommendation', 'authors': ['Alejo Lopez-Avila', 'Jinhua Du', 'Abbas Shimary', 'Ze Li'], 'summary': \"The rapid growth of streaming media and e-commerce has driven advancements in\\nrecommendation systems, particularly Sequential Recommendation Systems (SRS).\\nThese systems employ users' interaction histories to predict future\\npreferences. While recent research has focused on architectural innovations\\nlike transformer blocks and feature extraction, positional encodings, crucial\\nfor capturing temporal patterns, have received less attention. These encodings\\nare often conflated with contextual, such as the temporal footprint, which\\nprevious works tend to treat as interchangeable with positional information.\\nThis paper highlights the critical distinction between temporal footprint and\\npositional encodings, demonstrating that the latter offers unique relational\\ncues between items, which the temporal footprint alone cannot provide. Through\\nextensive experimentation on eight Amazon datasets and subsets, we assess the\\nimpact of various encodings on performance metrics and training stability. We\\nintroduce new positional encodings and investigate integration strategies that\\nimprove both metrics and stability, surpassing state-of-the-art results at the\\ntime of this work's initial preprint. Importantly, we demonstrate that\\nselecting the appropriate encoding is not only key to better performance but\\nalso essential for building robust, reliable SRS models.\", 'url': 'http://arxiv.org/abs/2405.10436v2', 'published': '2024-05-16', 'relevance_score': 30}, {'id': 'http://arxiv.org/abs/2506.10371v1', 'title': 'Revisiting Transformers with Insights from Image Filtering', 'authors': ['Laziz U. Abdullaev', 'Maksim Tkachenko', 'Tan M. Nguyen'], 'summary': 'The self-attention mechanism, a cornerstone of Transformer-based\\nstate-of-the-art deep learning architectures, is largely heuristic-driven and\\nfundamentally challenging to interpret. Establishing a robust theoretical\\nfoundation to explain its remarkable success and limitations has therefore\\nbecome an increasingly prominent focus in recent research. Some notable\\ndirections have explored understanding self-attention through the lens of image\\ndenoising and nonparametric regression. While promising, existing frameworks\\nstill lack a deeper mechanistic interpretation of various architectural\\ncomponents that enhance self-attention, both in its original formulation and\\nsubsequent variants. In this work, we aim to advance this understanding by\\ndeveloping a unifying image processing framework, capable of explaining not\\nonly the self-attention computation itself but also the role of components such\\nas positional encoding and residual connections, including numerous later\\nvariants. We also pinpoint potential distinctions between the two concepts\\nbuilding upon our framework, and make effort to close this gap. We introduce\\ntwo independent architectural modifications within transformers. While our\\nprimary objective is interpretability, we empirically observe that image\\nprocessing-inspired modifications can also lead to notably improved accuracy\\nand robustness against data contamination and adversaries across language and\\nvision tasks as well as better long sequence understanding.', 'url': 'http://arxiv.org/abs/2506.10371v1', 'published': '2025-06-12', 'relevance_score': 55}], 'iteration': 1, 'messages': [AIMessage(content='Found 5 papers on ArXiv for query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Researcher')]}),), interrupts=()),\n",
       " StateSnapshot(values={'query': 'What are the key innovations in transformer architectures?', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2, 'papers': [], 'messages': []}, next=('clarifier',), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b82a9-2132-6b14-8000-685e1fa890d8'}}, metadata={'source': 'loop', 'step': 0, 'parents': {}}, created_at='2025-11-02T20:29:02.735219+00:00', parent_config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b82a9-2131-60b6-bfff-5f0130154889'}}, tasks=(PregelTask(id='9c33d40b-86a9-6714-f86f-9439a831fd07', name='clarifier', path=('__pregel_pull', 'clarifier'), error=None, interrupts=(), state=None, result={'refined_query': 'transformer architecture innovations attention variants positional encodings', 'iteration': 0, 'messages': [HumanMessage(content='What are the key innovations in transformer architectures?', additional_kwargs={}, response_metadata={}, name='User'), AIMessage(content='Refined query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Clarifier')]}),), interrupts=()),\n",
       " StateSnapshot(values={'papers': [], 'messages': []}, next=('__start__',), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0b82a9-2131-60b6-bfff-5f0130154889'}}, metadata={'source': 'input', 'step': -1, 'parents': {}}, created_at='2025-11-02T20:29:02.734549+00:00', parent_config=None, tasks=(PregelTask(id='a7768a24-fcfd-494c-c518-0f8b56c6cbef', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'query': 'What are the key innovations in transformer architectures?', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2}),), interrupts=())]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(graph.get_state_history(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798810c8",
   "metadata": {},
   "source": [
    "### Test 3: Memory Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45cc2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Clarifying query: 'Explain attention mechanisms'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 17:57:20,220 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-21 17:57:20,226 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=attention+mechanisms+in+neural+networks+review&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ú® Refined query: 'attention mechanisms in neural networks review'\n",
      "üîç Searching ArXiv: 'attention mechanisms in neural networks review' (iteration 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 17:57:21,480 - INFO - Got first page: 100 of 2705421 total results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Found 5 papers\n",
      "üìù Synthesizing 5 papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 17:57:45,685 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summary generated\n",
      "‚úÖ First query completed - 4 messages in memory\n",
      "üéØ Clarifying query: 'What about self-attention?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 17:57:51,946 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-21 17:57:51,955 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=self-attention+mechanisms+in+transformers&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ú® Refined query: 'self-attention mechanisms in transformers'\n",
      "üîç Searching ArXiv: 'self-attention mechanisms in transformers' (iteration 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 17:57:53,036 - INFO - Got first page: 100 of 2695572 total results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Found 5 papers\n",
      "üìù Synthesizing 5 papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 17:58:17,979 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Summary generated\n",
      "‚úÖ Second query completed - 8 messages in memory\n",
      "\n",
      "üìú Message History:\n",
      "  1. [AIMessage] Summarizer: ## R√©sum√©\n",
      "‚Ä¢ Attention is a mechanism that selects and differentially weights parts of an input so a ...\n",
      "  2. [HumanMessage] User: What about self-attention?\n",
      "  3. [AIMessage] Clarifier: Refined query: self-attention mechanisms in transformers\n",
      "  4. [AIMessage] Researcher: Found 5 papers on ArXiv for query: self-attention mechanisms in transformers\n",
      "  5. [AIMessage] Summarizer: ## R√©sum√©\n",
      "‚Ä¢ Self-attention emerges from a more general principle of learned pairwise affinity matric...\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"demo-thread-3\"}}\n",
    "\n",
    "# First query\n",
    "question1 = \"Explain attention mechanisms\"\n",
    "result1 = await graph.invoke({\n",
    "    \"query\": question1,\n",
    "    \"llm_model\": LLM_MODEL,\n",
    "    \"llm_temperature\": 0,\n",
    "    \"max_papers\": 5,\n",
    "    \"max_iterations\": 2,\n",
    "}, config=config)\n",
    "\n",
    "print(f\"‚úÖ First query completed - {len(result1['messages'])} messages in memory\")\n",
    "\n",
    "# Second query (same thread, memory persists automatically via checkpointer)\n",
    "question2 = \"What about self-attention?\"\n",
    "result2 = await graph.invoke({\n",
    "    \"query\": question2,\n",
    "    \"llm_model\": LLM_MODEL,\n",
    "    \"llm_temperature\": 0,\n",
    "    \"max_papers\": 5,\n",
    "    \"max_iterations\": 2,\n",
    "}, config=config)\n",
    "\n",
    "print(f\"‚úÖ Second query completed - {len(result2['messages'])} messages in memory\")\n",
    "\n",
    "# Display message history\n",
    "print(\"\\nüìú Message History:\")\n",
    "for i, msg in enumerate(result2['messages'][-5:], 1):  # Show last 5 messages\n",
    "    msg_type = msg.__class__.__name__\n",
    "    name = getattr(msg, 'name', 'Unknown')\n",
    "    content_preview = msg.content[:100] + \"...\" if len(msg.content) > 80 else msg.content\n",
    "    print(f\"  {i}. [{msg_type}] {name}: {content_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d547b2a3",
   "metadata": {},
   "source": [
    "## View Results in LangSmith Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f8c1c2",
   "metadata": {},
   "source": [
    "### Option 1: View Past Runs (Tracing)\n",
    "\n",
    "If you have `LANGCHAIN_TRACING_V2=true` in your `.env`:\n",
    "1. Go to https://smith.langchain.com\n",
    "2. Navigate to your project (e.g., \"scientific-graph-agent\")\n",
    "3. Click on any run to see:\n",
    "   - Full execution trace\n",
    "   - Each node's input/output\n",
    "   - LLM calls and tokens used\n",
    "   - Execution time per node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b2471",
   "metadata": {},
   "source": [
    "### Option 2: Interactive Studio (Live Development)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1156049a",
   "metadata": {},
   "source": [
    "To interact with your graph in real-time:\n",
    "\n",
    "```bash\n",
    "# In your terminal, run:\n",
    "langgraph dev\n",
    "\n",
    "# This will open in browser:\n",
    "# https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "```\n",
    "\n",
    "The Studio provides:\n",
    "- **Visual graph editor** - See your nodes and edges\n",
    "- **Interactive chat** - Test queries in real-time\n",
    "- **State inspector** - View state after each node\n",
    "- **Thread history** - Browse past conversations by thread_id\n",
    "- **Debugger** - Step through execution node by node\n",
    "\n",
    "**Quick Setup for Studio:**\n",
    "\n",
    "1. Ensure `langgraph.json` exists in project root\n",
    "2. Start the server: `langgraph dev`\n",
    "3. Open https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "4. Click \"New Thread\" and type your question\n",
    "5. Watch the graph execute in real-time!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c4a87",
   "metadata": {},
   "source": [
    "## Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b30df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call (cache MISS):\n",
      "Response: Most commonly, CNN stands for Convolutional Neural Network. Here‚Äôs a concise overview:\n",
      "\n",
      "- What it is...\n",
      "‚è±Ô∏è  Time: 6.929s\n",
      "\n",
      "Second call (cache HIT):\n",
      "Response: Most commonly, CNN stands for Convolutional Neural Network. Here‚Äôs a concise overview:\n",
      "\n",
      "- What it is...\n",
      "‚è±Ô∏è  Time: 0.002s\n",
      "\n",
      "‚úÖ Cache working! Speedup: 4304.5x faster\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.cache import InMemoryCache\n",
    "import time\n",
    "\n",
    "from langchain_core.globals import set_llm_cache\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano\")\n",
    "\n",
    "question = \"What is a CNN?\"\n",
    "\n",
    "# First call - hits the LLM (slow)\n",
    "print(\"First call:\")\n",
    "start = time.time()\n",
    "response1 = llm.invoke(question)\n",
    "time1 = time.time() - start\n",
    "print(f\"Response: {response1.content[:100]}...\")\n",
    "print(f\"‚è±Ô∏è  Time: {time1:.3f}s\\n\")\n",
    "\n",
    "# Second call - returns from cache (fast!)\n",
    "print(\"Second call (cache HIT):\")\n",
    "start = time.time()\n",
    "response2 = llm.invoke(question)\n",
    "time2 = time.time() - start\n",
    "print(f\"Response: {response2.content[:100]}...\")\n",
    "print(f\"‚è±Ô∏è  Time: {time2:.3f}s\\n\")\n",
    "\n",
    "# Compare\n",
    "if time2 < time1 * 0.1:  # Cache should be 10x+ faster\n",
    "    print(f\"‚úÖ Cache working! Speedup: {time1/time2:.1f}x faster\")\n",
    "else:\n",
    "    print(f\"‚ùå Cache might not be working\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scientific-graph-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
