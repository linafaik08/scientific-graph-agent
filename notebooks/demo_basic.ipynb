{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e7de14f",
   "metadata": {},
   "source": [
    "# üß™ Scientific Graph Agent - Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e943173",
   "metadata": {},
   "source": [
    "This notebook demonstrates a simple agent graph for scientific paper exploration using LangGraph + ArXiv API with memory support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e2a928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ced08c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL = \"gpt-5-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa1950f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment variables loaded\n"
     ]
    }
   ],
   "source": [
    "# Verify that API keys are set\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"‚ö†Ô∏è  OPENAI_API_KEY not found in .env file\")\n",
    "\n",
    "print(\"‚úÖ Environment variables loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be7942fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/linafaik/Documents/projects/scientific-graph-agent\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc756288",
   "metadata": {},
   "source": [
    "## Graph Agent Initialization\n",
    "\n",
    "The shared state has been refactored into `InputState`, `OutputState`, and `InternalState` (see `src/agent_graph/state.py`).\n",
    "We now import the graph constructor directly to reflect that structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e7e4d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent graph created\n"
     ]
    }
   ],
   "source": [
    "from src.agent_graph.graph import create_graph\n",
    "\n",
    "# Create graph with custom parameters (state handled via InternalState reducers)\n",
    "graph = create_graph(with_checkpointer=True)\n",
    "\n",
    "print(\"‚úÖ Agent graph created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a01e19",
   "metadata": {},
   "source": [
    "### Graph Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1e2eae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph structure:\n",
      "START ‚Üí clarifier ‚Üí researcher ‚Üí summarizer ‚Üí END\n",
      "           ‚Üë            ‚Üì\n",
      "           ‚îî‚îÄ(if <3 papers)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "    \n",
    "# Generate graph visualization as PNG\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "print(\"Graph structure:\")\n",
    "print(\"START ‚Üí clarifier ‚Üí researcher ‚Üí summarizer ‚Üí END\")\n",
    "print(\"           ‚Üë            ‚Üì\")\n",
    "print(\"           ‚îî‚îÄ(if <3 papers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1090516",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce34676",
   "metadata": {},
   "source": [
    "### Test 1: Simple Research Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d6be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting agent graph execution...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:13:46,214 - INFO - Clarifying query: 'What are the key innovations in transformer architectures?'\n",
      "2025-11-16 12:13:46,226 - INFO - Refined query: 'transformer architecture innovations attention variants positional encodings'\n",
      "2025-11-16 12:13:46,228 - INFO - Searching ArXiv: 'transformer architecture innovations attention variants positional encodings' (iteration 0)\n",
      "2025-11-16 12:13:46,231 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=transformer+architecture+innovations+attention+variants+positional+encodings&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 max_papers\n",
      "max_results tool 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:13:46,847 - INFO - Got first page: 5 of 577858 total results\n",
      "2025-11-16 12:13:46,852 - INFO - Found 5 papers\n",
      "2025-11-16 12:13:46,854 - INFO - Scoring paper relevance...\n",
      "2025-11-16 12:13:46,859 - INFO -   üìÑ Dilated Neighborhood Attention Transformer... - Score: 60\n",
      "2025-11-16 12:13:46,864 - INFO -   üìÑ Trading with the Momentum Transformer: An Intelligent and In... - Score: 35\n",
      "2025-11-16 12:13:46,867 - INFO -   üìÑ Dalorex: A Data-Local Program Execution and Architecture for... - Score: 2\n",
      "2025-11-16 12:13:46,871 - INFO -   üìÑ Architectural Implications of Graph Neural Networks... - Score: 5\n",
      "2025-11-16 12:13:46,873 - INFO -   üìÑ Mask-Attention-Free Transformer for 3D Instance Segmentation... - Score: 35\n",
      "2025-11-16 12:13:46,876 - INFO - üìù Synthesizing 5 papers...\n",
      "2025-11-16 12:13:46,879 - INFO - ‚úÖ Summary generated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary': '## R√©sum√©\\n‚Ä¢ Point 1 ‚Äî Localized and dilated attention improves vision transformers by keeping attention computation tractable while enlarging receptive fields: Dilated Neighborhood Attention restricts attention to local neighborhoods but uses dilation to capture wider context, enabling hierarchical vision transformers to scale with lower compute and memory than full self-attention. This yields better integration into existing vision pipelines and improved efficiency on visual tasks [Paper 1].\\n\\n‚Ä¢ Point 2 ‚Äî Attention applied to time series can replace sequential RNNs with more interpretable, globally connected models: The Momentum Transformer provides direct access to all past timesteps, models momentum-like dynamics, and offers improved predictive performance and interpretability for trading/time-series tasks compared with LSTMs and classical strategies. This shows transformer attention‚Äôs strength at capturing long-range dependencies in non‚Äëvision modalities [Paper 2].\\n\\n‚Ä¢ Point 3 ‚Äî Removing mask-guided cross-attention can speed convergence in instance-segmentation transformers: The Mask-Attention-Free Transformer for 3D instance segmentation identifies slow convergence caused by low-recall initial masks and replaces mask-attention pipelines with alternative query refinement, simplifying the architecture and accelerating training while maintaining competitive instance segmentation performance [Paper 3].\\n\\n‚Ä¢ Point 4 ‚Äî Transformer design gains from hardware- and data-aware architectural thinking: Analyses of graph neural network architectures highlight how irregular data access and algorithm structure interact with system design, and proposals like Dalorex advocate data-local execution to mitigate memory-bound bottlenecks. Together these works emphasize that scaling transformer variants (and related graph/attention models) requires co-design of algorithmic attention patterns and memory/compute architectures to realize practical efficiency gains [Paper 4, Paper 5].\\n\\n## R√©f√©rences\\n[Paper 1] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3\\n\\n[Paper 2] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3\\n\\n[Paper 3] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1\\n\\n[Paper 4] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2\\n\\n[Paper 5] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4',\n",
       " 'papers': [{'id': 'http://arxiv.org/abs/2209.15001v3',\n",
       "   'title': 'Dilated Neighborhood Attention Transformer',\n",
       "   'authors': ['Ali Hassani', 'Humphrey Shi'],\n",
       "   'summary': \"Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).\",\n",
       "   'url': 'http://arxiv.org/abs/2209.15001v3',\n",
       "   'published': '2022-09-29',\n",
       "   'relevance_score': 60},\n",
       "  {'id': 'http://arxiv.org/abs/2112.08534v3',\n",
       "   'title': 'Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture',\n",
       "   'authors': ['Kieran Wood',\n",
       "    'Sven Giegerich',\n",
       "    'Stephen Roberts',\n",
       "    'Stefan Zohren'],\n",
       "   'summary': 'We introduce the Momentum Transformer, an attention-based deep-learning architecture, which outperforms benchmark time-series momentum and mean-reversion trading strategies. Unlike state-of-the-art Long Short-Term Memory (LSTM) architectures, which are sequential in nature and tailored to local processing, an attention mechanism provides our architecture with a direct connection to all previous time-steps. Our architecture, an attention-LSTM hybrid, enables us to learn longer-term dependencies, improves performance when considering returns net of transaction costs and naturally adapts to new market regimes, such as during the SARS-CoV-2 crisis. Via the introduction of multiple attention heads, we can capture concurrent regimes, or temporal dynamics, which are occurring at different timescales. The Momentum Transformer is inherently interpretable, providing us with greater insights into our deep-learning momentum trading strategy, including the importance of different factors over time and the past time-steps which are of the greatest significance to the model.',\n",
       "   'url': 'http://arxiv.org/abs/2112.08534v3',\n",
       "   'published': '2021-12-16',\n",
       "   'relevance_score': 35},\n",
       "  {'id': 'http://arxiv.org/abs/2309.01692v1',\n",
       "   'title': 'Mask-Attention-Free Transformer for 3D Instance Segmentation',\n",
       "   'authors': ['Xin Lai',\n",
       "    'Yuhui Yuan',\n",
       "    'Ruihang Chu',\n",
       "    'Yukang Chen',\n",
       "    'Han Hu',\n",
       "    'Jiaya Jia'],\n",
       "   'summary': 'Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.',\n",
       "   'url': 'http://arxiv.org/abs/2309.01692v1',\n",
       "   'published': '2023-09-04',\n",
       "   'relevance_score': 35},\n",
       "  {'id': 'http://arxiv.org/abs/2009.00804v2',\n",
       "   'title': 'Architectural Implications of Graph Neural Networks',\n",
       "   'authors': ['Zhihui Zhang',\n",
       "    'Jingwen Leng',\n",
       "    'Lingxiao Ma',\n",
       "    'Youshan Miao',\n",
       "    'Chao Li',\n",
       "    'Minyi Guo'],\n",
       "   'summary': 'Graph neural networks (GNN) represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This work tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.',\n",
       "   'url': 'http://arxiv.org/abs/2009.00804v2',\n",
       "   'published': '2020-09-02',\n",
       "   'relevance_score': 5},\n",
       "  {'id': 'http://arxiv.org/abs/2207.13219v4',\n",
       "   'title': 'Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications',\n",
       "   'authors': ['Marcelo Orenes-Vera',\n",
       "    'Esin Tureci',\n",
       "    'David Wentzlaff',\n",
       "    'Margaret Martonosi'],\n",
       "   'summary': 'Applications with low data reuse and frequent irregular memory accesses, such as graph or sparse linear algebra workloads, fail to scale well due to memory bottlenecks and poor core utilization. While prior work with prefetching, decoupling, or pipelining can mitigate memory latency and improve core utilization, memory bottlenecks persist due to limited off-chip bandwidth. Approaches doing processing in-memory (PIM) with Hybrid Memory Cube (HMC) overcome bandwidth limitations but fail to achieve high core utilization due to poor task scheduling and synchronization overheads. Moreover, the high memory-per-core ratio available with HMC limits strong scaling.\\n  We introduce Dalorex, a hardware-software co-design that achieves high parallelism and energy efficiency, demonstrating strong scaling with >16,000 cores when processing graph and sparse linear algebra workloads. Over the prior work in PIM, both using 256 cores, Dalorex improves performance and energy consumption by two orders of magnitude through (1) a tile-based distributed-memory architecture where each processing tile holds an equal amount of data, and all memory operations are local; (2) a task-based parallel programming model where tasks are executed by the processing unit that is co-located with the target data; (3) a network design optimized for irregular traffic, where all communication is one-way, and messages do not contain routing metadata; (4) novel traffic-aware task scheduling hardware that maintains high core utilization; and (5) a data placement strategy that improves work balance.\\n  This work proposes architectural and software innovations to provide the greatest scalability to date for running graph algorithms while still being programmable for other domains.',\n",
       "   'url': 'http://arxiv.org/abs/2207.13219v4',\n",
       "   'published': '2022-07-26',\n",
       "   'relevance_score': 2}],\n",
       " 'messages': [SystemMessage(content='User inquired about the key innovations in transformer architectures, which suggests an interest in advancements in this field.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Refined query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Clarifier'),\n",
       "  AIMessage(content='Found 5 papers on ArXiv for query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Researcher'),\n",
       "  AIMessage(content='## R√©sum√©\\n‚Ä¢ Point 1 ‚Äî Localized and dilated attention improves vision transformers by keeping attention computation tractable while enlarging receptive fields: Dilated Neighborhood Attention restricts attention to local neighborhoods but uses dilation to capture wider context, enabling hierarchical vision transformers to scale with lower compute and memory than full self-attention. This yields better integration into existing vision pipelines and improved efficiency on visual tasks [Paper 1].\\n\\n‚Ä¢ Point 2 ‚Äî Attention applied to time series can replace sequential RNNs with more interpretable, globally connected models: The Momentum Transformer provides direct access to all past timesteps, models momentum-like dynamics, and offers improved predictive performance and interpretability for trading/time-series tasks compared with LSTMs and classical strategies. This shows transformer attention‚Äôs strength at capturing long-range dependencies in non‚Äëvision modalities [Paper 2].\\n\\n‚Ä¢ Point 3 ‚Äî Removing mask-guided cross-attention can speed convergence in instance-segmentation transformers: The Mask-Attention-Free Transformer for 3D instance segmentation identifies slow convergence caused by low-recall initial masks and replaces mask-attention pipelines with alternative query refinement, simplifying the architecture and accelerating training while maintaining competitive instance segmentation performance [Paper 3].\\n\\n‚Ä¢ Point 4 ‚Äî Transformer design gains from hardware- and data-aware architectural thinking: Analyses of graph neural network architectures highlight how irregular data access and algorithm structure interact with system design, and proposals like Dalorex advocate data-local execution to mitigate memory-bound bottlenecks. Together these works emphasize that scaling transformer variants (and related graph/attention models) requires co-design of algorithmic attention patterns and memory/compute architectures to realize practical efficiency gains [Paper 4, Paper 5].\\n\\n## R√©f√©rences\\n[Paper 1] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3\\n\\n[Paper 2] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3\\n\\n[Paper 3] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1\\n\\n[Paper 4] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2\\n\\n[Paper 5] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', additional_kwargs={}, response_metadata={}, name='Summarizer')]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:13:46,994 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=dilated+neighborhood+attention+hierarchical+vision+transformers+efficiency&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=5\n",
      "2025-11-16 12:13:47,602 - INFO - Got first page: 5 of 627815 total results\n",
      "2025-11-16 12:13:47,689 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=quantum+machine+learning+protein+folding+NISQ+hybrid+variational&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=5\n",
      "2025-11-16 12:13:48,804 - INFO - Got first page: 5 of 1017964 total results\n",
      "2025-11-16 12:13:48,903 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=Attention+mechanisms+survey+self-attention+transformers&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=5\n",
      "2025-11-16 12:13:49,399 - INFO - Got first page: 5 of 501174 total results\n",
      "2025-11-16 12:13:53,493 - INFO - Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=self-attention+transformers+survey+theory+efficiency&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=5\n",
      "2025-11-16 12:13:54,288 - INFO - Got first page: 5 of 1141268 total results\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Configuration for this run (needed for memory/checkpointing)\n",
    "config = {\"configurable\": {\"thread_id\": \"demo-thread-1\"}}\n",
    "\n",
    "question = \"What are the key innovations in transformer architectures?\"\n",
    "\n",
    "# Provide required fields directly in state (no nested 'config' dict)\n",
    "initial_state = {\n",
    "    \"query\": question,\n",
    "    \"llm_model\": LLM_MODEL,\n",
    "    \"llm_temperature\": 0,\n",
    "    \"max_papers\": 5,\n",
    "    \"max_iterations\": 2,\n",
    "}\n",
    "\n",
    "print(\"Starting agent graph execution...\\n\")\n",
    "\n",
    "# Invoke the graph\n",
    "result = graph.invoke(initial_state, config=config)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8117bbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL RESULT\n",
      "================================================================================\n",
      "- Papers found: 5\n",
      "\n",
      "## R√©sum√©\n",
      "‚Ä¢ Point 1 ‚Äî Localized and dilated attention improves vision transformers by keeping attention computation tractable while enlarging receptive fields: Dilated Neighborhood Attention restricts attention to local neighborhoods but uses dilation to capture wider context, enabling hierarchical vision transformers to scale with lower compute and memory than full self-attention. This yields better integration into existing vision pipelines and improved efficiency on visual tasks [Paper 1].\n",
      "\n",
      "‚Ä¢ Point 2 ‚Äî Attention applied to time series can replace sequential RNNs with more interpretable, globally connected models: The Momentum Transformer provides direct access to all past timesteps, models momentum-like dynamics, and offers improved predictive performance and interpretability for trading/time-series tasks compared with LSTMs and classical strategies. This shows transformer attention‚Äôs strength at capturing long-range dependencies in non‚Äëvision modalities [Paper 2].\n",
      "\n",
      "‚Ä¢ Point 3 ‚Äî Removing mask-guided cross-attention can speed convergence in instance-segmentation transformers: The Mask-Attention-Free Transformer for 3D instance segmentation identifies slow convergence caused by low-recall initial masks and replaces mask-attention pipelines with alternative query refinement, simplifying the architecture and accelerating training while maintaining competitive instance segmentation performance [Paper 3].\n",
      "\n",
      "‚Ä¢ Point 4 ‚Äî Transformer design gains from hardware- and data-aware architectural thinking: Analyses of graph neural network architectures highlight how irregular data access and algorithm structure interact with system design, and proposals like Dalorex advocate data-local execution to mitigate memory-bound bottlenecks. Together these works emphasize that scaling transformer variants (and related graph/attention models) requires co-design of algorithmic attention patterns and memory/compute architectures to realize practical efficiency gains [Paper 4, Paper 5].\n",
      "\n",
      "## R√©f√©rences\n",
      "[Paper 1] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3\n",
      "\n",
      "[Paper 2] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3\n",
      "\n",
      "[Paper 3] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1\n",
      "\n",
      "[Paper 4] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2\n",
      "\n",
      "[Paper 5] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"- Papers found: {len(result['papers'])}\")\n",
    "print(f\"\\n{result['summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e846657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:13:46,986 - INFO - Clarifying query: 'Can you give more detail regarding your first point?'\n",
      "2025-11-16 12:13:46,989 - INFO - Refined query: 'dilated neighborhood attention hierarchical vision transformers efficiency'\n",
      "2025-11-16 12:13:46,993 - INFO - Searching ArXiv: 'dilated neighborhood attention hierarchical vision transformers efficiency' (iteration 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Asking follow-up question...\n",
      "\n",
      "5 max_papers\n",
      "max_results tool 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:13:47,608 - INFO - Found 5 papers\n",
      "2025-11-16 12:13:47,609 - INFO - Scoring paper relevance...\n",
      "2025-11-16 12:13:47,616 - INFO -   üìÑ Dilated Neighborhood Attention Transformer... - Score: 90\n",
      "2025-11-16 12:13:47,620 - INFO -   üìÑ Vision Transformer with Quadrangle Attention... - Score: 10\n",
      "2025-11-16 12:13:47,622 - INFO -   üìÑ PatchRot: A Self-Supervised Technique for Training Vision Tr... - Score: 90\n",
      "2025-11-16 12:13:47,624 - INFO -   üìÑ Vision Transformer with Cross-attention by Temporal Shift fo... - Score: 85\n",
      "2025-11-16 12:13:47,626 - INFO -   üìÑ Attention Guided CAM: Visual Explanations of Vision Transfor... - Score: 100\n",
      "2025-11-16 12:13:47,632 - INFO - üìù Synthesizing 9 papers...\n",
      "2025-11-16 12:13:47,635 - INFO - ‚úÖ Summary generated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FOLLOW-UP RESULT\n",
      "================================================================================\n",
      "- Papers found: 9\n",
      "\n",
      "## R√©sum√©\n",
      "‚Ä¢ Attention-guided CAMs adapt ViT self-attention to produce class-localization heatmaps by extracting and aggregating attention weights across layers and heads, then projecting that aggregated attention back to image space. In practice one computes per-head attention maps, applies head- and layer-weighting or attention-rollout to estimate token-to-token relevance, and upsamples the resulting relevance to pixels to get a visualization; this typically yields improved localization versus na√Øve gradient-based CAMs but depends on head selection and aggregation heuristics [Paper 1, Paper 2].  \n",
      "‚Ä¢ Localized and dilated neighborhood attention mechanisms reduce compute and enlarge effective receptive fields by restricting attention to spatial neighborhoods or using dilated patterns, which preserves long-range context with lower cost ‚Äî useful both for efficiency and for producing attention maps that reflect multi-scale object structure [Paper 2, Paper 7].  \n",
      "‚Ä¢ Self-supervised schemes like PatchRot shape the representations learned by ViTs without labels, which can improve downstream localization and robustness of attention-based explanations because the model learns useful patch-level features and geometric cues from unlabeled data [Paper 3].  \n",
      "‚Ä¢ Task-specific attention variants (temporal cross-attention, mask-attention-free pipelines, momentum-style attention) adapt the basic ViT attention to video, 3D instance segmentation, or time-series, changing how attention is computed and interpreted and therefore affecting both performance and the semantics of any derived explanations [Paper 4, Paper 6, Paper 5].  \n",
      "\n",
      "## R√©f√©rences\n",
      "[Paper 1] Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention - Saebom Leem, Hyunseok Seo (2024) - http://arxiv.org/abs/2402.04563v1  \n",
      "[Paper 2] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3  \n",
      "[Paper 3] PatchRot: A Self-Supervised Technique for Training Vision Transformers - Sachin Chhabra, Prabal Bijoy Dutta, Hemanth Venkateswara (2022) - http://arxiv.org/abs/2210.15722v1  \n",
      "[Paper 4] Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition - Ryota Hashiguchi, Toru Tamaki (2022) - http://arxiv.org/abs/2204.00452v2  \n",
      "[Paper 5] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3  \n",
      "[Paper 6] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1  \n",
      "[Paper 7] Vision Transformer with Quadrangle Attention - Qiming Zhang, Jing Zhang, Yufei Xu (2023) - http://arxiv.org/abs/2303.15105v1  \n",
      "[Paper 8] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2  \n",
      "[Paper 9] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4\n"
     ]
    }
   ],
   "source": [
    "follow_up_question = \"Can you give more detail regarding your first point?\"\n",
    "\n",
    "# IMPORTANT: Only pass the new query, not the full config\n",
    "# The checkpointer will restore previous state (messages, papers, etc.)\n",
    "follow_up_state = {\n",
    "    \"query\": follow_up_question,\n",
    "}\n",
    "\n",
    "print(\"\\n\\nAsking follow-up question...\\n\")\n",
    "\n",
    "# Use the SAME config with the same thread_id\n",
    "# The checkpointer loads previous messages, papers, config automatically\n",
    "follow_up_result = graph.invoke(follow_up_state, config=config)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FOLLOW-UP RESULT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"- Papers found: {len(follow_up_result['papers'])}\")\n",
    "print(f\"\\n{follow_up_result['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa35b6",
   "metadata": {},
   "source": [
    "### Test 2: Query with Few Results (Triggers Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc56bbad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['summary', 'papers', 'messages'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d83bf138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:13:47,683 - INFO - Clarifying query: 'quantum machine learning for protein folding using NISQ devices'\n",
      "2025-11-16 12:13:47,685 - INFO - Refined query: 'quantum machine learning protein folding NISQ hybrid variational'\n",
      "2025-11-16 12:13:47,688 - INFO - Searching ArXiv: 'quantum machine learning protein folding NISQ hybrid variational' (iteration 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting agent graph execution...\n",
      "\n",
      "5 max_papers\n",
      "max_results tool 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:13:48,806 - INFO - Found 5 papers\n",
      "2025-11-16 12:13:48,807 - INFO - Scoring paper relevance...\n",
      "2025-11-16 12:13:48,812 - INFO -   üìÑ Generalization Error Bound for Quantum Machine Learning in N... - Score: 70\n",
      "2025-11-16 12:13:48,816 - INFO -   üìÑ Changing Data Sources in the Age of Machine Learning for Off... - Score: 1\n",
      "2025-11-16 12:13:48,818 - INFO -   üìÑ DOME: Recommendations for supervised machine learning valida... - Score: 10\n",
      "2025-11-16 12:13:48,820 - INFO -   üìÑ Discrete molecular dynamics studies of the folding of a prot... - Score: 5\n",
      "2025-11-16 12:13:48,822 - INFO -   üìÑ Learning Curves for Decision Making in Supervised Machine Le... - Score: 5\n",
      "2025-11-16 12:13:48,826 - INFO - üìù Synthesizing 10 papers...\n",
      "2025-11-16 12:13:48,829 - INFO - ‚úÖ Summary generated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL RESULT\n",
      "================================================================================\n",
      "- Papers found: 10\n",
      "\n",
      "## R√©sum√©\n",
      "‚Ä¢ Les limites et opportunit√©s du QML en NISQ : les dispositifs NISQ imposent des contraintes fortes (nombre de qubits, profondeur de circuit, bruit) qui affectent la g√©n√©ralisation et la complexit√© d‚Äôapprentissage ; une analyse des bornes d‚Äôerreur de g√©n√©ralisation est donc essentielle pour √©valuer la viabilit√© des mod√®les quantiques pour le repliement de prot√©ines [Paper 5].  \n",
      "‚Ä¢ Tirer parti des architectures classiques comme mod√®les : les prot√©ines se pr√™tent naturellement √† des repr√©sentations en graphe et √† des m√©canismes d‚Äôattention pour capturer les interactions √† longue port√©e ; les r√©sultats et principes d‚Äôarchitecture des GNN et des transformers peuvent guider la conception d‚Äôencodages quantiques et d‚Äôans√§tze variationnels hybrides adapt√©s aux NISQ [Paper 10, Paper 2].  \n",
      "‚Ä¢ Apprentissage auto-supervis√© et interpr√©tabilit√© pour peu de labels : les techniques auto-supervis√©es et les m√©thodes d‚Äôinterpr√©tation de l‚Äôattention r√©duisent la d√©pendance √† de grands jeux √©tiquet√©s et aident √† diagnostiquer les pr√©dictions structurelles, ce qui est crucial pour valider des mod√®les QML appliqu√©s √† la biologie structurale [Paper 3, Paper 1, Paper 9].  \n",
      "‚Ä¢ Recommandations pratiques pour un pipeline hybride QML‚Äìclassique : pr√©entra√Æner des repr√©sentations classiques (GNN/transformer), injecter ces caract√©ristiques dans des circuits variationnels peu profonds, appliquer mitigation du bruit et √©valuer strictement via protocoles de validation biologiques et bornes th√©oriques de g√©n√©ralisation ; privil√©gier modules d‚Äôattention efficaces et architectures √©co-resources pour r√©duire la charge quantique sur NISQ [Paper 5, Paper 7, Paper 9].\n",
      "\n",
      "## R√©f√©rences\n",
      "[Paper 1] Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention - Saebom Leem, Hyunseok Seo (2024) - http://arxiv.org/abs/2402.04563v1\n",
      "\n",
      "[Paper 2] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3\n",
      "\n",
      "[Paper 3] PatchRot: A Self-Supervised Technique for Training Vision Transformers - Sachin Chhabra, Prabal Bijoy Dutta, Hemanth Venkateswara (2022) - http://arxiv.org/abs/2210.15722v1\n",
      "\n",
      "[Paper 4] Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition - Ryota Hashiguchi, Toru Tamaki (2022) - http://arxiv.org/abs/2204.00452v2\n",
      "\n",
      "[Paper 5] Generalization Error Bound for Quantum Machine Learning in NISQ Era -- A Survey - Bikram Khanal, Pablo Rivas, Arun Sanjel (2024) - http://arxiv.org/abs/2409.07626v2\n",
      "\n",
      "[Paper 6] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3\n",
      "\n",
      "[Paper 7] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1\n",
      "\n",
      "[Paper 8] Vision Transformer with Quadrangle Attention - Qiming Zhang, Jing Zhang, Yufei Xu (2023) - http://arxiv.org/abs/2303.15105v1\n",
      "\n",
      "[Paper 9] DOME: Recommendations for supervised machine learning validation in biology - Ian Walsh, Dmytro Fishman, Dario Garcia-Gasulla (2020) - http://arxiv.org/abs/2006.16189v4\n",
      "\n",
      "[Paper 10] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Configuration for this run (needed for memory/checkpointing)\n",
    "config = {\"configurable\": {\"thread_id\": \"demo-thread-1\"}}\n",
    "\n",
    "question = \"quantum machine learning for protein folding using NISQ devices\"\n",
    "\n",
    "initial_state = {\n",
    "    \"query\": question,\n",
    "    \"llm_model\": LLM_MODEL,\n",
    "    \"llm_temperature\": 0,\n",
    "    \"max_papers\": 5,\n",
    "    \"max_iterations\": 2,\n",
    "}\n",
    "\n",
    "print(\"Starting agent graph execution...\\n\")\n",
    "\n",
    "# Invoke the graph\n",
    "result = graph.invoke(initial_state, config=config)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"- Papers found: {len(result['papers'])}\")\n",
    "print(f\"\\n{result['summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c0f64a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StateSnapshot(values={'query': 'quantum machine learning for protein folding using NISQ devices', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2, 'summary': '## R√©sum√©\\n‚Ä¢ Les limites et opportunit√©s du QML en NISQ : les dispositifs NISQ imposent des contraintes fortes (nombre de qubits, profondeur de circuit, bruit) qui affectent la g√©n√©ralisation et la complexit√© d‚Äôapprentissage ; une analyse des bornes d‚Äôerreur de g√©n√©ralisation est donc essentielle pour √©valuer la viabilit√© des mod√®les quantiques pour le repliement de prot√©ines [Paper 5].  \\n‚Ä¢ Tirer parti des architectures classiques comme mod√®les : les prot√©ines se pr√™tent naturellement √† des repr√©sentations en graphe et √† des m√©canismes d‚Äôattention pour capturer les interactions √† longue port√©e ; les r√©sultats et principes d‚Äôarchitecture des GNN et des transformers peuvent guider la conception d‚Äôencodages quantiques et d‚Äôans√§tze variationnels hybrides adapt√©s aux NISQ [Paper 10, Paper 2].  \\n‚Ä¢ Apprentissage auto-supervis√© et interpr√©tabilit√© pour peu de labels : les techniques auto-supervis√©es et les m√©thodes d‚Äôinterpr√©tation de l‚Äôattention r√©duisent la d√©pendance √† de grands jeux √©tiquet√©s et aident √† diagnostiquer les pr√©dictions structurelles, ce qui est crucial pour valider des mod√®les QML appliqu√©s √† la biologie structurale [Paper 3, Paper 1, Paper 9].  \\n‚Ä¢ Recommandations pratiques pour un pipeline hybride QML‚Äìclassique : pr√©entra√Æner des repr√©sentations classiques (GNN/transformer), injecter ces caract√©ristiques dans des circuits variationnels peu profonds, appliquer mitigation du bruit et √©valuer strictement via protocoles de validation biologiques et bornes th√©oriques de g√©n√©ralisation ; privil√©gier modules d‚Äôattention efficaces et architectures √©co-resources pour r√©duire la charge quantique sur NISQ [Paper 5, Paper 7, Paper 9].\\n\\n## R√©f√©rences\\n[Paper 1] Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention - Saebom Leem, Hyunseok Seo (2024) - http://arxiv.org/abs/2402.04563v1\\n\\n[Paper 2] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3\\n\\n[Paper 3] PatchRot: A Self-Supervised Technique for Training Vision Transformers - Sachin Chhabra, Prabal Bijoy Dutta, Hemanth Venkateswara (2022) - http://arxiv.org/abs/2210.15722v1\\n\\n[Paper 4] Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition - Ryota Hashiguchi, Toru Tamaki (2022) - http://arxiv.org/abs/2204.00452v2\\n\\n[Paper 5] Generalization Error Bound for Quantum Machine Learning in NISQ Era -- A Survey - Bikram Khanal, Pablo Rivas, Arun Sanjel (2024) - http://arxiv.org/abs/2409.07626v2\\n\\n[Paper 6] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3\\n\\n[Paper 7] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1\\n\\n[Paper 8] Vision Transformer with Quadrangle Attention - Qiming Zhang, Jing Zhang, Yufei Xu (2023) - http://arxiv.org/abs/2303.15105v1\\n\\n[Paper 9] DOME: Recommendations for supervised machine learning validation in biology - Ian Walsh, Dmytro Fishman, Dario Garcia-Gasulla (2020) - http://arxiv.org/abs/2006.16189v4\\n\\n[Paper 10] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2', 'papers': [{'id': 'http://arxiv.org/abs/2402.04563v1', 'title': 'Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention', 'authors': ['Saebom Leem', 'Hyunseok Seo'], 'summary': 'Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.', 'url': 'http://arxiv.org/abs/2402.04563v1', 'published': '2024-02-07', 'relevance_score': 100}, {'id': 'http://arxiv.org/abs/2209.15001v3', 'title': 'Dilated Neighborhood Attention Transformer', 'authors': ['Ali Hassani', 'Humphrey Shi'], 'summary': \"Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).\", 'url': 'http://arxiv.org/abs/2209.15001v3', 'published': '2022-09-29', 'relevance_score': 90}, {'id': 'http://arxiv.org/abs/2210.15722v1', 'title': 'PatchRot: A Self-Supervised Technique for Training Vision Transformers', 'authors': ['Sachin Chhabra', 'Prabal Bijoy Dutta', 'Hemanth Venkateswara', 'Baoxin Li'], 'summary': 'Vision transformers require a huge amount of labeled data to outperform convolutional neural networks. However, labeling a huge dataset is a very expensive process. Self-supervised learning techniques alleviate this problem by learning features similar to supervised learning in an unsupervised way. In this paper, we propose a self-supervised technique PatchRot that is crafted for vision transformers. PatchRot rotates images and image patches and trains the network to predict the rotation angles. The network learns to extract both global and local features from an image. Our extensive experiments on different datasets showcase PatchRot training learns rich features which outperform supervised learning and compared baseline.', 'url': 'http://arxiv.org/abs/2210.15722v1', 'published': '2022-10-27', 'relevance_score': 90}, {'id': 'http://arxiv.org/abs/2204.00452v2', 'title': 'Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition', 'authors': ['Ryota Hashiguchi', 'Toru Tamaki'], 'summary': 'Feature shifts have been shown to be useful for action recognition with CNN-based models since Temporal Shift Module (TSM) was proposed. It is based on frame-wise feature extraction with late fusion, and layer features are shifted along the time direction for the temporal interaction. TokenShift, a recent model based on Vision Transformer (ViT), also uses the temporal feature shift mechanism, which, however, does not fully exploit the structure of Multi-head Self-Attention (MSA) in ViT. In this paper, we propose Multi-head Self/Cross-Attention (MSCA), which fully utilizes the attention structure. TokenShift is based on a frame-wise ViT with features temporally shifted with successive frames (at time t+1 and t-1). In contrast, the proposed MSCA replaces MSA in the frame-wise ViT, and some MSA heads attend to successive frames instead of the current frame. The computation cost is the same as the frame-wise ViT and TokenShift as it simply changes the target to which the attention is taken. There is a choice about which of key, query, and value are taken from the successive frames, then we experimentally compared these variants with Kinetics400. We also investigate other variants in which the proposed MSCA is used along the patch dimension of ViT, instead of the head dimension. Experimental results show that a variant, MSCA-KV, shows the best performance and is better than TokenShift by 0.1% and then ViT by 1.2%.', 'url': 'http://arxiv.org/abs/2204.00452v2', 'published': '2022-04-01', 'relevance_score': 85}, {'id': 'http://arxiv.org/abs/2409.07626v2', 'title': 'Generalization Error Bound for Quantum Machine Learning in NISQ Era -- A Survey', 'authors': ['Bikram Khanal', 'Pablo Rivas', 'Arun Sanjel', 'Korn Sooksatra', 'Ernesto Quevedo', 'Alejandro Rodriguez'], 'summary': 'Despite the mounting anticipation for the quantum revolution, the success of Quantum Machine Learning (QML) in the Noisy Intermediate-Scale Quantum (NISQ) era hinges on a largely unexplored factor: the generalization error bound, a cornerstone of robust and reliable machine learning models. Current QML research, while exploring novel algorithms and applications extensively, is predominantly situated in the context of noise-free, ideal quantum computers. However, Quantum Circuit (QC) operations in NISQ-era devices are susceptible to various noise sources and errors. In this article, we conduct a Systematic Mapping Study (SMS) to explore the state-of-the-art generalization bound for supervised QML in NISQ-era and analyze the latest practices in the field. Our study systematically summarizes the existing computational platforms with quantum hardware, datasets, optimization techniques, and the common properties of the bounds found in the literature. We further present the performance accuracy of various approaches in classical benchmark datasets like the MNIST and IRIS datasets. The SMS also highlights the limitations and challenges in QML in the NISQ era and discusses future research directions to advance the field. Using a detailed Boolean operators query in five reliable indexers, we collected 544 papers and filtered them to a small set of 37 relevant articles. This filtration was done following the best practice of SMS with well-defined research questions and inclusion and exclusion criteria.', 'url': 'http://arxiv.org/abs/2409.07626v2', 'published': '2024-09-11', 'relevance_score': 70}, {'id': 'http://arxiv.org/abs/2112.08534v3', 'title': 'Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture', 'authors': ['Kieran Wood', 'Sven Giegerich', 'Stephen Roberts', 'Stefan Zohren'], 'summary': 'We introduce the Momentum Transformer, an attention-based deep-learning architecture, which outperforms benchmark time-series momentum and mean-reversion trading strategies. Unlike state-of-the-art Long Short-Term Memory (LSTM) architectures, which are sequential in nature and tailored to local processing, an attention mechanism provides our architecture with a direct connection to all previous time-steps. Our architecture, an attention-LSTM hybrid, enables us to learn longer-term dependencies, improves performance when considering returns net of transaction costs and naturally adapts to new market regimes, such as during the SARS-CoV-2 crisis. Via the introduction of multiple attention heads, we can capture concurrent regimes, or temporal dynamics, which are occurring at different timescales. The Momentum Transformer is inherently interpretable, providing us with greater insights into our deep-learning momentum trading strategy, including the importance of different factors over time and the past time-steps which are of the greatest significance to the model.', 'url': 'http://arxiv.org/abs/2112.08534v3', 'published': '2021-12-16', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2309.01692v1', 'title': 'Mask-Attention-Free Transformer for 3D Instance Segmentation', 'authors': ['Xin Lai', 'Yuhui Yuan', 'Ruihang Chu', 'Yukang Chen', 'Han Hu', 'Jiaya Jia'], 'summary': 'Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.', 'url': 'http://arxiv.org/abs/2309.01692v1', 'published': '2023-09-04', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2303.15105v1', 'title': 'Vision Transformer with Quadrangle Attention', 'authors': ['Qiming Zhang', 'Jing Zhang', 'Yufei Xu', 'Dacheng Tao'], 'summary': 'Window-based attention has become a popular choice in vision transformers due to its superior performance, lower computational complexity, and less memory footprint. However, the design of hand-crafted windows, which is data-agnostic, constrains the flexibility of transformers to adapt to objects of varying sizes, shapes, and orientations. To address this issue, we propose a novel quadrangle attention (QA) method that extends the window-based attention to a general quadrangle formulation. Our method employs an end-to-end learnable quadrangle regression module that predicts a transformation matrix to transform default windows into target quadrangles for token sampling and attention calculation, enabling the network to model various targets with different shapes and orientations and capture rich context information. We integrate QA into plain and hierarchical vision transformers to create a new architecture named QFormer, which offers minor code modifications and negligible extra computational cost. Extensive experiments on public benchmarks demonstrate that QFormer outperforms existing representative vision transformers on various vision tasks, including classification, object detection, semantic segmentation, and pose estimation. The code will be made publicly available at \\\\href{https://github.com/ViTAE-Transformer/QFormer}{QFormer}.', 'url': 'http://arxiv.org/abs/2303.15105v1', 'published': '2023-03-27', 'relevance_score': 10}, {'id': 'http://arxiv.org/abs/2006.16189v4', 'title': 'DOME: Recommendations for supervised machine learning validation in biology', 'authors': ['Ian Walsh', 'Dmytro Fishman', 'Dario Garcia-Gasulla', 'Tiina Titma', 'Gianluca Pollastri', 'The ELIXIR Machine Learning focus group', 'Jen Harrow', 'Fotis E. Psomopoulos', 'Silvio C. E. Tosatto'], 'summary': 'Modern biology frequently relies on machine learning to provide predictions and improve decision processes. There have been recent calls for more scrutiny on machine learning performance and possible limitations. Here we present a set of community-wide recommendations aiming to help establish standards of supervised machine learning validation in biology. Adopting a structured methods description for machine learning based on data, optimization, model, evaluation (DOME) will aim to help both reviewers and readers to better understand and assess the performance and limitations of a method or outcome. The recommendations are formulated as questions to anyone wishing to pursue implementation of a machine learning algorithm. Answers to these questions can be easily included in the supplementary material of published papers.', 'url': 'http://arxiv.org/abs/2006.16189v4', 'published': '2020-06-25', 'relevance_score': 10}, {'id': 'http://arxiv.org/abs/2009.00804v2', 'title': 'Architectural Implications of Graph Neural Networks', 'authors': ['Zhihui Zhang', 'Jingwen Leng', 'Lingxiao Ma', 'Youshan Miao', 'Chao Li', 'Minyi Guo'], 'summary': 'Graph neural networks (GNN) represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This work tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.', 'url': 'http://arxiv.org/abs/2009.00804v2', 'published': '2020-09-02', 'relevance_score': 5}], 'messages': [SystemMessage(content='The user inquired about the application of quantum machine learning in protein folding using Noisy Intermediate-Scale Quantum (NISQ) devices.', additional_kwargs={}, response_metadata={}), AIMessage(content='Refined query: quantum machine learning protein folding NISQ hybrid variational', additional_kwargs={}, response_metadata={}, name='Clarifier'), AIMessage(content='Found 5 papers on ArXiv for query: quantum machine learning protein folding NISQ hybrid variational', additional_kwargs={}, response_metadata={}, name='Researcher'), AIMessage(content='## R√©sum√©\\n‚Ä¢ Les limites et opportunit√©s du QML en NISQ : les dispositifs NISQ imposent des contraintes fortes (nombre de qubits, profondeur de circuit, bruit) qui affectent la g√©n√©ralisation et la complexit√© d‚Äôapprentissage ; une analyse des bornes d‚Äôerreur de g√©n√©ralisation est donc essentielle pour √©valuer la viabilit√© des mod√®les quantiques pour le repliement de prot√©ines [Paper 5].  \\n‚Ä¢ Tirer parti des architectures classiques comme mod√®les : les prot√©ines se pr√™tent naturellement √† des repr√©sentations en graphe et √† des m√©canismes d‚Äôattention pour capturer les interactions √† longue port√©e ; les r√©sultats et principes d‚Äôarchitecture des GNN et des transformers peuvent guider la conception d‚Äôencodages quantiques et d‚Äôans√§tze variationnels hybrides adapt√©s aux NISQ [Paper 10, Paper 2].  \\n‚Ä¢ Apprentissage auto-supervis√© et interpr√©tabilit√© pour peu de labels : les techniques auto-supervis√©es et les m√©thodes d‚Äôinterpr√©tation de l‚Äôattention r√©duisent la d√©pendance √† de grands jeux √©tiquet√©s et aident √† diagnostiquer les pr√©dictions structurelles, ce qui est crucial pour valider des mod√®les QML appliqu√©s √† la biologie structurale [Paper 3, Paper 1, Paper 9].  \\n‚Ä¢ Recommandations pratiques pour un pipeline hybride QML‚Äìclassique : pr√©entra√Æner des repr√©sentations classiques (GNN/transformer), injecter ces caract√©ristiques dans des circuits variationnels peu profonds, appliquer mitigation du bruit et √©valuer strictement via protocoles de validation biologiques et bornes th√©oriques de g√©n√©ralisation ; privil√©gier modules d‚Äôattention efficaces et architectures √©co-resources pour r√©duire la charge quantique sur NISQ [Paper 5, Paper 7, Paper 9].\\n\\n## R√©f√©rences\\n[Paper 1] Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention - Saebom Leem, Hyunseok Seo (2024) - http://arxiv.org/abs/2402.04563v1\\n\\n[Paper 2] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3\\n\\n[Paper 3] PatchRot: A Self-Supervised Technique for Training Vision Transformers - Sachin Chhabra, Prabal Bijoy Dutta, Hemanth Venkateswara (2022) - http://arxiv.org/abs/2210.15722v1\\n\\n[Paper 4] Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition - Ryota Hashiguchi, Toru Tamaki (2022) - http://arxiv.org/abs/2204.00452v2\\n\\n[Paper 5] Generalization Error Bound for Quantum Machine Learning in NISQ Era -- A Survey - Bikram Khanal, Pablo Rivas, Arun Sanjel (2024) - http://arxiv.org/abs/2409.07626v2\\n\\n[Paper 6] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3\\n\\n[Paper 7] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1\\n\\n[Paper 8] Vision Transformer with Quadrangle Attention - Qiming Zhang, Jing Zhang, Yufei Xu (2023) - http://arxiv.org/abs/2303.15105v1\\n\\n[Paper 9] DOME: Recommendations for supervised machine learning validation in biology - Ian Walsh, Dmytro Fishman, Dario Garcia-Gasulla (2020) - http://arxiv.org/abs/2006.16189v4\\n\\n[Paper 10] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2', additional_kwargs={}, response_metadata={}, name='Summarizer')], 'refined_query': 'quantum machine learning protein folding NISQ hybrid variational', 'iteration': 1}, next=(), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-339e-60f0-800d-5aece1686381'}}, metadata={'source': 'loop', 'step': 13, 'parents': {}}, created_at='2025-11-16T11:13:48.834830+00:00', parent_config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-3387-6224-800c-c7ed3f69b45c'}}, tasks=(), interrupts=()),\n",
       " StateSnapshot(values={'query': 'quantum machine learning for protein folding using NISQ devices', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2, 'summary': '## R√©sum√©\\n‚Ä¢ Attention-guided CAMs adapt ViT self-attention to produce class-localization heatmaps by extracting and aggregating attention weights across layers and heads, then projecting that aggregated attention back to image space. In practice one computes per-head attention maps, applies head- and layer-weighting or attention-rollout to estimate token-to-token relevance, and upsamples the resulting relevance to pixels to get a visualization; this typically yields improved localization versus na√Øve gradient-based CAMs but depends on head selection and aggregation heuristics [Paper 1, Paper 2].  \\n‚Ä¢ Localized and dilated neighborhood attention mechanisms reduce compute and enlarge effective receptive fields by restricting attention to spatial neighborhoods or using dilated patterns, which preserves long-range context with lower cost ‚Äî useful both for efficiency and for producing attention maps that reflect multi-scale object structure [Paper 2, Paper 7].  \\n‚Ä¢ Self-supervised schemes like PatchRot shape the representations learned by ViTs without labels, which can improve downstream localization and robustness of attention-based explanations because the model learns useful patch-level features and geometric cues from unlabeled data [Paper 3].  \\n‚Ä¢ Task-specific attention variants (temporal cross-attention, mask-attention-free pipelines, momentum-style attention) adapt the basic ViT attention to video, 3D instance segmentation, or time-series, changing how attention is computed and interpreted and therefore affecting both performance and the semantics of any derived explanations [Paper 4, Paper 6, Paper 5].  \\n\\n## R√©f√©rences\\n[Paper 1] Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention - Saebom Leem, Hyunseok Seo (2024) - http://arxiv.org/abs/2402.04563v1  \\n[Paper 2] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3  \\n[Paper 3] PatchRot: A Self-Supervised Technique for Training Vision Transformers - Sachin Chhabra, Prabal Bijoy Dutta, Hemanth Venkateswara (2022) - http://arxiv.org/abs/2210.15722v1  \\n[Paper 4] Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition - Ryota Hashiguchi, Toru Tamaki (2022) - http://arxiv.org/abs/2204.00452v2  \\n[Paper 5] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3  \\n[Paper 6] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1  \\n[Paper 7] Vision Transformer with Quadrangle Attention - Qiming Zhang, Jing Zhang, Yufei Xu (2023) - http://arxiv.org/abs/2303.15105v1  \\n[Paper 8] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2  \\n[Paper 9] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', 'papers': [{'id': 'http://arxiv.org/abs/2402.04563v1', 'title': 'Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention', 'authors': ['Saebom Leem', 'Hyunseok Seo'], 'summary': 'Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.', 'url': 'http://arxiv.org/abs/2402.04563v1', 'published': '2024-02-07', 'relevance_score': 100}, {'id': 'http://arxiv.org/abs/2209.15001v3', 'title': 'Dilated Neighborhood Attention Transformer', 'authors': ['Ali Hassani', 'Humphrey Shi'], 'summary': \"Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).\", 'url': 'http://arxiv.org/abs/2209.15001v3', 'published': '2022-09-29', 'relevance_score': 90}, {'id': 'http://arxiv.org/abs/2210.15722v1', 'title': 'PatchRot: A Self-Supervised Technique for Training Vision Transformers', 'authors': ['Sachin Chhabra', 'Prabal Bijoy Dutta', 'Hemanth Venkateswara', 'Baoxin Li'], 'summary': 'Vision transformers require a huge amount of labeled data to outperform convolutional neural networks. However, labeling a huge dataset is a very expensive process. Self-supervised learning techniques alleviate this problem by learning features similar to supervised learning in an unsupervised way. In this paper, we propose a self-supervised technique PatchRot that is crafted for vision transformers. PatchRot rotates images and image patches and trains the network to predict the rotation angles. The network learns to extract both global and local features from an image. Our extensive experiments on different datasets showcase PatchRot training learns rich features which outperform supervised learning and compared baseline.', 'url': 'http://arxiv.org/abs/2210.15722v1', 'published': '2022-10-27', 'relevance_score': 90}, {'id': 'http://arxiv.org/abs/2204.00452v2', 'title': 'Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition', 'authors': ['Ryota Hashiguchi', 'Toru Tamaki'], 'summary': 'Feature shifts have been shown to be useful for action recognition with CNN-based models since Temporal Shift Module (TSM) was proposed. It is based on frame-wise feature extraction with late fusion, and layer features are shifted along the time direction for the temporal interaction. TokenShift, a recent model based on Vision Transformer (ViT), also uses the temporal feature shift mechanism, which, however, does not fully exploit the structure of Multi-head Self-Attention (MSA) in ViT. In this paper, we propose Multi-head Self/Cross-Attention (MSCA), which fully utilizes the attention structure. TokenShift is based on a frame-wise ViT with features temporally shifted with successive frames (at time t+1 and t-1). In contrast, the proposed MSCA replaces MSA in the frame-wise ViT, and some MSA heads attend to successive frames instead of the current frame. The computation cost is the same as the frame-wise ViT and TokenShift as it simply changes the target to which the attention is taken. There is a choice about which of key, query, and value are taken from the successive frames, then we experimentally compared these variants with Kinetics400. We also investigate other variants in which the proposed MSCA is used along the patch dimension of ViT, instead of the head dimension. Experimental results show that a variant, MSCA-KV, shows the best performance and is better than TokenShift by 0.1% and then ViT by 1.2%.', 'url': 'http://arxiv.org/abs/2204.00452v2', 'published': '2022-04-01', 'relevance_score': 85}, {'id': 'http://arxiv.org/abs/2409.07626v2', 'title': 'Generalization Error Bound for Quantum Machine Learning in NISQ Era -- A Survey', 'authors': ['Bikram Khanal', 'Pablo Rivas', 'Arun Sanjel', 'Korn Sooksatra', 'Ernesto Quevedo', 'Alejandro Rodriguez'], 'summary': 'Despite the mounting anticipation for the quantum revolution, the success of Quantum Machine Learning (QML) in the Noisy Intermediate-Scale Quantum (NISQ) era hinges on a largely unexplored factor: the generalization error bound, a cornerstone of robust and reliable machine learning models. Current QML research, while exploring novel algorithms and applications extensively, is predominantly situated in the context of noise-free, ideal quantum computers. However, Quantum Circuit (QC) operations in NISQ-era devices are susceptible to various noise sources and errors. In this article, we conduct a Systematic Mapping Study (SMS) to explore the state-of-the-art generalization bound for supervised QML in NISQ-era and analyze the latest practices in the field. Our study systematically summarizes the existing computational platforms with quantum hardware, datasets, optimization techniques, and the common properties of the bounds found in the literature. We further present the performance accuracy of various approaches in classical benchmark datasets like the MNIST and IRIS datasets. The SMS also highlights the limitations and challenges in QML in the NISQ era and discusses future research directions to advance the field. Using a detailed Boolean operators query in five reliable indexers, we collected 544 papers and filtered them to a small set of 37 relevant articles. This filtration was done following the best practice of SMS with well-defined research questions and inclusion and exclusion criteria.', 'url': 'http://arxiv.org/abs/2409.07626v2', 'published': '2024-09-11', 'relevance_score': 70}, {'id': 'http://arxiv.org/abs/2112.08534v3', 'title': 'Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture', 'authors': ['Kieran Wood', 'Sven Giegerich', 'Stephen Roberts', 'Stefan Zohren'], 'summary': 'We introduce the Momentum Transformer, an attention-based deep-learning architecture, which outperforms benchmark time-series momentum and mean-reversion trading strategies. Unlike state-of-the-art Long Short-Term Memory (LSTM) architectures, which are sequential in nature and tailored to local processing, an attention mechanism provides our architecture with a direct connection to all previous time-steps. Our architecture, an attention-LSTM hybrid, enables us to learn longer-term dependencies, improves performance when considering returns net of transaction costs and naturally adapts to new market regimes, such as during the SARS-CoV-2 crisis. Via the introduction of multiple attention heads, we can capture concurrent regimes, or temporal dynamics, which are occurring at different timescales. The Momentum Transformer is inherently interpretable, providing us with greater insights into our deep-learning momentum trading strategy, including the importance of different factors over time and the past time-steps which are of the greatest significance to the model.', 'url': 'http://arxiv.org/abs/2112.08534v3', 'published': '2021-12-16', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2309.01692v1', 'title': 'Mask-Attention-Free Transformer for 3D Instance Segmentation', 'authors': ['Xin Lai', 'Yuhui Yuan', 'Ruihang Chu', 'Yukang Chen', 'Han Hu', 'Jiaya Jia'], 'summary': 'Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.', 'url': 'http://arxiv.org/abs/2309.01692v1', 'published': '2023-09-04', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2303.15105v1', 'title': 'Vision Transformer with Quadrangle Attention', 'authors': ['Qiming Zhang', 'Jing Zhang', 'Yufei Xu', 'Dacheng Tao'], 'summary': 'Window-based attention has become a popular choice in vision transformers due to its superior performance, lower computational complexity, and less memory footprint. However, the design of hand-crafted windows, which is data-agnostic, constrains the flexibility of transformers to adapt to objects of varying sizes, shapes, and orientations. To address this issue, we propose a novel quadrangle attention (QA) method that extends the window-based attention to a general quadrangle formulation. Our method employs an end-to-end learnable quadrangle regression module that predicts a transformation matrix to transform default windows into target quadrangles for token sampling and attention calculation, enabling the network to model various targets with different shapes and orientations and capture rich context information. We integrate QA into plain and hierarchical vision transformers to create a new architecture named QFormer, which offers minor code modifications and negligible extra computational cost. Extensive experiments on public benchmarks demonstrate that QFormer outperforms existing representative vision transformers on various vision tasks, including classification, object detection, semantic segmentation, and pose estimation. The code will be made publicly available at \\\\href{https://github.com/ViTAE-Transformer/QFormer}{QFormer}.', 'url': 'http://arxiv.org/abs/2303.15105v1', 'published': '2023-03-27', 'relevance_score': 10}, {'id': 'http://arxiv.org/abs/2006.16189v4', 'title': 'DOME: Recommendations for supervised machine learning validation in biology', 'authors': ['Ian Walsh', 'Dmytro Fishman', 'Dario Garcia-Gasulla', 'Tiina Titma', 'Gianluca Pollastri', 'The ELIXIR Machine Learning focus group', 'Jen Harrow', 'Fotis E. Psomopoulos', 'Silvio C. E. Tosatto'], 'summary': 'Modern biology frequently relies on machine learning to provide predictions and improve decision processes. There have been recent calls for more scrutiny on machine learning performance and possible limitations. Here we present a set of community-wide recommendations aiming to help establish standards of supervised machine learning validation in biology. Adopting a structured methods description for machine learning based on data, optimization, model, evaluation (DOME) will aim to help both reviewers and readers to better understand and assess the performance and limitations of a method or outcome. The recommendations are formulated as questions to anyone wishing to pursue implementation of a machine learning algorithm. Answers to these questions can be easily included in the supplementary material of published papers.', 'url': 'http://arxiv.org/abs/2006.16189v4', 'published': '2020-06-25', 'relevance_score': 10}, {'id': 'http://arxiv.org/abs/2009.00804v2', 'title': 'Architectural Implications of Graph Neural Networks', 'authors': ['Zhihui Zhang', 'Jingwen Leng', 'Lingxiao Ma', 'Youshan Miao', 'Chao Li', 'Minyi Guo'], 'summary': 'Graph neural networks (GNN) represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This work tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.', 'url': 'http://arxiv.org/abs/2009.00804v2', 'published': '2020-09-02', 'relevance_score': 5}], 'messages': [SystemMessage(content='The user asked about innovations in vision transformers, focusing on efficiency improvements. A researcher provided five relevant ArXiv papers discussing topics such as dilated neighborhood attention in hierarchical vision transformers, self-supervised training techniques, and task-specific attention variants that adapt the attention mechanisms for different applications. These contributions aim to enhance performance while reducing computational costs and improving the interpretability of attention-based explanations.', additional_kwargs={}, response_metadata={}), HumanMessage(content='quantum machine learning for protein folding using NISQ devices', additional_kwargs={}, response_metadata={}, name='User'), AIMessage(content='Refined query: quantum machine learning protein folding NISQ hybrid variational', additional_kwargs={}, response_metadata={}, name='Clarifier'), AIMessage(content='Found 5 papers on ArXiv for query: quantum machine learning protein folding NISQ hybrid variational', additional_kwargs={}, response_metadata={}, name='Researcher')], 'refined_query': 'quantum machine learning protein folding NISQ hybrid variational', 'iteration': 1}, next=('summarizer',), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-3387-6224-800c-c7ed3f69b45c'}}, metadata={'source': 'loop', 'step': 12, 'parents': {}}, created_at='2025-11-16T11:13:48.825442+00:00', parent_config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-28af-65e0-800b-654b960a6770'}}, tasks=(PregelTask(id='3d308850-21ac-cf94-f11c-95197e78f964', name='summarizer', path=('__pregel_pull', 'summarizer'), error=None, interrupts=(), state=None, result={'summary': '## R√©sum√©\\n‚Ä¢ Les limites et opportunit√©s du QML en NISQ : les dispositifs NISQ imposent des contraintes fortes (nombre de qubits, profondeur de circuit, bruit) qui affectent la g√©n√©ralisation et la complexit√© d‚Äôapprentissage ; une analyse des bornes d‚Äôerreur de g√©n√©ralisation est donc essentielle pour √©valuer la viabilit√© des mod√®les quantiques pour le repliement de prot√©ines [Paper 5].  \\n‚Ä¢ Tirer parti des architectures classiques comme mod√®les : les prot√©ines se pr√™tent naturellement √† des repr√©sentations en graphe et √† des m√©canismes d‚Äôattention pour capturer les interactions √† longue port√©e ; les r√©sultats et principes d‚Äôarchitecture des GNN et des transformers peuvent guider la conception d‚Äôencodages quantiques et d‚Äôans√§tze variationnels hybrides adapt√©s aux NISQ [Paper 10, Paper 2].  \\n‚Ä¢ Apprentissage auto-supervis√© et interpr√©tabilit√© pour peu de labels : les techniques auto-supervis√©es et les m√©thodes d‚Äôinterpr√©tation de l‚Äôattention r√©duisent la d√©pendance √† de grands jeux √©tiquet√©s et aident √† diagnostiquer les pr√©dictions structurelles, ce qui est crucial pour valider des mod√®les QML appliqu√©s √† la biologie structurale [Paper 3, Paper 1, Paper 9].  \\n‚Ä¢ Recommandations pratiques pour un pipeline hybride QML‚Äìclassique : pr√©entra√Æner des repr√©sentations classiques (GNN/transformer), injecter ces caract√©ristiques dans des circuits variationnels peu profonds, appliquer mitigation du bruit et √©valuer strictement via protocoles de validation biologiques et bornes th√©oriques de g√©n√©ralisation ; privil√©gier modules d‚Äôattention efficaces et architectures √©co-resources pour r√©duire la charge quantique sur NISQ [Paper 5, Paper 7, Paper 9].\\n\\n## R√©f√©rences\\n[Paper 1] Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention - Saebom Leem, Hyunseok Seo (2024) - http://arxiv.org/abs/2402.04563v1\\n\\n[Paper 2] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3\\n\\n[Paper 3] PatchRot: A Self-Supervised Technique for Training Vision Transformers - Sachin Chhabra, Prabal Bijoy Dutta, Hemanth Venkateswara (2022) - http://arxiv.org/abs/2210.15722v1\\n\\n[Paper 4] Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition - Ryota Hashiguchi, Toru Tamaki (2022) - http://arxiv.org/abs/2204.00452v2\\n\\n[Paper 5] Generalization Error Bound for Quantum Machine Learning in NISQ Era -- A Survey - Bikram Khanal, Pablo Rivas, Arun Sanjel (2024) - http://arxiv.org/abs/2409.07626v2\\n\\n[Paper 6] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3\\n\\n[Paper 7] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1\\n\\n[Paper 8] Vision Transformer with Quadrangle Attention - Qiming Zhang, Jing Zhang, Yufei Xu (2023) - http://arxiv.org/abs/2303.15105v1\\n\\n[Paper 9] DOME: Recommendations for supervised machine learning validation in biology - Ian Walsh, Dmytro Fishman, Dario Garcia-Gasulla (2020) - http://arxiv.org/abs/2006.16189v4\\n\\n[Paper 10] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2', 'messages': [AIMessage(content='## R√©sum√©\\n‚Ä¢ Les limites et opportunit√©s du QML en NISQ : les dispositifs NISQ imposent des contraintes fortes (nombre de qubits, profondeur de circuit, bruit) qui affectent la g√©n√©ralisation et la complexit√© d‚Äôapprentissage ; une analyse des bornes d‚Äôerreur de g√©n√©ralisation est donc essentielle pour √©valuer la viabilit√© des mod√®les quantiques pour le repliement de prot√©ines [Paper 5].  \\n‚Ä¢ Tirer parti des architectures classiques comme mod√®les : les prot√©ines se pr√™tent naturellement √† des repr√©sentations en graphe et √† des m√©canismes d‚Äôattention pour capturer les interactions √† longue port√©e ; les r√©sultats et principes d‚Äôarchitecture des GNN et des transformers peuvent guider la conception d‚Äôencodages quantiques et d‚Äôans√§tze variationnels hybrides adapt√©s aux NISQ [Paper 10, Paper 2].  \\n‚Ä¢ Apprentissage auto-supervis√© et interpr√©tabilit√© pour peu de labels : les techniques auto-supervis√©es et les m√©thodes d‚Äôinterpr√©tation de l‚Äôattention r√©duisent la d√©pendance √† de grands jeux √©tiquet√©s et aident √† diagnostiquer les pr√©dictions structurelles, ce qui est crucial pour valider des mod√®les QML appliqu√©s √† la biologie structurale [Paper 3, Paper 1, Paper 9].  \\n‚Ä¢ Recommandations pratiques pour un pipeline hybride QML‚Äìclassique : pr√©entra√Æner des repr√©sentations classiques (GNN/transformer), injecter ces caract√©ristiques dans des circuits variationnels peu profonds, appliquer mitigation du bruit et √©valuer strictement via protocoles de validation biologiques et bornes th√©oriques de g√©n√©ralisation ; privil√©gier modules d‚Äôattention efficaces et architectures √©co-resources pour r√©duire la charge quantique sur NISQ [Paper 5, Paper 7, Paper 9].\\n\\n## R√©f√©rences\\n[Paper 1] Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention - Saebom Leem, Hyunseok Seo (2024) - http://arxiv.org/abs/2402.04563v1\\n\\n[Paper 2] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3\\n\\n[Paper 3] PatchRot: A Self-Supervised Technique for Training Vision Transformers - Sachin Chhabra, Prabal Bijoy Dutta, Hemanth Venkateswara (2022) - http://arxiv.org/abs/2210.15722v1\\n\\n[Paper 4] Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition - Ryota Hashiguchi, Toru Tamaki (2022) - http://arxiv.org/abs/2204.00452v2\\n\\n[Paper 5] Generalization Error Bound for Quantum Machine Learning in NISQ Era -- A Survey - Bikram Khanal, Pablo Rivas, Arun Sanjel (2024) - http://arxiv.org/abs/2409.07626v2\\n\\n[Paper 6] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3\\n\\n[Paper 7] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1\\n\\n[Paper 8] Vision Transformer with Quadrangle Attention - Qiming Zhang, Jing Zhang, Yufei Xu (2023) - http://arxiv.org/abs/2303.15105v1\\n\\n[Paper 9] DOME: Recommendations for supervised machine learning validation in biology - Ian Walsh, Dmytro Fishman, Dario Garcia-Gasulla (2020) - http://arxiv.org/abs/2006.16189v4\\n\\n[Paper 10] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2', additional_kwargs={}, response_metadata={}, name='Summarizer')]}),), interrupts=()),\n",
       " StateSnapshot(values={'query': 'quantum machine learning for protein folding using NISQ devices', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2, 'summary': '## R√©sum√©\\n‚Ä¢ Attention-guided CAMs adapt ViT self-attention to produce class-localization heatmaps by extracting and aggregating attention weights across layers and heads, then projecting that aggregated attention back to image space. In practice one computes per-head attention maps, applies head- and layer-weighting or attention-rollout to estimate token-to-token relevance, and upsamples the resulting relevance to pixels to get a visualization; this typically yields improved localization versus na√Øve gradient-based CAMs but depends on head selection and aggregation heuristics [Paper 1, Paper 2].  \\n‚Ä¢ Localized and dilated neighborhood attention mechanisms reduce compute and enlarge effective receptive fields by restricting attention to spatial neighborhoods or using dilated patterns, which preserves long-range context with lower cost ‚Äî useful both for efficiency and for producing attention maps that reflect multi-scale object structure [Paper 2, Paper 7].  \\n‚Ä¢ Self-supervised schemes like PatchRot shape the representations learned by ViTs without labels, which can improve downstream localization and robustness of attention-based explanations because the model learns useful patch-level features and geometric cues from unlabeled data [Paper 3].  \\n‚Ä¢ Task-specific attention variants (temporal cross-attention, mask-attention-free pipelines, momentum-style attention) adapt the basic ViT attention to video, 3D instance segmentation, or time-series, changing how attention is computed and interpreted and therefore affecting both performance and the semantics of any derived explanations [Paper 4, Paper 6, Paper 5].  \\n\\n## R√©f√©rences\\n[Paper 1] Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention - Saebom Leem, Hyunseok Seo (2024) - http://arxiv.org/abs/2402.04563v1  \\n[Paper 2] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3  \\n[Paper 3] PatchRot: A Self-Supervised Technique for Training Vision Transformers - Sachin Chhabra, Prabal Bijoy Dutta, Hemanth Venkateswara (2022) - http://arxiv.org/abs/2210.15722v1  \\n[Paper 4] Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition - Ryota Hashiguchi, Toru Tamaki (2022) - http://arxiv.org/abs/2204.00452v2  \\n[Paper 5] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3  \\n[Paper 6] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1  \\n[Paper 7] Vision Transformer with Quadrangle Attention - Qiming Zhang, Jing Zhang, Yufei Xu (2023) - http://arxiv.org/abs/2303.15105v1  \\n[Paper 8] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2  \\n[Paper 9] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', 'papers': [{'id': 'http://arxiv.org/abs/2402.04563v1', 'title': 'Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention', 'authors': ['Saebom Leem', 'Hyunseok Seo'], 'summary': 'Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.', 'url': 'http://arxiv.org/abs/2402.04563v1', 'published': '2024-02-07', 'relevance_score': 100}, {'id': 'http://arxiv.org/abs/2209.15001v3', 'title': 'Dilated Neighborhood Attention Transformer', 'authors': ['Ali Hassani', 'Humphrey Shi'], 'summary': \"Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).\", 'url': 'http://arxiv.org/abs/2209.15001v3', 'published': '2022-09-29', 'relevance_score': 90}, {'id': 'http://arxiv.org/abs/2210.15722v1', 'title': 'PatchRot: A Self-Supervised Technique for Training Vision Transformers', 'authors': ['Sachin Chhabra', 'Prabal Bijoy Dutta', 'Hemanth Venkateswara', 'Baoxin Li'], 'summary': 'Vision transformers require a huge amount of labeled data to outperform convolutional neural networks. However, labeling a huge dataset is a very expensive process. Self-supervised learning techniques alleviate this problem by learning features similar to supervised learning in an unsupervised way. In this paper, we propose a self-supervised technique PatchRot that is crafted for vision transformers. PatchRot rotates images and image patches and trains the network to predict the rotation angles. The network learns to extract both global and local features from an image. Our extensive experiments on different datasets showcase PatchRot training learns rich features which outperform supervised learning and compared baseline.', 'url': 'http://arxiv.org/abs/2210.15722v1', 'published': '2022-10-27', 'relevance_score': 90}, {'id': 'http://arxiv.org/abs/2204.00452v2', 'title': 'Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition', 'authors': ['Ryota Hashiguchi', 'Toru Tamaki'], 'summary': 'Feature shifts have been shown to be useful for action recognition with CNN-based models since Temporal Shift Module (TSM) was proposed. It is based on frame-wise feature extraction with late fusion, and layer features are shifted along the time direction for the temporal interaction. TokenShift, a recent model based on Vision Transformer (ViT), also uses the temporal feature shift mechanism, which, however, does not fully exploit the structure of Multi-head Self-Attention (MSA) in ViT. In this paper, we propose Multi-head Self/Cross-Attention (MSCA), which fully utilizes the attention structure. TokenShift is based on a frame-wise ViT with features temporally shifted with successive frames (at time t+1 and t-1). In contrast, the proposed MSCA replaces MSA in the frame-wise ViT, and some MSA heads attend to successive frames instead of the current frame. The computation cost is the same as the frame-wise ViT and TokenShift as it simply changes the target to which the attention is taken. There is a choice about which of key, query, and value are taken from the successive frames, then we experimentally compared these variants with Kinetics400. We also investigate other variants in which the proposed MSCA is used along the patch dimension of ViT, instead of the head dimension. Experimental results show that a variant, MSCA-KV, shows the best performance and is better than TokenShift by 0.1% and then ViT by 1.2%.', 'url': 'http://arxiv.org/abs/2204.00452v2', 'published': '2022-04-01', 'relevance_score': 85}, {'id': 'http://arxiv.org/abs/2112.08534v3', 'title': 'Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture', 'authors': ['Kieran Wood', 'Sven Giegerich', 'Stephen Roberts', 'Stefan Zohren'], 'summary': 'We introduce the Momentum Transformer, an attention-based deep-learning architecture, which outperforms benchmark time-series momentum and mean-reversion trading strategies. Unlike state-of-the-art Long Short-Term Memory (LSTM) architectures, which are sequential in nature and tailored to local processing, an attention mechanism provides our architecture with a direct connection to all previous time-steps. Our architecture, an attention-LSTM hybrid, enables us to learn longer-term dependencies, improves performance when considering returns net of transaction costs and naturally adapts to new market regimes, such as during the SARS-CoV-2 crisis. Via the introduction of multiple attention heads, we can capture concurrent regimes, or temporal dynamics, which are occurring at different timescales. The Momentum Transformer is inherently interpretable, providing us with greater insights into our deep-learning momentum trading strategy, including the importance of different factors over time and the past time-steps which are of the greatest significance to the model.', 'url': 'http://arxiv.org/abs/2112.08534v3', 'published': '2021-12-16', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2309.01692v1', 'title': 'Mask-Attention-Free Transformer for 3D Instance Segmentation', 'authors': ['Xin Lai', 'Yuhui Yuan', 'Ruihang Chu', 'Yukang Chen', 'Han Hu', 'Jiaya Jia'], 'summary': 'Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.', 'url': 'http://arxiv.org/abs/2309.01692v1', 'published': '2023-09-04', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2303.15105v1', 'title': 'Vision Transformer with Quadrangle Attention', 'authors': ['Qiming Zhang', 'Jing Zhang', 'Yufei Xu', 'Dacheng Tao'], 'summary': 'Window-based attention has become a popular choice in vision transformers due to its superior performance, lower computational complexity, and less memory footprint. However, the design of hand-crafted windows, which is data-agnostic, constrains the flexibility of transformers to adapt to objects of varying sizes, shapes, and orientations. To address this issue, we propose a novel quadrangle attention (QA) method that extends the window-based attention to a general quadrangle formulation. Our method employs an end-to-end learnable quadrangle regression module that predicts a transformation matrix to transform default windows into target quadrangles for token sampling and attention calculation, enabling the network to model various targets with different shapes and orientations and capture rich context information. We integrate QA into plain and hierarchical vision transformers to create a new architecture named QFormer, which offers minor code modifications and negligible extra computational cost. Extensive experiments on public benchmarks demonstrate that QFormer outperforms existing representative vision transformers on various vision tasks, including classification, object detection, semantic segmentation, and pose estimation. The code will be made publicly available at \\\\href{https://github.com/ViTAE-Transformer/QFormer}{QFormer}.', 'url': 'http://arxiv.org/abs/2303.15105v1', 'published': '2023-03-27', 'relevance_score': 10}, {'id': 'http://arxiv.org/abs/2009.00804v2', 'title': 'Architectural Implications of Graph Neural Networks', 'authors': ['Zhihui Zhang', 'Jingwen Leng', 'Lingxiao Ma', 'Youshan Miao', 'Chao Li', 'Minyi Guo'], 'summary': 'Graph neural networks (GNN) represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This work tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.', 'url': 'http://arxiv.org/abs/2009.00804v2', 'published': '2020-09-02', 'relevance_score': 5}, {'id': 'http://arxiv.org/abs/2207.13219v4', 'title': 'Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications', 'authors': ['Marcelo Orenes-Vera', 'Esin Tureci', 'David Wentzlaff', 'Margaret Martonosi'], 'summary': 'Applications with low data reuse and frequent irregular memory accesses, such as graph or sparse linear algebra workloads, fail to scale well due to memory bottlenecks and poor core utilization. While prior work with prefetching, decoupling, or pipelining can mitigate memory latency and improve core utilization, memory bottlenecks persist due to limited off-chip bandwidth. Approaches doing processing in-memory (PIM) with Hybrid Memory Cube (HMC) overcome bandwidth limitations but fail to achieve high core utilization due to poor task scheduling and synchronization overheads. Moreover, the high memory-per-core ratio available with HMC limits strong scaling.\\n  We introduce Dalorex, a hardware-software co-design that achieves high parallelism and energy efficiency, demonstrating strong scaling with >16,000 cores when processing graph and sparse linear algebra workloads. Over the prior work in PIM, both using 256 cores, Dalorex improves performance and energy consumption by two orders of magnitude through (1) a tile-based distributed-memory architecture where each processing tile holds an equal amount of data, and all memory operations are local; (2) a task-based parallel programming model where tasks are executed by the processing unit that is co-located with the target data; (3) a network design optimized for irregular traffic, where all communication is one-way, and messages do not contain routing metadata; (4) novel traffic-aware task scheduling hardware that maintains high core utilization; and (5) a data placement strategy that improves work balance.\\n  This work proposes architectural and software innovations to provide the greatest scalability to date for running graph algorithms while still being programmable for other domains.', 'url': 'http://arxiv.org/abs/2207.13219v4', 'published': '2022-07-26', 'relevance_score': 2}], 'messages': [SystemMessage(content='The user inquired about innovations in vision transformers, particularly regarding their efficiency improvements. A researcher responded by locating five relevant papers on ArXiv that discuss the topic of dilated neighborhood attention in hierarchical vision transformers.', additional_kwargs={}, response_metadata={}), AIMessage(content='## R√©sum√©\\n‚Ä¢ Attention-guided CAMs adapt ViT self-attention to produce class-localization heatmaps by extracting and aggregating attention weights across layers and heads, then projecting that aggregated attention back to image space. In practice one computes per-head attention maps, applies head- and layer-weighting or attention-rollout to estimate token-to-token relevance, and upsamples the resulting relevance to pixels to get a visualization; this typically yields improved localization versus na√Øve gradient-based CAMs but depends on head selection and aggregation heuristics [Paper 1, Paper 2].  \\n‚Ä¢ Localized and dilated neighborhood attention mechanisms reduce compute and enlarge effective receptive fields by restricting attention to spatial neighborhoods or using dilated patterns, which preserves long-range context with lower cost ‚Äî useful both for efficiency and for producing attention maps that reflect multi-scale object structure [Paper 2, Paper 7].  \\n‚Ä¢ Self-supervised schemes like PatchRot shape the representations learned by ViTs without labels, which can improve downstream localization and robustness of attention-based explanations because the model learns useful patch-level features and geometric cues from unlabeled data [Paper 3].  \\n‚Ä¢ Task-specific attention variants (temporal cross-attention, mask-attention-free pipelines, momentum-style attention) adapt the basic ViT attention to video, 3D instance segmentation, or time-series, changing how attention is computed and interpreted and therefore affecting both performance and the semantics of any derived explanations [Paper 4, Paper 6, Paper 5].  \\n\\n## R√©f√©rences\\n[Paper 1] Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention - Saebom Leem, Hyunseok Seo (2024) - http://arxiv.org/abs/2402.04563v1  \\n[Paper 2] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3  \\n[Paper 3] PatchRot: A Self-Supervised Technique for Training Vision Transformers - Sachin Chhabra, Prabal Bijoy Dutta, Hemanth Venkateswara (2022) - http://arxiv.org/abs/2210.15722v1  \\n[Paper 4] Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition - Ryota Hashiguchi, Toru Tamaki (2022) - http://arxiv.org/abs/2204.00452v2  \\n[Paper 5] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3  \\n[Paper 6] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1  \\n[Paper 7] Vision Transformer with Quadrangle Attention - Qiming Zhang, Jing Zhang, Yufei Xu (2023) - http://arxiv.org/abs/2303.15105v1  \\n[Paper 8] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2  \\n[Paper 9] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', additional_kwargs={}, response_metadata={}, name='Summarizer'), HumanMessage(content='quantum machine learning for protein folding using NISQ devices', additional_kwargs={}, response_metadata={}, name='User'), AIMessage(content='Refined query: quantum machine learning protein folding NISQ hybrid variational', additional_kwargs={}, response_metadata={}, name='Clarifier')], 'refined_query': 'quantum machine learning protein folding NISQ hybrid variational', 'iteration': 0}, next=('researcher',), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-28af-65e0-800b-654b960a6770'}}, metadata={'source': 'loop', 'step': 11, 'parents': {}}, created_at='2025-11-16T11:13:47.688488+00:00', parent_config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-28a0-648c-800a-c9405d1a0e25'}}, tasks=(PregelTask(id='0199ca95-eea5-c343-f3dc-38e1e21c4923', name='researcher', path=('__pregel_pull', 'researcher'), error=None, interrupts=(), state=None, result={'papers': [{'id': 'http://arxiv.org/abs/2409.07626v2', 'title': 'Generalization Error Bound for Quantum Machine Learning in NISQ Era -- A Survey', 'authors': ['Bikram Khanal', 'Pablo Rivas', 'Arun Sanjel', 'Korn Sooksatra', 'Ernesto Quevedo', 'Alejandro Rodriguez'], 'summary': 'Despite the mounting anticipation for the quantum revolution, the success of Quantum Machine Learning (QML) in the Noisy Intermediate-Scale Quantum (NISQ) era hinges on a largely unexplored factor: the generalization error bound, a cornerstone of robust and reliable machine learning models. Current QML research, while exploring novel algorithms and applications extensively, is predominantly situated in the context of noise-free, ideal quantum computers. However, Quantum Circuit (QC) operations in NISQ-era devices are susceptible to various noise sources and errors. In this article, we conduct a Systematic Mapping Study (SMS) to explore the state-of-the-art generalization bound for supervised QML in NISQ-era and analyze the latest practices in the field. Our study systematically summarizes the existing computational platforms with quantum hardware, datasets, optimization techniques, and the common properties of the bounds found in the literature. We further present the performance accuracy of various approaches in classical benchmark datasets like the MNIST and IRIS datasets. The SMS also highlights the limitations and challenges in QML in the NISQ era and discusses future research directions to advance the field. Using a detailed Boolean operators query in five reliable indexers, we collected 544 papers and filtered them to a small set of 37 relevant articles. This filtration was done following the best practice of SMS with well-defined research questions and inclusion and exclusion criteria.', 'url': 'http://arxiv.org/abs/2409.07626v2', 'published': '2024-09-11', 'relevance_score': 70}, {'id': 'http://arxiv.org/abs/2306.04338v1', 'title': 'Changing Data Sources in the Age of Machine Learning for Official Statistics', 'authors': ['Cedric De Boom', 'Michael Reusens'], 'summary': 'Data science has become increasingly essential for the production of official statistics, as it enables the automated collection, processing, and analysis of large amounts of data. With such data science practices in place, it enables more timely, more insightful and more flexible reporting. However, the quality and integrity of data-science-driven statistics rely on the accuracy and reliability of the data sources and the machine learning techniques that support them. In particular, changes in data sources are inevitable to occur and pose significant risks that are crucial to address in the context of machine learning for official statistics.\\n  This paper gives an overview of the main risks, liabilities, and uncertainties associated with changing data sources in the context of machine learning for official statistics. We provide a checklist of the most prevalent origins and causes of changing data sources; not only on a technical level but also regarding ownership, ethics, regulation, and public perception. Next, we highlight the repercussions of changing data sources on statistical reporting. These include technical effects such as concept drift, bias, availability, validity, accuracy and completeness, but also the neutrality and potential discontinuation of the statistical offering. We offer a few important precautionary measures, such as enhancing robustness in both data sourcing and statistical techniques, and thorough monitoring. In doing so, machine learning-based official statistics can maintain integrity, reliability, consistency, and relevance in policy-making, decision-making, and public discourse.', 'url': 'http://arxiv.org/abs/2306.04338v1', 'published': '2023-06-07', 'relevance_score': 1}, {'id': 'http://arxiv.org/abs/2006.16189v4', 'title': 'DOME: Recommendations for supervised machine learning validation in biology', 'authors': ['Ian Walsh', 'Dmytro Fishman', 'Dario Garcia-Gasulla', 'Tiina Titma', 'Gianluca Pollastri', 'The ELIXIR Machine Learning focus group', 'Jen Harrow', 'Fotis E. Psomopoulos', 'Silvio C. E. Tosatto'], 'summary': 'Modern biology frequently relies on machine learning to provide predictions and improve decision processes. There have been recent calls for more scrutiny on machine learning performance and possible limitations. Here we present a set of community-wide recommendations aiming to help establish standards of supervised machine learning validation in biology. Adopting a structured methods description for machine learning based on data, optimization, model, evaluation (DOME) will aim to help both reviewers and readers to better understand and assess the performance and limitations of a method or outcome. The recommendations are formulated as questions to anyone wishing to pursue implementation of a machine learning algorithm. Answers to these questions can be easily included in the supplementary material of published papers.', 'url': 'http://arxiv.org/abs/2006.16189v4', 'published': '2020-06-25', 'relevance_score': 10}, {'id': 'http://arxiv.org/abs/cond-mat/9812291v1', 'title': 'Discrete molecular dynamics studies of the folding of a protein-like model', 'authors': ['Nikolay V. Dokholyan', 'Sergey V. Buldyrev', 'H. Eugene Stanley', 'Eugene I. Shakhnovich'], 'summary': 'Background: Many attempts have been made to resolve in time the folding of model proteins in computer simulations. Different computational approaches have emerged. Some of these approaches suffer from the insensitivity to the geometrical properties of the proteins (lattice models), while others are computationally heavy (traditional MD).\\n  Results: We use a recently-proposed approach of Zhou and Karplus to study the folding of the protein model based on the discrete time molecular dynamics algorithm. We show that this algorithm resolves with respect to time the folding --- unfolding transition. In addition, we demonstrate the ability to study the coreof the model protein.\\n  Conclusion: The algorithm along with the model of inter-residue interactions can serve as a tool to study the thermodynamics and kinetics of protein models.', 'url': 'http://arxiv.org/abs/cond-mat/9812291v1', 'published': '1998-12-17', 'relevance_score': 5}, {'id': 'http://arxiv.org/abs/2201.12150v2', 'title': 'Learning Curves for Decision Making in Supervised Machine Learning: A Survey', 'authors': ['Felix Mohr', 'Jan N. van Rijn'], 'summary': 'Learning curves are a concept from social sciences that has been adopted in the context of machine learning to assess the performance of a learning algorithm with respect to a certain resource, e.g., the number of training examples or the number of training iterations. Learning curves have important applications in several machine learning contexts, most notably in data acquisition, early stopping of model training, and model selection. For instance, learning curves can be used to model the performance of the combination of an algorithm and its hyperparameter configuration, providing insights into their potential suitability at an early stage and often expediting the algorithm selection process. Various learning curve models have been proposed to use learning curves for decision making. Some of these models answer the binary decision question of whether a given algorithm at a certain budget will outperform a certain reference performance, whereas more complex models predict the entire learning curve of an algorithm. We contribute a framework that categorises learning curve approaches using three criteria: the decision-making situation they address, the intrinsic learning curve question they answer and the type of resources they use. We survey papers from the literature and classify them into this framework.', 'url': 'http://arxiv.org/abs/2201.12150v2', 'published': '2022-01-28', 'relevance_score': 5}], 'iteration': 1, 'messages': [AIMessage(content='Found 5 papers on ArXiv for query: quantum machine learning protein folding NISQ hybrid variational', additional_kwargs={}, response_metadata={}, name='Researcher')]}),), interrupts=()),\n",
       " StateSnapshot(values={'query': 'quantum machine learning for protein folding using NISQ devices', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2, 'summary': '## R√©sum√©\\n‚Ä¢ Attention-guided CAMs adapt ViT self-attention to produce class-localization heatmaps by extracting and aggregating attention weights across layers and heads, then projecting that aggregated attention back to image space. In practice one computes per-head attention maps, applies head- and layer-weighting or attention-rollout to estimate token-to-token relevance, and upsamples the resulting relevance to pixels to get a visualization; this typically yields improved localization versus na√Øve gradient-based CAMs but depends on head selection and aggregation heuristics [Paper 1, Paper 2].  \\n‚Ä¢ Localized and dilated neighborhood attention mechanisms reduce compute and enlarge effective receptive fields by restricting attention to spatial neighborhoods or using dilated patterns, which preserves long-range context with lower cost ‚Äî useful both for efficiency and for producing attention maps that reflect multi-scale object structure [Paper 2, Paper 7].  \\n‚Ä¢ Self-supervised schemes like PatchRot shape the representations learned by ViTs without labels, which can improve downstream localization and robustness of attention-based explanations because the model learns useful patch-level features and geometric cues from unlabeled data [Paper 3].  \\n‚Ä¢ Task-specific attention variants (temporal cross-attention, mask-attention-free pipelines, momentum-style attention) adapt the basic ViT attention to video, 3D instance segmentation, or time-series, changing how attention is computed and interpreted and therefore affecting both performance and the semantics of any derived explanations [Paper 4, Paper 6, Paper 5].  \\n\\n## R√©f√©rences\\n[Paper 1] Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention - Saebom Leem, Hyunseok Seo (2024) - http://arxiv.org/abs/2402.04563v1  \\n[Paper 2] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3  \\n[Paper 3] PatchRot: A Self-Supervised Technique for Training Vision Transformers - Sachin Chhabra, Prabal Bijoy Dutta, Hemanth Venkateswara (2022) - http://arxiv.org/abs/2210.15722v1  \\n[Paper 4] Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition - Ryota Hashiguchi, Toru Tamaki (2022) - http://arxiv.org/abs/2204.00452v2  \\n[Paper 5] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3  \\n[Paper 6] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1  \\n[Paper 7] Vision Transformer with Quadrangle Attention - Qiming Zhang, Jing Zhang, Yufei Xu (2023) - http://arxiv.org/abs/2303.15105v1  \\n[Paper 8] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2  \\n[Paper 9] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', 'papers': [{'id': 'http://arxiv.org/abs/2402.04563v1', 'title': 'Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention', 'authors': ['Saebom Leem', 'Hyunseok Seo'], 'summary': 'Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.', 'url': 'http://arxiv.org/abs/2402.04563v1', 'published': '2024-02-07', 'relevance_score': 100}, {'id': 'http://arxiv.org/abs/2209.15001v3', 'title': 'Dilated Neighborhood Attention Transformer', 'authors': ['Ali Hassani', 'Humphrey Shi'], 'summary': \"Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).\", 'url': 'http://arxiv.org/abs/2209.15001v3', 'published': '2022-09-29', 'relevance_score': 90}, {'id': 'http://arxiv.org/abs/2210.15722v1', 'title': 'PatchRot: A Self-Supervised Technique for Training Vision Transformers', 'authors': ['Sachin Chhabra', 'Prabal Bijoy Dutta', 'Hemanth Venkateswara', 'Baoxin Li'], 'summary': 'Vision transformers require a huge amount of labeled data to outperform convolutional neural networks. However, labeling a huge dataset is a very expensive process. Self-supervised learning techniques alleviate this problem by learning features similar to supervised learning in an unsupervised way. In this paper, we propose a self-supervised technique PatchRot that is crafted for vision transformers. PatchRot rotates images and image patches and trains the network to predict the rotation angles. The network learns to extract both global and local features from an image. Our extensive experiments on different datasets showcase PatchRot training learns rich features which outperform supervised learning and compared baseline.', 'url': 'http://arxiv.org/abs/2210.15722v1', 'published': '2022-10-27', 'relevance_score': 90}, {'id': 'http://arxiv.org/abs/2204.00452v2', 'title': 'Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition', 'authors': ['Ryota Hashiguchi', 'Toru Tamaki'], 'summary': 'Feature shifts have been shown to be useful for action recognition with CNN-based models since Temporal Shift Module (TSM) was proposed. It is based on frame-wise feature extraction with late fusion, and layer features are shifted along the time direction for the temporal interaction. TokenShift, a recent model based on Vision Transformer (ViT), also uses the temporal feature shift mechanism, which, however, does not fully exploit the structure of Multi-head Self-Attention (MSA) in ViT. In this paper, we propose Multi-head Self/Cross-Attention (MSCA), which fully utilizes the attention structure. TokenShift is based on a frame-wise ViT with features temporally shifted with successive frames (at time t+1 and t-1). In contrast, the proposed MSCA replaces MSA in the frame-wise ViT, and some MSA heads attend to successive frames instead of the current frame. The computation cost is the same as the frame-wise ViT and TokenShift as it simply changes the target to which the attention is taken. There is a choice about which of key, query, and value are taken from the successive frames, then we experimentally compared these variants with Kinetics400. We also investigate other variants in which the proposed MSCA is used along the patch dimension of ViT, instead of the head dimension. Experimental results show that a variant, MSCA-KV, shows the best performance and is better than TokenShift by 0.1% and then ViT by 1.2%.', 'url': 'http://arxiv.org/abs/2204.00452v2', 'published': '2022-04-01', 'relevance_score': 85}, {'id': 'http://arxiv.org/abs/2112.08534v3', 'title': 'Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture', 'authors': ['Kieran Wood', 'Sven Giegerich', 'Stephen Roberts', 'Stefan Zohren'], 'summary': 'We introduce the Momentum Transformer, an attention-based deep-learning architecture, which outperforms benchmark time-series momentum and mean-reversion trading strategies. Unlike state-of-the-art Long Short-Term Memory (LSTM) architectures, which are sequential in nature and tailored to local processing, an attention mechanism provides our architecture with a direct connection to all previous time-steps. Our architecture, an attention-LSTM hybrid, enables us to learn longer-term dependencies, improves performance when considering returns net of transaction costs and naturally adapts to new market regimes, such as during the SARS-CoV-2 crisis. Via the introduction of multiple attention heads, we can capture concurrent regimes, or temporal dynamics, which are occurring at different timescales. The Momentum Transformer is inherently interpretable, providing us with greater insights into our deep-learning momentum trading strategy, including the importance of different factors over time and the past time-steps which are of the greatest significance to the model.', 'url': 'http://arxiv.org/abs/2112.08534v3', 'published': '2021-12-16', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2309.01692v1', 'title': 'Mask-Attention-Free Transformer for 3D Instance Segmentation', 'authors': ['Xin Lai', 'Yuhui Yuan', 'Ruihang Chu', 'Yukang Chen', 'Han Hu', 'Jiaya Jia'], 'summary': 'Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.', 'url': 'http://arxiv.org/abs/2309.01692v1', 'published': '2023-09-04', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2303.15105v1', 'title': 'Vision Transformer with Quadrangle Attention', 'authors': ['Qiming Zhang', 'Jing Zhang', 'Yufei Xu', 'Dacheng Tao'], 'summary': 'Window-based attention has become a popular choice in vision transformers due to its superior performance, lower computational complexity, and less memory footprint. However, the design of hand-crafted windows, which is data-agnostic, constrains the flexibility of transformers to adapt to objects of varying sizes, shapes, and orientations. To address this issue, we propose a novel quadrangle attention (QA) method that extends the window-based attention to a general quadrangle formulation. Our method employs an end-to-end learnable quadrangle regression module that predicts a transformation matrix to transform default windows into target quadrangles for token sampling and attention calculation, enabling the network to model various targets with different shapes and orientations and capture rich context information. We integrate QA into plain and hierarchical vision transformers to create a new architecture named QFormer, which offers minor code modifications and negligible extra computational cost. Extensive experiments on public benchmarks demonstrate that QFormer outperforms existing representative vision transformers on various vision tasks, including classification, object detection, semantic segmentation, and pose estimation. The code will be made publicly available at \\\\href{https://github.com/ViTAE-Transformer/QFormer}{QFormer}.', 'url': 'http://arxiv.org/abs/2303.15105v1', 'published': '2023-03-27', 'relevance_score': 10}, {'id': 'http://arxiv.org/abs/2009.00804v2', 'title': 'Architectural Implications of Graph Neural Networks', 'authors': ['Zhihui Zhang', 'Jingwen Leng', 'Lingxiao Ma', 'Youshan Miao', 'Chao Li', 'Minyi Guo'], 'summary': 'Graph neural networks (GNN) represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This work tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.', 'url': 'http://arxiv.org/abs/2009.00804v2', 'published': '2020-09-02', 'relevance_score': 5}, {'id': 'http://arxiv.org/abs/2207.13219v4', 'title': 'Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications', 'authors': ['Marcelo Orenes-Vera', 'Esin Tureci', 'David Wentzlaff', 'Margaret Martonosi'], 'summary': 'Applications with low data reuse and frequent irregular memory accesses, such as graph or sparse linear algebra workloads, fail to scale well due to memory bottlenecks and poor core utilization. While prior work with prefetching, decoupling, or pipelining can mitigate memory latency and improve core utilization, memory bottlenecks persist due to limited off-chip bandwidth. Approaches doing processing in-memory (PIM) with Hybrid Memory Cube (HMC) overcome bandwidth limitations but fail to achieve high core utilization due to poor task scheduling and synchronization overheads. Moreover, the high memory-per-core ratio available with HMC limits strong scaling.\\n  We introduce Dalorex, a hardware-software co-design that achieves high parallelism and energy efficiency, demonstrating strong scaling with >16,000 cores when processing graph and sparse linear algebra workloads. Over the prior work in PIM, both using 256 cores, Dalorex improves performance and energy consumption by two orders of magnitude through (1) a tile-based distributed-memory architecture where each processing tile holds an equal amount of data, and all memory operations are local; (2) a task-based parallel programming model where tasks are executed by the processing unit that is co-located with the target data; (3) a network design optimized for irregular traffic, where all communication is one-way, and messages do not contain routing metadata; (4) novel traffic-aware task scheduling hardware that maintains high core utilization; and (5) a data placement strategy that improves work balance.\\n  This work proposes architectural and software innovations to provide the greatest scalability to date for running graph algorithms while still being programmable for other domains.', 'url': 'http://arxiv.org/abs/2207.13219v4', 'published': '2022-07-26', 'relevance_score': 2}], 'messages': [SystemMessage(content='The user requested further details about the first point related to innovations in vision transformers, specifically improvements in their efficiency.', additional_kwargs={}, response_metadata={}), AIMessage(content='Refined query: dilated neighborhood attention hierarchical vision transformers efficiency', additional_kwargs={}, response_metadata={}, name='Clarifier'), AIMessage(content='Found 5 papers on ArXiv for query: dilated neighborhood attention hierarchical vision transformers efficiency', additional_kwargs={}, response_metadata={}, name='Researcher'), AIMessage(content='## R√©sum√©\\n‚Ä¢ Attention-guided CAMs adapt ViT self-attention to produce class-localization heatmaps by extracting and aggregating attention weights across layers and heads, then projecting that aggregated attention back to image space. In practice one computes per-head attention maps, applies head- and layer-weighting or attention-rollout to estimate token-to-token relevance, and upsamples the resulting relevance to pixels to get a visualization; this typically yields improved localization versus na√Øve gradient-based CAMs but depends on head selection and aggregation heuristics [Paper 1, Paper 2].  \\n‚Ä¢ Localized and dilated neighborhood attention mechanisms reduce compute and enlarge effective receptive fields by restricting attention to spatial neighborhoods or using dilated patterns, which preserves long-range context with lower cost ‚Äî useful both for efficiency and for producing attention maps that reflect multi-scale object structure [Paper 2, Paper 7].  \\n‚Ä¢ Self-supervised schemes like PatchRot shape the representations learned by ViTs without labels, which can improve downstream localization and robustness of attention-based explanations because the model learns useful patch-level features and geometric cues from unlabeled data [Paper 3].  \\n‚Ä¢ Task-specific attention variants (temporal cross-attention, mask-attention-free pipelines, momentum-style attention) adapt the basic ViT attention to video, 3D instance segmentation, or time-series, changing how attention is computed and interpreted and therefore affecting both performance and the semantics of any derived explanations [Paper 4, Paper 6, Paper 5].  \\n\\n## R√©f√©rences\\n[Paper 1] Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention - Saebom Leem, Hyunseok Seo (2024) - http://arxiv.org/abs/2402.04563v1  \\n[Paper 2] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3  \\n[Paper 3] PatchRot: A Self-Supervised Technique for Training Vision Transformers - Sachin Chhabra, Prabal Bijoy Dutta, Hemanth Venkateswara (2022) - http://arxiv.org/abs/2210.15722v1  \\n[Paper 4] Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition - Ryota Hashiguchi, Toru Tamaki (2022) - http://arxiv.org/abs/2204.00452v2  \\n[Paper 5] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3  \\n[Paper 6] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1  \\n[Paper 7] Vision Transformer with Quadrangle Attention - Qiming Zhang, Jing Zhang, Yufei Xu (2023) - http://arxiv.org/abs/2303.15105v1  \\n[Paper 8] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2  \\n[Paper 9] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', additional_kwargs={}, response_metadata={}, name='Summarizer')], 'refined_query': 'dilated neighborhood attention hierarchical vision transformers efficiency', 'iteration': 1}, next=('clarifier',), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-28a0-648c-800a-c9405d1a0e25'}}, metadata={'source': 'loop', 'step': 10, 'parents': {}}, created_at='2025-11-16T11:13:47.682307+00:00', parent_config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-289e-6cc2-8009-52f40a424c9a'}}, tasks=(PregelTask(id='863bd569-87f4-89e1-8ac5-ee26473e29d7', name='clarifier', path=('__pregel_pull', 'clarifier'), error=None, interrupts=(), state=None, result={'refined_query': 'quantum machine learning protein folding NISQ hybrid variational', 'iteration': 0, 'messages': [HumanMessage(content='quantum machine learning for protein folding using NISQ devices', additional_kwargs={}, response_metadata={}, name='User'), AIMessage(content='Refined query: quantum machine learning protein folding NISQ hybrid variational', additional_kwargs={}, response_metadata={}, name='Clarifier')]}),), interrupts=()),\n",
       " StateSnapshot(values={'query': 'Can you give more detail regarding your first point?', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2, 'summary': '## R√©sum√©\\n‚Ä¢ Attention-guided CAMs adapt ViT self-attention to produce class-localization heatmaps by extracting and aggregating attention weights across layers and heads, then projecting that aggregated attention back to image space. In practice one computes per-head attention maps, applies head- and layer-weighting or attention-rollout to estimate token-to-token relevance, and upsamples the resulting relevance to pixels to get a visualization; this typically yields improved localization versus na√Øve gradient-based CAMs but depends on head selection and aggregation heuristics [Paper 1, Paper 2].  \\n‚Ä¢ Localized and dilated neighborhood attention mechanisms reduce compute and enlarge effective receptive fields by restricting attention to spatial neighborhoods or using dilated patterns, which preserves long-range context with lower cost ‚Äî useful both for efficiency and for producing attention maps that reflect multi-scale object structure [Paper 2, Paper 7].  \\n‚Ä¢ Self-supervised schemes like PatchRot shape the representations learned by ViTs without labels, which can improve downstream localization and robustness of attention-based explanations because the model learns useful patch-level features and geometric cues from unlabeled data [Paper 3].  \\n‚Ä¢ Task-specific attention variants (temporal cross-attention, mask-attention-free pipelines, momentum-style attention) adapt the basic ViT attention to video, 3D instance segmentation, or time-series, changing how attention is computed and interpreted and therefore affecting both performance and the semantics of any derived explanations [Paper 4, Paper 6, Paper 5].  \\n\\n## R√©f√©rences\\n[Paper 1] Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention - Saebom Leem, Hyunseok Seo (2024) - http://arxiv.org/abs/2402.04563v1  \\n[Paper 2] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3  \\n[Paper 3] PatchRot: A Self-Supervised Technique for Training Vision Transformers - Sachin Chhabra, Prabal Bijoy Dutta, Hemanth Venkateswara (2022) - http://arxiv.org/abs/2210.15722v1  \\n[Paper 4] Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition - Ryota Hashiguchi, Toru Tamaki (2022) - http://arxiv.org/abs/2204.00452v2  \\n[Paper 5] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3  \\n[Paper 6] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1  \\n[Paper 7] Vision Transformer with Quadrangle Attention - Qiming Zhang, Jing Zhang, Yufei Xu (2023) - http://arxiv.org/abs/2303.15105v1  \\n[Paper 8] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2  \\n[Paper 9] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', 'papers': [{'id': 'http://arxiv.org/abs/2402.04563v1', 'title': 'Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention', 'authors': ['Saebom Leem', 'Hyunseok Seo'], 'summary': 'Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.', 'url': 'http://arxiv.org/abs/2402.04563v1', 'published': '2024-02-07', 'relevance_score': 100}, {'id': 'http://arxiv.org/abs/2209.15001v3', 'title': 'Dilated Neighborhood Attention Transformer', 'authors': ['Ali Hassani', 'Humphrey Shi'], 'summary': \"Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).\", 'url': 'http://arxiv.org/abs/2209.15001v3', 'published': '2022-09-29', 'relevance_score': 90}, {'id': 'http://arxiv.org/abs/2210.15722v1', 'title': 'PatchRot: A Self-Supervised Technique for Training Vision Transformers', 'authors': ['Sachin Chhabra', 'Prabal Bijoy Dutta', 'Hemanth Venkateswara', 'Baoxin Li'], 'summary': 'Vision transformers require a huge amount of labeled data to outperform convolutional neural networks. However, labeling a huge dataset is a very expensive process. Self-supervised learning techniques alleviate this problem by learning features similar to supervised learning in an unsupervised way. In this paper, we propose a self-supervised technique PatchRot that is crafted for vision transformers. PatchRot rotates images and image patches and trains the network to predict the rotation angles. The network learns to extract both global and local features from an image. Our extensive experiments on different datasets showcase PatchRot training learns rich features which outperform supervised learning and compared baseline.', 'url': 'http://arxiv.org/abs/2210.15722v1', 'published': '2022-10-27', 'relevance_score': 90}, {'id': 'http://arxiv.org/abs/2204.00452v2', 'title': 'Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition', 'authors': ['Ryota Hashiguchi', 'Toru Tamaki'], 'summary': 'Feature shifts have been shown to be useful for action recognition with CNN-based models since Temporal Shift Module (TSM) was proposed. It is based on frame-wise feature extraction with late fusion, and layer features are shifted along the time direction for the temporal interaction. TokenShift, a recent model based on Vision Transformer (ViT), also uses the temporal feature shift mechanism, which, however, does not fully exploit the structure of Multi-head Self-Attention (MSA) in ViT. In this paper, we propose Multi-head Self/Cross-Attention (MSCA), which fully utilizes the attention structure. TokenShift is based on a frame-wise ViT with features temporally shifted with successive frames (at time t+1 and t-1). In contrast, the proposed MSCA replaces MSA in the frame-wise ViT, and some MSA heads attend to successive frames instead of the current frame. The computation cost is the same as the frame-wise ViT and TokenShift as it simply changes the target to which the attention is taken. There is a choice about which of key, query, and value are taken from the successive frames, then we experimentally compared these variants with Kinetics400. We also investigate other variants in which the proposed MSCA is used along the patch dimension of ViT, instead of the head dimension. Experimental results show that a variant, MSCA-KV, shows the best performance and is better than TokenShift by 0.1% and then ViT by 1.2%.', 'url': 'http://arxiv.org/abs/2204.00452v2', 'published': '2022-04-01', 'relevance_score': 85}, {'id': 'http://arxiv.org/abs/2112.08534v3', 'title': 'Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture', 'authors': ['Kieran Wood', 'Sven Giegerich', 'Stephen Roberts', 'Stefan Zohren'], 'summary': 'We introduce the Momentum Transformer, an attention-based deep-learning architecture, which outperforms benchmark time-series momentum and mean-reversion trading strategies. Unlike state-of-the-art Long Short-Term Memory (LSTM) architectures, which are sequential in nature and tailored to local processing, an attention mechanism provides our architecture with a direct connection to all previous time-steps. Our architecture, an attention-LSTM hybrid, enables us to learn longer-term dependencies, improves performance when considering returns net of transaction costs and naturally adapts to new market regimes, such as during the SARS-CoV-2 crisis. Via the introduction of multiple attention heads, we can capture concurrent regimes, or temporal dynamics, which are occurring at different timescales. The Momentum Transformer is inherently interpretable, providing us with greater insights into our deep-learning momentum trading strategy, including the importance of different factors over time and the past time-steps which are of the greatest significance to the model.', 'url': 'http://arxiv.org/abs/2112.08534v3', 'published': '2021-12-16', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2309.01692v1', 'title': 'Mask-Attention-Free Transformer for 3D Instance Segmentation', 'authors': ['Xin Lai', 'Yuhui Yuan', 'Ruihang Chu', 'Yukang Chen', 'Han Hu', 'Jiaya Jia'], 'summary': 'Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.', 'url': 'http://arxiv.org/abs/2309.01692v1', 'published': '2023-09-04', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2303.15105v1', 'title': 'Vision Transformer with Quadrangle Attention', 'authors': ['Qiming Zhang', 'Jing Zhang', 'Yufei Xu', 'Dacheng Tao'], 'summary': 'Window-based attention has become a popular choice in vision transformers due to its superior performance, lower computational complexity, and less memory footprint. However, the design of hand-crafted windows, which is data-agnostic, constrains the flexibility of transformers to adapt to objects of varying sizes, shapes, and orientations. To address this issue, we propose a novel quadrangle attention (QA) method that extends the window-based attention to a general quadrangle formulation. Our method employs an end-to-end learnable quadrangle regression module that predicts a transformation matrix to transform default windows into target quadrangles for token sampling and attention calculation, enabling the network to model various targets with different shapes and orientations and capture rich context information. We integrate QA into plain and hierarchical vision transformers to create a new architecture named QFormer, which offers minor code modifications and negligible extra computational cost. Extensive experiments on public benchmarks demonstrate that QFormer outperforms existing representative vision transformers on various vision tasks, including classification, object detection, semantic segmentation, and pose estimation. The code will be made publicly available at \\\\href{https://github.com/ViTAE-Transformer/QFormer}{QFormer}.', 'url': 'http://arxiv.org/abs/2303.15105v1', 'published': '2023-03-27', 'relevance_score': 10}, {'id': 'http://arxiv.org/abs/2009.00804v2', 'title': 'Architectural Implications of Graph Neural Networks', 'authors': ['Zhihui Zhang', 'Jingwen Leng', 'Lingxiao Ma', 'Youshan Miao', 'Chao Li', 'Minyi Guo'], 'summary': 'Graph neural networks (GNN) represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This work tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.', 'url': 'http://arxiv.org/abs/2009.00804v2', 'published': '2020-09-02', 'relevance_score': 5}, {'id': 'http://arxiv.org/abs/2207.13219v4', 'title': 'Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications', 'authors': ['Marcelo Orenes-Vera', 'Esin Tureci', 'David Wentzlaff', 'Margaret Martonosi'], 'summary': 'Applications with low data reuse and frequent irregular memory accesses, such as graph or sparse linear algebra workloads, fail to scale well due to memory bottlenecks and poor core utilization. While prior work with prefetching, decoupling, or pipelining can mitigate memory latency and improve core utilization, memory bottlenecks persist due to limited off-chip bandwidth. Approaches doing processing in-memory (PIM) with Hybrid Memory Cube (HMC) overcome bandwidth limitations but fail to achieve high core utilization due to poor task scheduling and synchronization overheads. Moreover, the high memory-per-core ratio available with HMC limits strong scaling.\\n  We introduce Dalorex, a hardware-software co-design that achieves high parallelism and energy efficiency, demonstrating strong scaling with >16,000 cores when processing graph and sparse linear algebra workloads. Over the prior work in PIM, both using 256 cores, Dalorex improves performance and energy consumption by two orders of magnitude through (1) a tile-based distributed-memory architecture where each processing tile holds an equal amount of data, and all memory operations are local; (2) a task-based parallel programming model where tasks are executed by the processing unit that is co-located with the target data; (3) a network design optimized for irregular traffic, where all communication is one-way, and messages do not contain routing metadata; (4) novel traffic-aware task scheduling hardware that maintains high core utilization; and (5) a data placement strategy that improves work balance.\\n  This work proposes architectural and software innovations to provide the greatest scalability to date for running graph algorithms while still being programmable for other domains.', 'url': 'http://arxiv.org/abs/2207.13219v4', 'published': '2022-07-26', 'relevance_score': 2}], 'messages': [SystemMessage(content='The user requested further details about the first point related to innovations in vision transformers, specifically improvements in their efficiency.', additional_kwargs={}, response_metadata={}), AIMessage(content='Refined query: dilated neighborhood attention hierarchical vision transformers efficiency', additional_kwargs={}, response_metadata={}, name='Clarifier'), AIMessage(content='Found 5 papers on ArXiv for query: dilated neighborhood attention hierarchical vision transformers efficiency', additional_kwargs={}, response_metadata={}, name='Researcher'), AIMessage(content='## R√©sum√©\\n‚Ä¢ Attention-guided CAMs adapt ViT self-attention to produce class-localization heatmaps by extracting and aggregating attention weights across layers and heads, then projecting that aggregated attention back to image space. In practice one computes per-head attention maps, applies head- and layer-weighting or attention-rollout to estimate token-to-token relevance, and upsamples the resulting relevance to pixels to get a visualization; this typically yields improved localization versus na√Øve gradient-based CAMs but depends on head selection and aggregation heuristics [Paper 1, Paper 2].  \\n‚Ä¢ Localized and dilated neighborhood attention mechanisms reduce compute and enlarge effective receptive fields by restricting attention to spatial neighborhoods or using dilated patterns, which preserves long-range context with lower cost ‚Äî useful both for efficiency and for producing attention maps that reflect multi-scale object structure [Paper 2, Paper 7].  \\n‚Ä¢ Self-supervised schemes like PatchRot shape the representations learned by ViTs without labels, which can improve downstream localization and robustness of attention-based explanations because the model learns useful patch-level features and geometric cues from unlabeled data [Paper 3].  \\n‚Ä¢ Task-specific attention variants (temporal cross-attention, mask-attention-free pipelines, momentum-style attention) adapt the basic ViT attention to video, 3D instance segmentation, or time-series, changing how attention is computed and interpreted and therefore affecting both performance and the semantics of any derived explanations [Paper 4, Paper 6, Paper 5].  \\n\\n## R√©f√©rences\\n[Paper 1] Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention - Saebom Leem, Hyunseok Seo (2024) - http://arxiv.org/abs/2402.04563v1  \\n[Paper 2] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3  \\n[Paper 3] PatchRot: A Self-Supervised Technique for Training Vision Transformers - Sachin Chhabra, Prabal Bijoy Dutta, Hemanth Venkateswara (2022) - http://arxiv.org/abs/2210.15722v1  \\n[Paper 4] Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition - Ryota Hashiguchi, Toru Tamaki (2022) - http://arxiv.org/abs/2204.00452v2  \\n[Paper 5] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3  \\n[Paper 6] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1  \\n[Paper 7] Vision Transformer with Quadrangle Attention - Qiming Zhang, Jing Zhang, Yufei Xu (2023) - http://arxiv.org/abs/2303.15105v1  \\n[Paper 8] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2  \\n[Paper 9] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', additional_kwargs={}, response_metadata={}, name='Summarizer')], 'refined_query': 'dilated neighborhood attention hierarchical vision transformers efficiency', 'iteration': 1}, next=('__start__',), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-289e-6cc2-8009-52f40a424c9a'}}, metadata={'source': 'input', 'step': 9, 'parents': {}}, created_at='2025-11-16T11:13:47.681696+00:00', parent_config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-2839-6098-8008-7f94fa0b7fc5'}}, tasks=(PregelTask(id='4f0479cb-c8c5-307a-2b3f-c84ea48ed548', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'query': 'quantum machine learning for protein folding using NISQ devices', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2}),), interrupts=()),\n",
       " StateSnapshot(values={'query': 'Can you give more detail regarding your first point?', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2, 'summary': '## R√©sum√©\\n‚Ä¢ Attention-guided CAMs adapt ViT self-attention to produce class-localization heatmaps by extracting and aggregating attention weights across layers and heads, then projecting that aggregated attention back to image space. In practice one computes per-head attention maps, applies head- and layer-weighting or attention-rollout to estimate token-to-token relevance, and upsamples the resulting relevance to pixels to get a visualization; this typically yields improved localization versus na√Øve gradient-based CAMs but depends on head selection and aggregation heuristics [Paper 1, Paper 2].  \\n‚Ä¢ Localized and dilated neighborhood attention mechanisms reduce compute and enlarge effective receptive fields by restricting attention to spatial neighborhoods or using dilated patterns, which preserves long-range context with lower cost ‚Äî useful both for efficiency and for producing attention maps that reflect multi-scale object structure [Paper 2, Paper 7].  \\n‚Ä¢ Self-supervised schemes like PatchRot shape the representations learned by ViTs without labels, which can improve downstream localization and robustness of attention-based explanations because the model learns useful patch-level features and geometric cues from unlabeled data [Paper 3].  \\n‚Ä¢ Task-specific attention variants (temporal cross-attention, mask-attention-free pipelines, momentum-style attention) adapt the basic ViT attention to video, 3D instance segmentation, or time-series, changing how attention is computed and interpreted and therefore affecting both performance and the semantics of any derived explanations [Paper 4, Paper 6, Paper 5].  \\n\\n## R√©f√©rences\\n[Paper 1] Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention - Saebom Leem, Hyunseok Seo (2024) - http://arxiv.org/abs/2402.04563v1  \\n[Paper 2] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3  \\n[Paper 3] PatchRot: A Self-Supervised Technique for Training Vision Transformers - Sachin Chhabra, Prabal Bijoy Dutta, Hemanth Venkateswara (2022) - http://arxiv.org/abs/2210.15722v1  \\n[Paper 4] Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition - Ryota Hashiguchi, Toru Tamaki (2022) - http://arxiv.org/abs/2204.00452v2  \\n[Paper 5] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3  \\n[Paper 6] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1  \\n[Paper 7] Vision Transformer with Quadrangle Attention - Qiming Zhang, Jing Zhang, Yufei Xu (2023) - http://arxiv.org/abs/2303.15105v1  \\n[Paper 8] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2  \\n[Paper 9] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', 'papers': [{'id': 'http://arxiv.org/abs/2402.04563v1', 'title': 'Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention', 'authors': ['Saebom Leem', 'Hyunseok Seo'], 'summary': 'Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.', 'url': 'http://arxiv.org/abs/2402.04563v1', 'published': '2024-02-07', 'relevance_score': 100}, {'id': 'http://arxiv.org/abs/2209.15001v3', 'title': 'Dilated Neighborhood Attention Transformer', 'authors': ['Ali Hassani', 'Humphrey Shi'], 'summary': \"Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).\", 'url': 'http://arxiv.org/abs/2209.15001v3', 'published': '2022-09-29', 'relevance_score': 90}, {'id': 'http://arxiv.org/abs/2210.15722v1', 'title': 'PatchRot: A Self-Supervised Technique for Training Vision Transformers', 'authors': ['Sachin Chhabra', 'Prabal Bijoy Dutta', 'Hemanth Venkateswara', 'Baoxin Li'], 'summary': 'Vision transformers require a huge amount of labeled data to outperform convolutional neural networks. However, labeling a huge dataset is a very expensive process. Self-supervised learning techniques alleviate this problem by learning features similar to supervised learning in an unsupervised way. In this paper, we propose a self-supervised technique PatchRot that is crafted for vision transformers. PatchRot rotates images and image patches and trains the network to predict the rotation angles. The network learns to extract both global and local features from an image. Our extensive experiments on different datasets showcase PatchRot training learns rich features which outperform supervised learning and compared baseline.', 'url': 'http://arxiv.org/abs/2210.15722v1', 'published': '2022-10-27', 'relevance_score': 90}, {'id': 'http://arxiv.org/abs/2204.00452v2', 'title': 'Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition', 'authors': ['Ryota Hashiguchi', 'Toru Tamaki'], 'summary': 'Feature shifts have been shown to be useful for action recognition with CNN-based models since Temporal Shift Module (TSM) was proposed. It is based on frame-wise feature extraction with late fusion, and layer features are shifted along the time direction for the temporal interaction. TokenShift, a recent model based on Vision Transformer (ViT), also uses the temporal feature shift mechanism, which, however, does not fully exploit the structure of Multi-head Self-Attention (MSA) in ViT. In this paper, we propose Multi-head Self/Cross-Attention (MSCA), which fully utilizes the attention structure. TokenShift is based on a frame-wise ViT with features temporally shifted with successive frames (at time t+1 and t-1). In contrast, the proposed MSCA replaces MSA in the frame-wise ViT, and some MSA heads attend to successive frames instead of the current frame. The computation cost is the same as the frame-wise ViT and TokenShift as it simply changes the target to which the attention is taken. There is a choice about which of key, query, and value are taken from the successive frames, then we experimentally compared these variants with Kinetics400. We also investigate other variants in which the proposed MSCA is used along the patch dimension of ViT, instead of the head dimension. Experimental results show that a variant, MSCA-KV, shows the best performance and is better than TokenShift by 0.1% and then ViT by 1.2%.', 'url': 'http://arxiv.org/abs/2204.00452v2', 'published': '2022-04-01', 'relevance_score': 85}, {'id': 'http://arxiv.org/abs/2112.08534v3', 'title': 'Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture', 'authors': ['Kieran Wood', 'Sven Giegerich', 'Stephen Roberts', 'Stefan Zohren'], 'summary': 'We introduce the Momentum Transformer, an attention-based deep-learning architecture, which outperforms benchmark time-series momentum and mean-reversion trading strategies. Unlike state-of-the-art Long Short-Term Memory (LSTM) architectures, which are sequential in nature and tailored to local processing, an attention mechanism provides our architecture with a direct connection to all previous time-steps. Our architecture, an attention-LSTM hybrid, enables us to learn longer-term dependencies, improves performance when considering returns net of transaction costs and naturally adapts to new market regimes, such as during the SARS-CoV-2 crisis. Via the introduction of multiple attention heads, we can capture concurrent regimes, or temporal dynamics, which are occurring at different timescales. The Momentum Transformer is inherently interpretable, providing us with greater insights into our deep-learning momentum trading strategy, including the importance of different factors over time and the past time-steps which are of the greatest significance to the model.', 'url': 'http://arxiv.org/abs/2112.08534v3', 'published': '2021-12-16', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2309.01692v1', 'title': 'Mask-Attention-Free Transformer for 3D Instance Segmentation', 'authors': ['Xin Lai', 'Yuhui Yuan', 'Ruihang Chu', 'Yukang Chen', 'Han Hu', 'Jiaya Jia'], 'summary': 'Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.', 'url': 'http://arxiv.org/abs/2309.01692v1', 'published': '2023-09-04', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2303.15105v1', 'title': 'Vision Transformer with Quadrangle Attention', 'authors': ['Qiming Zhang', 'Jing Zhang', 'Yufei Xu', 'Dacheng Tao'], 'summary': 'Window-based attention has become a popular choice in vision transformers due to its superior performance, lower computational complexity, and less memory footprint. However, the design of hand-crafted windows, which is data-agnostic, constrains the flexibility of transformers to adapt to objects of varying sizes, shapes, and orientations. To address this issue, we propose a novel quadrangle attention (QA) method that extends the window-based attention to a general quadrangle formulation. Our method employs an end-to-end learnable quadrangle regression module that predicts a transformation matrix to transform default windows into target quadrangles for token sampling and attention calculation, enabling the network to model various targets with different shapes and orientations and capture rich context information. We integrate QA into plain and hierarchical vision transformers to create a new architecture named QFormer, which offers minor code modifications and negligible extra computational cost. Extensive experiments on public benchmarks demonstrate that QFormer outperforms existing representative vision transformers on various vision tasks, including classification, object detection, semantic segmentation, and pose estimation. The code will be made publicly available at \\\\href{https://github.com/ViTAE-Transformer/QFormer}{QFormer}.', 'url': 'http://arxiv.org/abs/2303.15105v1', 'published': '2023-03-27', 'relevance_score': 10}, {'id': 'http://arxiv.org/abs/2009.00804v2', 'title': 'Architectural Implications of Graph Neural Networks', 'authors': ['Zhihui Zhang', 'Jingwen Leng', 'Lingxiao Ma', 'Youshan Miao', 'Chao Li', 'Minyi Guo'], 'summary': 'Graph neural networks (GNN) represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This work tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.', 'url': 'http://arxiv.org/abs/2009.00804v2', 'published': '2020-09-02', 'relevance_score': 5}, {'id': 'http://arxiv.org/abs/2207.13219v4', 'title': 'Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications', 'authors': ['Marcelo Orenes-Vera', 'Esin Tureci', 'David Wentzlaff', 'Margaret Martonosi'], 'summary': 'Applications with low data reuse and frequent irregular memory accesses, such as graph or sparse linear algebra workloads, fail to scale well due to memory bottlenecks and poor core utilization. While prior work with prefetching, decoupling, or pipelining can mitigate memory latency and improve core utilization, memory bottlenecks persist due to limited off-chip bandwidth. Approaches doing processing in-memory (PIM) with Hybrid Memory Cube (HMC) overcome bandwidth limitations but fail to achieve high core utilization due to poor task scheduling and synchronization overheads. Moreover, the high memory-per-core ratio available with HMC limits strong scaling.\\n  We introduce Dalorex, a hardware-software co-design that achieves high parallelism and energy efficiency, demonstrating strong scaling with >16,000 cores when processing graph and sparse linear algebra workloads. Over the prior work in PIM, both using 256 cores, Dalorex improves performance and energy consumption by two orders of magnitude through (1) a tile-based distributed-memory architecture where each processing tile holds an equal amount of data, and all memory operations are local; (2) a task-based parallel programming model where tasks are executed by the processing unit that is co-located with the target data; (3) a network design optimized for irregular traffic, where all communication is one-way, and messages do not contain routing metadata; (4) novel traffic-aware task scheduling hardware that maintains high core utilization; and (5) a data placement strategy that improves work balance.\\n  This work proposes architectural and software innovations to provide the greatest scalability to date for running graph algorithms while still being programmable for other domains.', 'url': 'http://arxiv.org/abs/2207.13219v4', 'published': '2022-07-26', 'relevance_score': 2}], 'messages': [SystemMessage(content='The user requested further details about the first point related to innovations in vision transformers, specifically improvements in their efficiency.', additional_kwargs={}, response_metadata={}), AIMessage(content='Refined query: dilated neighborhood attention hierarchical vision transformers efficiency', additional_kwargs={}, response_metadata={}, name='Clarifier'), AIMessage(content='Found 5 papers on ArXiv for query: dilated neighborhood attention hierarchical vision transformers efficiency', additional_kwargs={}, response_metadata={}, name='Researcher'), AIMessage(content='## R√©sum√©\\n‚Ä¢ Attention-guided CAMs adapt ViT self-attention to produce class-localization heatmaps by extracting and aggregating attention weights across layers and heads, then projecting that aggregated attention back to image space. In practice one computes per-head attention maps, applies head- and layer-weighting or attention-rollout to estimate token-to-token relevance, and upsamples the resulting relevance to pixels to get a visualization; this typically yields improved localization versus na√Øve gradient-based CAMs but depends on head selection and aggregation heuristics [Paper 1, Paper 2].  \\n‚Ä¢ Localized and dilated neighborhood attention mechanisms reduce compute and enlarge effective receptive fields by restricting attention to spatial neighborhoods or using dilated patterns, which preserves long-range context with lower cost ‚Äî useful both for efficiency and for producing attention maps that reflect multi-scale object structure [Paper 2, Paper 7].  \\n‚Ä¢ Self-supervised schemes like PatchRot shape the representations learned by ViTs without labels, which can improve downstream localization and robustness of attention-based explanations because the model learns useful patch-level features and geometric cues from unlabeled data [Paper 3].  \\n‚Ä¢ Task-specific attention variants (temporal cross-attention, mask-attention-free pipelines, momentum-style attention) adapt the basic ViT attention to video, 3D instance segmentation, or time-series, changing how attention is computed and interpreted and therefore affecting both performance and the semantics of any derived explanations [Paper 4, Paper 6, Paper 5].  \\n\\n## R√©f√©rences\\n[Paper 1] Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention - Saebom Leem, Hyunseok Seo (2024) - http://arxiv.org/abs/2402.04563v1  \\n[Paper 2] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3  \\n[Paper 3] PatchRot: A Self-Supervised Technique for Training Vision Transformers - Sachin Chhabra, Prabal Bijoy Dutta, Hemanth Venkateswara (2022) - http://arxiv.org/abs/2210.15722v1  \\n[Paper 4] Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition - Ryota Hashiguchi, Toru Tamaki (2022) - http://arxiv.org/abs/2204.00452v2  \\n[Paper 5] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3  \\n[Paper 6] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1  \\n[Paper 7] Vision Transformer with Quadrangle Attention - Qiming Zhang, Jing Zhang, Yufei Xu (2023) - http://arxiv.org/abs/2303.15105v1  \\n[Paper 8] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2  \\n[Paper 9] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', additional_kwargs={}, response_metadata={}, name='Summarizer')], 'refined_query': 'dilated neighborhood attention hierarchical vision transformers efficiency', 'iteration': 1}, next=(), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-2839-6098-8008-7f94fa0b7fc5'}}, metadata={'source': 'loop', 'step': 8, 'parents': {}}, created_at='2025-11-16T11:13:47.640019+00:00', parent_config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-2823-66b2-8007-3a0e45954d6b'}}, tasks=(), interrupts=()),\n",
       " StateSnapshot(values={'query': 'Can you give more detail regarding your first point?', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2, 'summary': '## R√©sum√©\\n‚Ä¢ Point 1 ‚Äî Localized and dilated attention improves vision transformers by keeping attention computation tractable while enlarging receptive fields: Dilated Neighborhood Attention restricts attention to local neighborhoods but uses dilation to capture wider context, enabling hierarchical vision transformers to scale with lower compute and memory than full self-attention. This yields better integration into existing vision pipelines and improved efficiency on visual tasks [Paper 1].\\n\\n‚Ä¢ Point 2 ‚Äî Attention applied to time series can replace sequential RNNs with more interpretable, globally connected models: The Momentum Transformer provides direct access to all past timesteps, models momentum-like dynamics, and offers improved predictive performance and interpretability for trading/time-series tasks compared with LSTMs and classical strategies. This shows transformer attention‚Äôs strength at capturing long-range dependencies in non‚Äëvision modalities [Paper 2].\\n\\n‚Ä¢ Point 3 ‚Äî Removing mask-guided cross-attention can speed convergence in instance-segmentation transformers: The Mask-Attention-Free Transformer for 3D instance segmentation identifies slow convergence caused by low-recall initial masks and replaces mask-attention pipelines with alternative query refinement, simplifying the architecture and accelerating training while maintaining competitive instance segmentation performance [Paper 3].\\n\\n‚Ä¢ Point 4 ‚Äî Transformer design gains from hardware- and data-aware architectural thinking: Analyses of graph neural network architectures highlight how irregular data access and algorithm structure interact with system design, and proposals like Dalorex advocate data-local execution to mitigate memory-bound bottlenecks. Together these works emphasize that scaling transformer variants (and related graph/attention models) requires co-design of algorithmic attention patterns and memory/compute architectures to realize practical efficiency gains [Paper 4, Paper 5].\\n\\n## R√©f√©rences\\n[Paper 1] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3\\n\\n[Paper 2] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3\\n\\n[Paper 3] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1\\n\\n[Paper 4] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2\\n\\n[Paper 5] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', 'papers': [{'id': 'http://arxiv.org/abs/2402.04563v1', 'title': 'Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention', 'authors': ['Saebom Leem', 'Hyunseok Seo'], 'summary': 'Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.', 'url': 'http://arxiv.org/abs/2402.04563v1', 'published': '2024-02-07', 'relevance_score': 100}, {'id': 'http://arxiv.org/abs/2209.15001v3', 'title': 'Dilated Neighborhood Attention Transformer', 'authors': ['Ali Hassani', 'Humphrey Shi'], 'summary': \"Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).\", 'url': 'http://arxiv.org/abs/2209.15001v3', 'published': '2022-09-29', 'relevance_score': 90}, {'id': 'http://arxiv.org/abs/2210.15722v1', 'title': 'PatchRot: A Self-Supervised Technique for Training Vision Transformers', 'authors': ['Sachin Chhabra', 'Prabal Bijoy Dutta', 'Hemanth Venkateswara', 'Baoxin Li'], 'summary': 'Vision transformers require a huge amount of labeled data to outperform convolutional neural networks. However, labeling a huge dataset is a very expensive process. Self-supervised learning techniques alleviate this problem by learning features similar to supervised learning in an unsupervised way. In this paper, we propose a self-supervised technique PatchRot that is crafted for vision transformers. PatchRot rotates images and image patches and trains the network to predict the rotation angles. The network learns to extract both global and local features from an image. Our extensive experiments on different datasets showcase PatchRot training learns rich features which outperform supervised learning and compared baseline.', 'url': 'http://arxiv.org/abs/2210.15722v1', 'published': '2022-10-27', 'relevance_score': 90}, {'id': 'http://arxiv.org/abs/2204.00452v2', 'title': 'Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition', 'authors': ['Ryota Hashiguchi', 'Toru Tamaki'], 'summary': 'Feature shifts have been shown to be useful for action recognition with CNN-based models since Temporal Shift Module (TSM) was proposed. It is based on frame-wise feature extraction with late fusion, and layer features are shifted along the time direction for the temporal interaction. TokenShift, a recent model based on Vision Transformer (ViT), also uses the temporal feature shift mechanism, which, however, does not fully exploit the structure of Multi-head Self-Attention (MSA) in ViT. In this paper, we propose Multi-head Self/Cross-Attention (MSCA), which fully utilizes the attention structure. TokenShift is based on a frame-wise ViT with features temporally shifted with successive frames (at time t+1 and t-1). In contrast, the proposed MSCA replaces MSA in the frame-wise ViT, and some MSA heads attend to successive frames instead of the current frame. The computation cost is the same as the frame-wise ViT and TokenShift as it simply changes the target to which the attention is taken. There is a choice about which of key, query, and value are taken from the successive frames, then we experimentally compared these variants with Kinetics400. We also investigate other variants in which the proposed MSCA is used along the patch dimension of ViT, instead of the head dimension. Experimental results show that a variant, MSCA-KV, shows the best performance and is better than TokenShift by 0.1% and then ViT by 1.2%.', 'url': 'http://arxiv.org/abs/2204.00452v2', 'published': '2022-04-01', 'relevance_score': 85}, {'id': 'http://arxiv.org/abs/2112.08534v3', 'title': 'Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture', 'authors': ['Kieran Wood', 'Sven Giegerich', 'Stephen Roberts', 'Stefan Zohren'], 'summary': 'We introduce the Momentum Transformer, an attention-based deep-learning architecture, which outperforms benchmark time-series momentum and mean-reversion trading strategies. Unlike state-of-the-art Long Short-Term Memory (LSTM) architectures, which are sequential in nature and tailored to local processing, an attention mechanism provides our architecture with a direct connection to all previous time-steps. Our architecture, an attention-LSTM hybrid, enables us to learn longer-term dependencies, improves performance when considering returns net of transaction costs and naturally adapts to new market regimes, such as during the SARS-CoV-2 crisis. Via the introduction of multiple attention heads, we can capture concurrent regimes, or temporal dynamics, which are occurring at different timescales. The Momentum Transformer is inherently interpretable, providing us with greater insights into our deep-learning momentum trading strategy, including the importance of different factors over time and the past time-steps which are of the greatest significance to the model.', 'url': 'http://arxiv.org/abs/2112.08534v3', 'published': '2021-12-16', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2309.01692v1', 'title': 'Mask-Attention-Free Transformer for 3D Instance Segmentation', 'authors': ['Xin Lai', 'Yuhui Yuan', 'Ruihang Chu', 'Yukang Chen', 'Han Hu', 'Jiaya Jia'], 'summary': 'Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.', 'url': 'http://arxiv.org/abs/2309.01692v1', 'published': '2023-09-04', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2303.15105v1', 'title': 'Vision Transformer with Quadrangle Attention', 'authors': ['Qiming Zhang', 'Jing Zhang', 'Yufei Xu', 'Dacheng Tao'], 'summary': 'Window-based attention has become a popular choice in vision transformers due to its superior performance, lower computational complexity, and less memory footprint. However, the design of hand-crafted windows, which is data-agnostic, constrains the flexibility of transformers to adapt to objects of varying sizes, shapes, and orientations. To address this issue, we propose a novel quadrangle attention (QA) method that extends the window-based attention to a general quadrangle formulation. Our method employs an end-to-end learnable quadrangle regression module that predicts a transformation matrix to transform default windows into target quadrangles for token sampling and attention calculation, enabling the network to model various targets with different shapes and orientations and capture rich context information. We integrate QA into plain and hierarchical vision transformers to create a new architecture named QFormer, which offers minor code modifications and negligible extra computational cost. Extensive experiments on public benchmarks demonstrate that QFormer outperforms existing representative vision transformers on various vision tasks, including classification, object detection, semantic segmentation, and pose estimation. The code will be made publicly available at \\\\href{https://github.com/ViTAE-Transformer/QFormer}{QFormer}.', 'url': 'http://arxiv.org/abs/2303.15105v1', 'published': '2023-03-27', 'relevance_score': 10}, {'id': 'http://arxiv.org/abs/2009.00804v2', 'title': 'Architectural Implications of Graph Neural Networks', 'authors': ['Zhihui Zhang', 'Jingwen Leng', 'Lingxiao Ma', 'Youshan Miao', 'Chao Li', 'Minyi Guo'], 'summary': 'Graph neural networks (GNN) represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This work tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.', 'url': 'http://arxiv.org/abs/2009.00804v2', 'published': '2020-09-02', 'relevance_score': 5}, {'id': 'http://arxiv.org/abs/2207.13219v4', 'title': 'Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications', 'authors': ['Marcelo Orenes-Vera', 'Esin Tureci', 'David Wentzlaff', 'Margaret Martonosi'], 'summary': 'Applications with low data reuse and frequent irregular memory accesses, such as graph or sparse linear algebra workloads, fail to scale well due to memory bottlenecks and poor core utilization. While prior work with prefetching, decoupling, or pipelining can mitigate memory latency and improve core utilization, memory bottlenecks persist due to limited off-chip bandwidth. Approaches doing processing in-memory (PIM) with Hybrid Memory Cube (HMC) overcome bandwidth limitations but fail to achieve high core utilization due to poor task scheduling and synchronization overheads. Moreover, the high memory-per-core ratio available with HMC limits strong scaling.\\n  We introduce Dalorex, a hardware-software co-design that achieves high parallelism and energy efficiency, demonstrating strong scaling with >16,000 cores when processing graph and sparse linear algebra workloads. Over the prior work in PIM, both using 256 cores, Dalorex improves performance and energy consumption by two orders of magnitude through (1) a tile-based distributed-memory architecture where each processing tile holds an equal amount of data, and all memory operations are local; (2) a task-based parallel programming model where tasks are executed by the processing unit that is co-located with the target data; (3) a network design optimized for irregular traffic, where all communication is one-way, and messages do not contain routing metadata; (4) novel traffic-aware task scheduling hardware that maintains high core utilization; and (5) a data placement strategy that improves work balance.\\n  This work proposes architectural and software innovations to provide the greatest scalability to date for running graph algorithms while still being programmable for other domains.', 'url': 'http://arxiv.org/abs/2207.13219v4', 'published': '2022-07-26', 'relevance_score': 2}], 'messages': [SystemMessage(content='The user inquired about innovations in transformer architectures, specifically regarding attention mechanisms and positional encodings. The researcher provided a summary of five relevant papers from ArXiv, highlighting advancements such as improved efficiency in vision transformers, the application of attention for time series tasks, speeding convergence in instance segmentation, and the importance of co-designing algorithms with hardware for practical efficiency gains.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Can you give more detail regarding your first point?', additional_kwargs={}, response_metadata={}, name='User'), AIMessage(content='Refined query: dilated neighborhood attention hierarchical vision transformers efficiency', additional_kwargs={}, response_metadata={}, name='Clarifier'), AIMessage(content='Found 5 papers on ArXiv for query: dilated neighborhood attention hierarchical vision transformers efficiency', additional_kwargs={}, response_metadata={}, name='Researcher')], 'refined_query': 'dilated neighborhood attention hierarchical vision transformers efficiency', 'iteration': 1}, next=('summarizer',), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-2823-66b2-8007-3a0e45954d6b'}}, metadata={'source': 'loop', 'step': 7, 'parents': {}}, created_at='2025-11-16T11:13:47.631161+00:00', parent_config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-220e-64a2-8006-f3dfab3fdb04'}}, tasks=(PregelTask(id='c440b605-6624-a7ee-9f0e-1aa94e775f42', name='summarizer', path=('__pregel_pull', 'summarizer'), error=None, interrupts=(), state=None, result={'summary': '## R√©sum√©\\n‚Ä¢ Attention-guided CAMs adapt ViT self-attention to produce class-localization heatmaps by extracting and aggregating attention weights across layers and heads, then projecting that aggregated attention back to image space. In practice one computes per-head attention maps, applies head- and layer-weighting or attention-rollout to estimate token-to-token relevance, and upsamples the resulting relevance to pixels to get a visualization; this typically yields improved localization versus na√Øve gradient-based CAMs but depends on head selection and aggregation heuristics [Paper 1, Paper 2].  \\n‚Ä¢ Localized and dilated neighborhood attention mechanisms reduce compute and enlarge effective receptive fields by restricting attention to spatial neighborhoods or using dilated patterns, which preserves long-range context with lower cost ‚Äî useful both for efficiency and for producing attention maps that reflect multi-scale object structure [Paper 2, Paper 7].  \\n‚Ä¢ Self-supervised schemes like PatchRot shape the representations learned by ViTs without labels, which can improve downstream localization and robustness of attention-based explanations because the model learns useful patch-level features and geometric cues from unlabeled data [Paper 3].  \\n‚Ä¢ Task-specific attention variants (temporal cross-attention, mask-attention-free pipelines, momentum-style attention) adapt the basic ViT attention to video, 3D instance segmentation, or time-series, changing how attention is computed and interpreted and therefore affecting both performance and the semantics of any derived explanations [Paper 4, Paper 6, Paper 5].  \\n\\n## R√©f√©rences\\n[Paper 1] Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention - Saebom Leem, Hyunseok Seo (2024) - http://arxiv.org/abs/2402.04563v1  \\n[Paper 2] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3  \\n[Paper 3] PatchRot: A Self-Supervised Technique for Training Vision Transformers - Sachin Chhabra, Prabal Bijoy Dutta, Hemanth Venkateswara (2022) - http://arxiv.org/abs/2210.15722v1  \\n[Paper 4] Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition - Ryota Hashiguchi, Toru Tamaki (2022) - http://arxiv.org/abs/2204.00452v2  \\n[Paper 5] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3  \\n[Paper 6] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1  \\n[Paper 7] Vision Transformer with Quadrangle Attention - Qiming Zhang, Jing Zhang, Yufei Xu (2023) - http://arxiv.org/abs/2303.15105v1  \\n[Paper 8] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2  \\n[Paper 9] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', 'messages': [AIMessage(content='## R√©sum√©\\n‚Ä¢ Attention-guided CAMs adapt ViT self-attention to produce class-localization heatmaps by extracting and aggregating attention weights across layers and heads, then projecting that aggregated attention back to image space. In practice one computes per-head attention maps, applies head- and layer-weighting or attention-rollout to estimate token-to-token relevance, and upsamples the resulting relevance to pixels to get a visualization; this typically yields improved localization versus na√Øve gradient-based CAMs but depends on head selection and aggregation heuristics [Paper 1, Paper 2].  \\n‚Ä¢ Localized and dilated neighborhood attention mechanisms reduce compute and enlarge effective receptive fields by restricting attention to spatial neighborhoods or using dilated patterns, which preserves long-range context with lower cost ‚Äî useful both for efficiency and for producing attention maps that reflect multi-scale object structure [Paper 2, Paper 7].  \\n‚Ä¢ Self-supervised schemes like PatchRot shape the representations learned by ViTs without labels, which can improve downstream localization and robustness of attention-based explanations because the model learns useful patch-level features and geometric cues from unlabeled data [Paper 3].  \\n‚Ä¢ Task-specific attention variants (temporal cross-attention, mask-attention-free pipelines, momentum-style attention) adapt the basic ViT attention to video, 3D instance segmentation, or time-series, changing how attention is computed and interpreted and therefore affecting both performance and the semantics of any derived explanations [Paper 4, Paper 6, Paper 5].  \\n\\n## R√©f√©rences\\n[Paper 1] Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention - Saebom Leem, Hyunseok Seo (2024) - http://arxiv.org/abs/2402.04563v1  \\n[Paper 2] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3  \\n[Paper 3] PatchRot: A Self-Supervised Technique for Training Vision Transformers - Sachin Chhabra, Prabal Bijoy Dutta, Hemanth Venkateswara (2022) - http://arxiv.org/abs/2210.15722v1  \\n[Paper 4] Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition - Ryota Hashiguchi, Toru Tamaki (2022) - http://arxiv.org/abs/2204.00452v2  \\n[Paper 5] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3  \\n[Paper 6] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1  \\n[Paper 7] Vision Transformer with Quadrangle Attention - Qiming Zhang, Jing Zhang, Yufei Xu (2023) - http://arxiv.org/abs/2303.15105v1  \\n[Paper 8] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2  \\n[Paper 9] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', additional_kwargs={}, response_metadata={}, name='Summarizer')]}),), interrupts=()),\n",
       " StateSnapshot(values={'query': 'Can you give more detail regarding your first point?', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2, 'summary': '## R√©sum√©\\n‚Ä¢ Point 1 ‚Äî Localized and dilated attention improves vision transformers by keeping attention computation tractable while enlarging receptive fields: Dilated Neighborhood Attention restricts attention to local neighborhoods but uses dilation to capture wider context, enabling hierarchical vision transformers to scale with lower compute and memory than full self-attention. This yields better integration into existing vision pipelines and improved efficiency on visual tasks [Paper 1].\\n\\n‚Ä¢ Point 2 ‚Äî Attention applied to time series can replace sequential RNNs with more interpretable, globally connected models: The Momentum Transformer provides direct access to all past timesteps, models momentum-like dynamics, and offers improved predictive performance and interpretability for trading/time-series tasks compared with LSTMs and classical strategies. This shows transformer attention‚Äôs strength at capturing long-range dependencies in non‚Äëvision modalities [Paper 2].\\n\\n‚Ä¢ Point 3 ‚Äî Removing mask-guided cross-attention can speed convergence in instance-segmentation transformers: The Mask-Attention-Free Transformer for 3D instance segmentation identifies slow convergence caused by low-recall initial masks and replaces mask-attention pipelines with alternative query refinement, simplifying the architecture and accelerating training while maintaining competitive instance segmentation performance [Paper 3].\\n\\n‚Ä¢ Point 4 ‚Äî Transformer design gains from hardware- and data-aware architectural thinking: Analyses of graph neural network architectures highlight how irregular data access and algorithm structure interact with system design, and proposals like Dalorex advocate data-local execution to mitigate memory-bound bottlenecks. Together these works emphasize that scaling transformer variants (and related graph/attention models) requires co-design of algorithmic attention patterns and memory/compute architectures to realize practical efficiency gains [Paper 4, Paper 5].\\n\\n## R√©f√©rences\\n[Paper 1] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3\\n\\n[Paper 2] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3\\n\\n[Paper 3] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1\\n\\n[Paper 4] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2\\n\\n[Paper 5] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', 'papers': [{'id': 'http://arxiv.org/abs/2209.15001v3', 'title': 'Dilated Neighborhood Attention Transformer', 'authors': ['Ali Hassani', 'Humphrey Shi'], 'summary': \"Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).\", 'url': 'http://arxiv.org/abs/2209.15001v3', 'published': '2022-09-29', 'relevance_score': 60}, {'id': 'http://arxiv.org/abs/2112.08534v3', 'title': 'Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture', 'authors': ['Kieran Wood', 'Sven Giegerich', 'Stephen Roberts', 'Stefan Zohren'], 'summary': 'We introduce the Momentum Transformer, an attention-based deep-learning architecture, which outperforms benchmark time-series momentum and mean-reversion trading strategies. Unlike state-of-the-art Long Short-Term Memory (LSTM) architectures, which are sequential in nature and tailored to local processing, an attention mechanism provides our architecture with a direct connection to all previous time-steps. Our architecture, an attention-LSTM hybrid, enables us to learn longer-term dependencies, improves performance when considering returns net of transaction costs and naturally adapts to new market regimes, such as during the SARS-CoV-2 crisis. Via the introduction of multiple attention heads, we can capture concurrent regimes, or temporal dynamics, which are occurring at different timescales. The Momentum Transformer is inherently interpretable, providing us with greater insights into our deep-learning momentum trading strategy, including the importance of different factors over time and the past time-steps which are of the greatest significance to the model.', 'url': 'http://arxiv.org/abs/2112.08534v3', 'published': '2021-12-16', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2309.01692v1', 'title': 'Mask-Attention-Free Transformer for 3D Instance Segmentation', 'authors': ['Xin Lai', 'Yuhui Yuan', 'Ruihang Chu', 'Yukang Chen', 'Han Hu', 'Jiaya Jia'], 'summary': 'Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.', 'url': 'http://arxiv.org/abs/2309.01692v1', 'published': '2023-09-04', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2009.00804v2', 'title': 'Architectural Implications of Graph Neural Networks', 'authors': ['Zhihui Zhang', 'Jingwen Leng', 'Lingxiao Ma', 'Youshan Miao', 'Chao Li', 'Minyi Guo'], 'summary': 'Graph neural networks (GNN) represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This work tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.', 'url': 'http://arxiv.org/abs/2009.00804v2', 'published': '2020-09-02', 'relevance_score': 5}, {'id': 'http://arxiv.org/abs/2207.13219v4', 'title': 'Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications', 'authors': ['Marcelo Orenes-Vera', 'Esin Tureci', 'David Wentzlaff', 'Margaret Martonosi'], 'summary': 'Applications with low data reuse and frequent irregular memory accesses, such as graph or sparse linear algebra workloads, fail to scale well due to memory bottlenecks and poor core utilization. While prior work with prefetching, decoupling, or pipelining can mitigate memory latency and improve core utilization, memory bottlenecks persist due to limited off-chip bandwidth. Approaches doing processing in-memory (PIM) with Hybrid Memory Cube (HMC) overcome bandwidth limitations but fail to achieve high core utilization due to poor task scheduling and synchronization overheads. Moreover, the high memory-per-core ratio available with HMC limits strong scaling.\\n  We introduce Dalorex, a hardware-software co-design that achieves high parallelism and energy efficiency, demonstrating strong scaling with >16,000 cores when processing graph and sparse linear algebra workloads. Over the prior work in PIM, both using 256 cores, Dalorex improves performance and energy consumption by two orders of magnitude through (1) a tile-based distributed-memory architecture where each processing tile holds an equal amount of data, and all memory operations are local; (2) a task-based parallel programming model where tasks are executed by the processing unit that is co-located with the target data; (3) a network design optimized for irregular traffic, where all communication is one-way, and messages do not contain routing metadata; (4) novel traffic-aware task scheduling hardware that maintains high core utilization; and (5) a data placement strategy that improves work balance.\\n  This work proposes architectural and software innovations to provide the greatest scalability to date for running graph algorithms while still being programmable for other domains.', 'url': 'http://arxiv.org/abs/2207.13219v4', 'published': '2022-07-26', 'relevance_score': 2}], 'messages': [SystemMessage(content='The user expressed interest in key innovations in transformer architectures, particularly focusing on advancements in attention mechanisms and positional encodings. The researcher found five relevant papers on ArXiv that address these topics.', additional_kwargs={}, response_metadata={}), AIMessage(content='## R√©sum√©\\n‚Ä¢ Point 1 ‚Äî Localized and dilated attention improves vision transformers by keeping attention computation tractable while enlarging receptive fields: Dilated Neighborhood Attention restricts attention to local neighborhoods but uses dilation to capture wider context, enabling hierarchical vision transformers to scale with lower compute and memory than full self-attention. This yields better integration into existing vision pipelines and improved efficiency on visual tasks [Paper 1].\\n\\n‚Ä¢ Point 2 ‚Äî Attention applied to time series can replace sequential RNNs with more interpretable, globally connected models: The Momentum Transformer provides direct access to all past timesteps, models momentum-like dynamics, and offers improved predictive performance and interpretability for trading/time-series tasks compared with LSTMs and classical strategies. This shows transformer attention‚Äôs strength at capturing long-range dependencies in non‚Äëvision modalities [Paper 2].\\n\\n‚Ä¢ Point 3 ‚Äî Removing mask-guided cross-attention can speed convergence in instance-segmentation transformers: The Mask-Attention-Free Transformer for 3D instance segmentation identifies slow convergence caused by low-recall initial masks and replaces mask-attention pipelines with alternative query refinement, simplifying the architecture and accelerating training while maintaining competitive instance segmentation performance [Paper 3].\\n\\n‚Ä¢ Point 4 ‚Äî Transformer design gains from hardware- and data-aware architectural thinking: Analyses of graph neural network architectures highlight how irregular data access and algorithm structure interact with system design, and proposals like Dalorex advocate data-local execution to mitigate memory-bound bottlenecks. Together these works emphasize that scaling transformer variants (and related graph/attention models) requires co-design of algorithmic attention patterns and memory/compute architectures to realize practical efficiency gains [Paper 4, Paper 5].\\n\\n## R√©f√©rences\\n[Paper 1] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3\\n\\n[Paper 2] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3\\n\\n[Paper 3] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1\\n\\n[Paper 4] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2\\n\\n[Paper 5] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', additional_kwargs={}, response_metadata={}, name='Summarizer'), HumanMessage(content='Can you give more detail regarding your first point?', additional_kwargs={}, response_metadata={}, name='User'), AIMessage(content='Refined query: dilated neighborhood attention hierarchical vision transformers efficiency', additional_kwargs={}, response_metadata={}, name='Clarifier')], 'refined_query': 'dilated neighborhood attention hierarchical vision transformers efficiency', 'iteration': 0}, next=('researcher',), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-220e-64a2-8006-f3dfab3fdb04'}}, metadata={'source': 'loop', 'step': 6, 'parents': {}}, created_at='2025-11-16T11:13:46.993366+00:00', parent_config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-21fc-6e00-8005-0f4a73429923'}}, tasks=(PregelTask(id='c7dddafe-b3e4-9848-0ed4-d6e725edcb02', name='researcher', path=('__pregel_pull', 'researcher'), error=None, interrupts=(), state=None, result={'papers': [{'id': 'http://arxiv.org/abs/2209.15001v3', 'title': 'Dilated Neighborhood Attention Transformer', 'authors': ['Ali Hassani', 'Humphrey Shi'], 'summary': \"Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).\", 'url': 'http://arxiv.org/abs/2209.15001v3', 'published': '2022-09-29', 'relevance_score': 90}, {'id': 'http://arxiv.org/abs/2303.15105v1', 'title': 'Vision Transformer with Quadrangle Attention', 'authors': ['Qiming Zhang', 'Jing Zhang', 'Yufei Xu', 'Dacheng Tao'], 'summary': 'Window-based attention has become a popular choice in vision transformers due to its superior performance, lower computational complexity, and less memory footprint. However, the design of hand-crafted windows, which is data-agnostic, constrains the flexibility of transformers to adapt to objects of varying sizes, shapes, and orientations. To address this issue, we propose a novel quadrangle attention (QA) method that extends the window-based attention to a general quadrangle formulation. Our method employs an end-to-end learnable quadrangle regression module that predicts a transformation matrix to transform default windows into target quadrangles for token sampling and attention calculation, enabling the network to model various targets with different shapes and orientations and capture rich context information. We integrate QA into plain and hierarchical vision transformers to create a new architecture named QFormer, which offers minor code modifications and negligible extra computational cost. Extensive experiments on public benchmarks demonstrate that QFormer outperforms existing representative vision transformers on various vision tasks, including classification, object detection, semantic segmentation, and pose estimation. The code will be made publicly available at \\\\href{https://github.com/ViTAE-Transformer/QFormer}{QFormer}.', 'url': 'http://arxiv.org/abs/2303.15105v1', 'published': '2023-03-27', 'relevance_score': 10}, {'id': 'http://arxiv.org/abs/2210.15722v1', 'title': 'PatchRot: A Self-Supervised Technique for Training Vision Transformers', 'authors': ['Sachin Chhabra', 'Prabal Bijoy Dutta', 'Hemanth Venkateswara', 'Baoxin Li'], 'summary': 'Vision transformers require a huge amount of labeled data to outperform convolutional neural networks. However, labeling a huge dataset is a very expensive process. Self-supervised learning techniques alleviate this problem by learning features similar to supervised learning in an unsupervised way. In this paper, we propose a self-supervised technique PatchRot that is crafted for vision transformers. PatchRot rotates images and image patches and trains the network to predict the rotation angles. The network learns to extract both global and local features from an image. Our extensive experiments on different datasets showcase PatchRot training learns rich features which outperform supervised learning and compared baseline.', 'url': 'http://arxiv.org/abs/2210.15722v1', 'published': '2022-10-27', 'relevance_score': 90}, {'id': 'http://arxiv.org/abs/2204.00452v2', 'title': 'Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition', 'authors': ['Ryota Hashiguchi', 'Toru Tamaki'], 'summary': 'Feature shifts have been shown to be useful for action recognition with CNN-based models since Temporal Shift Module (TSM) was proposed. It is based on frame-wise feature extraction with late fusion, and layer features are shifted along the time direction for the temporal interaction. TokenShift, a recent model based on Vision Transformer (ViT), also uses the temporal feature shift mechanism, which, however, does not fully exploit the structure of Multi-head Self-Attention (MSA) in ViT. In this paper, we propose Multi-head Self/Cross-Attention (MSCA), which fully utilizes the attention structure. TokenShift is based on a frame-wise ViT with features temporally shifted with successive frames (at time t+1 and t-1). In contrast, the proposed MSCA replaces MSA in the frame-wise ViT, and some MSA heads attend to successive frames instead of the current frame. The computation cost is the same as the frame-wise ViT and TokenShift as it simply changes the target to which the attention is taken. There is a choice about which of key, query, and value are taken from the successive frames, then we experimentally compared these variants with Kinetics400. We also investigate other variants in which the proposed MSCA is used along the patch dimension of ViT, instead of the head dimension. Experimental results show that a variant, MSCA-KV, shows the best performance and is better than TokenShift by 0.1% and then ViT by 1.2%.', 'url': 'http://arxiv.org/abs/2204.00452v2', 'published': '2022-04-01', 'relevance_score': 85}, {'id': 'http://arxiv.org/abs/2402.04563v1', 'title': 'Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention', 'authors': ['Saebom Leem', 'Hyunseok Seo'], 'summary': 'Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.', 'url': 'http://arxiv.org/abs/2402.04563v1', 'published': '2024-02-07', 'relevance_score': 100}], 'iteration': 1, 'messages': [AIMessage(content='Found 5 papers on ArXiv for query: dilated neighborhood attention hierarchical vision transformers efficiency', additional_kwargs={}, response_metadata={}, name='Researcher')]}),), interrupts=()),\n",
       " StateSnapshot(values={'query': 'Can you give more detail regarding your first point?', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2, 'summary': '## R√©sum√©\\n‚Ä¢ Point 1 ‚Äî Localized and dilated attention improves vision transformers by keeping attention computation tractable while enlarging receptive fields: Dilated Neighborhood Attention restricts attention to local neighborhoods but uses dilation to capture wider context, enabling hierarchical vision transformers to scale with lower compute and memory than full self-attention. This yields better integration into existing vision pipelines and improved efficiency on visual tasks [Paper 1].\\n\\n‚Ä¢ Point 2 ‚Äî Attention applied to time series can replace sequential RNNs with more interpretable, globally connected models: The Momentum Transformer provides direct access to all past timesteps, models momentum-like dynamics, and offers improved predictive performance and interpretability for trading/time-series tasks compared with LSTMs and classical strategies. This shows transformer attention‚Äôs strength at capturing long-range dependencies in non‚Äëvision modalities [Paper 2].\\n\\n‚Ä¢ Point 3 ‚Äî Removing mask-guided cross-attention can speed convergence in instance-segmentation transformers: The Mask-Attention-Free Transformer for 3D instance segmentation identifies slow convergence caused by low-recall initial masks and replaces mask-attention pipelines with alternative query refinement, simplifying the architecture and accelerating training while maintaining competitive instance segmentation performance [Paper 3].\\n\\n‚Ä¢ Point 4 ‚Äî Transformer design gains from hardware- and data-aware architectural thinking: Analyses of graph neural network architectures highlight how irregular data access and algorithm structure interact with system design, and proposals like Dalorex advocate data-local execution to mitigate memory-bound bottlenecks. Together these works emphasize that scaling transformer variants (and related graph/attention models) requires co-design of algorithmic attention patterns and memory/compute architectures to realize practical efficiency gains [Paper 4, Paper 5].\\n\\n## R√©f√©rences\\n[Paper 1] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3\\n\\n[Paper 2] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3\\n\\n[Paper 3] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1\\n\\n[Paper 4] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2\\n\\n[Paper 5] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', 'papers': [{'id': 'http://arxiv.org/abs/2209.15001v3', 'title': 'Dilated Neighborhood Attention Transformer', 'authors': ['Ali Hassani', 'Humphrey Shi'], 'summary': \"Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).\", 'url': 'http://arxiv.org/abs/2209.15001v3', 'published': '2022-09-29', 'relevance_score': 60}, {'id': 'http://arxiv.org/abs/2112.08534v3', 'title': 'Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture', 'authors': ['Kieran Wood', 'Sven Giegerich', 'Stephen Roberts', 'Stefan Zohren'], 'summary': 'We introduce the Momentum Transformer, an attention-based deep-learning architecture, which outperforms benchmark time-series momentum and mean-reversion trading strategies. Unlike state-of-the-art Long Short-Term Memory (LSTM) architectures, which are sequential in nature and tailored to local processing, an attention mechanism provides our architecture with a direct connection to all previous time-steps. Our architecture, an attention-LSTM hybrid, enables us to learn longer-term dependencies, improves performance when considering returns net of transaction costs and naturally adapts to new market regimes, such as during the SARS-CoV-2 crisis. Via the introduction of multiple attention heads, we can capture concurrent regimes, or temporal dynamics, which are occurring at different timescales. The Momentum Transformer is inherently interpretable, providing us with greater insights into our deep-learning momentum trading strategy, including the importance of different factors over time and the past time-steps which are of the greatest significance to the model.', 'url': 'http://arxiv.org/abs/2112.08534v3', 'published': '2021-12-16', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2309.01692v1', 'title': 'Mask-Attention-Free Transformer for 3D Instance Segmentation', 'authors': ['Xin Lai', 'Yuhui Yuan', 'Ruihang Chu', 'Yukang Chen', 'Han Hu', 'Jiaya Jia'], 'summary': 'Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.', 'url': 'http://arxiv.org/abs/2309.01692v1', 'published': '2023-09-04', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2009.00804v2', 'title': 'Architectural Implications of Graph Neural Networks', 'authors': ['Zhihui Zhang', 'Jingwen Leng', 'Lingxiao Ma', 'Youshan Miao', 'Chao Li', 'Minyi Guo'], 'summary': 'Graph neural networks (GNN) represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This work tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.', 'url': 'http://arxiv.org/abs/2009.00804v2', 'published': '2020-09-02', 'relevance_score': 5}, {'id': 'http://arxiv.org/abs/2207.13219v4', 'title': 'Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications', 'authors': ['Marcelo Orenes-Vera', 'Esin Tureci', 'David Wentzlaff', 'Margaret Martonosi'], 'summary': 'Applications with low data reuse and frequent irregular memory accesses, such as graph or sparse linear algebra workloads, fail to scale well due to memory bottlenecks and poor core utilization. While prior work with prefetching, decoupling, or pipelining can mitigate memory latency and improve core utilization, memory bottlenecks persist due to limited off-chip bandwidth. Approaches doing processing in-memory (PIM) with Hybrid Memory Cube (HMC) overcome bandwidth limitations but fail to achieve high core utilization due to poor task scheduling and synchronization overheads. Moreover, the high memory-per-core ratio available with HMC limits strong scaling.\\n  We introduce Dalorex, a hardware-software co-design that achieves high parallelism and energy efficiency, demonstrating strong scaling with >16,000 cores when processing graph and sparse linear algebra workloads. Over the prior work in PIM, both using 256 cores, Dalorex improves performance and energy consumption by two orders of magnitude through (1) a tile-based distributed-memory architecture where each processing tile holds an equal amount of data, and all memory operations are local; (2) a task-based parallel programming model where tasks are executed by the processing unit that is co-located with the target data; (3) a network design optimized for irregular traffic, where all communication is one-way, and messages do not contain routing metadata; (4) novel traffic-aware task scheduling hardware that maintains high core utilization; and (5) a data placement strategy that improves work balance.\\n  This work proposes architectural and software innovations to provide the greatest scalability to date for running graph algorithms while still being programmable for other domains.', 'url': 'http://arxiv.org/abs/2207.13219v4', 'published': '2022-07-26', 'relevance_score': 2}], 'messages': [SystemMessage(content='User inquired about the key innovations in transformer architectures, which suggests an interest in advancements in this field.', additional_kwargs={}, response_metadata={}), AIMessage(content='Refined query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Clarifier'), AIMessage(content='Found 5 papers on ArXiv for query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Researcher'), AIMessage(content='## R√©sum√©\\n‚Ä¢ Point 1 ‚Äî Localized and dilated attention improves vision transformers by keeping attention computation tractable while enlarging receptive fields: Dilated Neighborhood Attention restricts attention to local neighborhoods but uses dilation to capture wider context, enabling hierarchical vision transformers to scale with lower compute and memory than full self-attention. This yields better integration into existing vision pipelines and improved efficiency on visual tasks [Paper 1].\\n\\n‚Ä¢ Point 2 ‚Äî Attention applied to time series can replace sequential RNNs with more interpretable, globally connected models: The Momentum Transformer provides direct access to all past timesteps, models momentum-like dynamics, and offers improved predictive performance and interpretability for trading/time-series tasks compared with LSTMs and classical strategies. This shows transformer attention‚Äôs strength at capturing long-range dependencies in non‚Äëvision modalities [Paper 2].\\n\\n‚Ä¢ Point 3 ‚Äî Removing mask-guided cross-attention can speed convergence in instance-segmentation transformers: The Mask-Attention-Free Transformer for 3D instance segmentation identifies slow convergence caused by low-recall initial masks and replaces mask-attention pipelines with alternative query refinement, simplifying the architecture and accelerating training while maintaining competitive instance segmentation performance [Paper 3].\\n\\n‚Ä¢ Point 4 ‚Äî Transformer design gains from hardware- and data-aware architectural thinking: Analyses of graph neural network architectures highlight how irregular data access and algorithm structure interact with system design, and proposals like Dalorex advocate data-local execution to mitigate memory-bound bottlenecks. Together these works emphasize that scaling transformer variants (and related graph/attention models) requires co-design of algorithmic attention patterns and memory/compute architectures to realize practical efficiency gains [Paper 4, Paper 5].\\n\\n## R√©f√©rences\\n[Paper 1] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3\\n\\n[Paper 2] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3\\n\\n[Paper 3] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1\\n\\n[Paper 4] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2\\n\\n[Paper 5] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', additional_kwargs={}, response_metadata={}, name='Summarizer')], 'refined_query': 'transformer architecture innovations attention variants positional encodings', 'iteration': 1}, next=('clarifier',), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-21fc-6e00-8005-0f4a73429923'}}, metadata={'source': 'loop', 'step': 5, 'parents': {}}, created_at='2025-11-16T11:13:46.986231+00:00', parent_config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-21fb-6744-8004-d5365f7b58ee'}}, tasks=(PregelTask(id='e17cc80d-50a7-51f6-c02c-c99cc9f60461', name='clarifier', path=('__pregel_pull', 'clarifier'), error=None, interrupts=(), state=None, result={'refined_query': 'dilated neighborhood attention hierarchical vision transformers efficiency', 'iteration': 0, 'messages': [HumanMessage(content='Can you give more detail regarding your first point?', additional_kwargs={}, response_metadata={}, name='User'), AIMessage(content='Refined query: dilated neighborhood attention hierarchical vision transformers efficiency', additional_kwargs={}, response_metadata={}, name='Clarifier')]}),), interrupts=()),\n",
       " StateSnapshot(values={'query': 'What are the key innovations in transformer architectures?', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2, 'summary': '## R√©sum√©\\n‚Ä¢ Point 1 ‚Äî Localized and dilated attention improves vision transformers by keeping attention computation tractable while enlarging receptive fields: Dilated Neighborhood Attention restricts attention to local neighborhoods but uses dilation to capture wider context, enabling hierarchical vision transformers to scale with lower compute and memory than full self-attention. This yields better integration into existing vision pipelines and improved efficiency on visual tasks [Paper 1].\\n\\n‚Ä¢ Point 2 ‚Äî Attention applied to time series can replace sequential RNNs with more interpretable, globally connected models: The Momentum Transformer provides direct access to all past timesteps, models momentum-like dynamics, and offers improved predictive performance and interpretability for trading/time-series tasks compared with LSTMs and classical strategies. This shows transformer attention‚Äôs strength at capturing long-range dependencies in non‚Äëvision modalities [Paper 2].\\n\\n‚Ä¢ Point 3 ‚Äî Removing mask-guided cross-attention can speed convergence in instance-segmentation transformers: The Mask-Attention-Free Transformer for 3D instance segmentation identifies slow convergence caused by low-recall initial masks and replaces mask-attention pipelines with alternative query refinement, simplifying the architecture and accelerating training while maintaining competitive instance segmentation performance [Paper 3].\\n\\n‚Ä¢ Point 4 ‚Äî Transformer design gains from hardware- and data-aware architectural thinking: Analyses of graph neural network architectures highlight how irregular data access and algorithm structure interact with system design, and proposals like Dalorex advocate data-local execution to mitigate memory-bound bottlenecks. Together these works emphasize that scaling transformer variants (and related graph/attention models) requires co-design of algorithmic attention patterns and memory/compute architectures to realize practical efficiency gains [Paper 4, Paper 5].\\n\\n## R√©f√©rences\\n[Paper 1] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3\\n\\n[Paper 2] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3\\n\\n[Paper 3] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1\\n\\n[Paper 4] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2\\n\\n[Paper 5] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', 'papers': [{'id': 'http://arxiv.org/abs/2209.15001v3', 'title': 'Dilated Neighborhood Attention Transformer', 'authors': ['Ali Hassani', 'Humphrey Shi'], 'summary': \"Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).\", 'url': 'http://arxiv.org/abs/2209.15001v3', 'published': '2022-09-29', 'relevance_score': 60}, {'id': 'http://arxiv.org/abs/2112.08534v3', 'title': 'Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture', 'authors': ['Kieran Wood', 'Sven Giegerich', 'Stephen Roberts', 'Stefan Zohren'], 'summary': 'We introduce the Momentum Transformer, an attention-based deep-learning architecture, which outperforms benchmark time-series momentum and mean-reversion trading strategies. Unlike state-of-the-art Long Short-Term Memory (LSTM) architectures, which are sequential in nature and tailored to local processing, an attention mechanism provides our architecture with a direct connection to all previous time-steps. Our architecture, an attention-LSTM hybrid, enables us to learn longer-term dependencies, improves performance when considering returns net of transaction costs and naturally adapts to new market regimes, such as during the SARS-CoV-2 crisis. Via the introduction of multiple attention heads, we can capture concurrent regimes, or temporal dynamics, which are occurring at different timescales. The Momentum Transformer is inherently interpretable, providing us with greater insights into our deep-learning momentum trading strategy, including the importance of different factors over time and the past time-steps which are of the greatest significance to the model.', 'url': 'http://arxiv.org/abs/2112.08534v3', 'published': '2021-12-16', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2309.01692v1', 'title': 'Mask-Attention-Free Transformer for 3D Instance Segmentation', 'authors': ['Xin Lai', 'Yuhui Yuan', 'Ruihang Chu', 'Yukang Chen', 'Han Hu', 'Jiaya Jia'], 'summary': 'Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.', 'url': 'http://arxiv.org/abs/2309.01692v1', 'published': '2023-09-04', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2009.00804v2', 'title': 'Architectural Implications of Graph Neural Networks', 'authors': ['Zhihui Zhang', 'Jingwen Leng', 'Lingxiao Ma', 'Youshan Miao', 'Chao Li', 'Minyi Guo'], 'summary': 'Graph neural networks (GNN) represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This work tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.', 'url': 'http://arxiv.org/abs/2009.00804v2', 'published': '2020-09-02', 'relevance_score': 5}, {'id': 'http://arxiv.org/abs/2207.13219v4', 'title': 'Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications', 'authors': ['Marcelo Orenes-Vera', 'Esin Tureci', 'David Wentzlaff', 'Margaret Martonosi'], 'summary': 'Applications with low data reuse and frequent irregular memory accesses, such as graph or sparse linear algebra workloads, fail to scale well due to memory bottlenecks and poor core utilization. While prior work with prefetching, decoupling, or pipelining can mitigate memory latency and improve core utilization, memory bottlenecks persist due to limited off-chip bandwidth. Approaches doing processing in-memory (PIM) with Hybrid Memory Cube (HMC) overcome bandwidth limitations but fail to achieve high core utilization due to poor task scheduling and synchronization overheads. Moreover, the high memory-per-core ratio available with HMC limits strong scaling.\\n  We introduce Dalorex, a hardware-software co-design that achieves high parallelism and energy efficiency, demonstrating strong scaling with >16,000 cores when processing graph and sparse linear algebra workloads. Over the prior work in PIM, both using 256 cores, Dalorex improves performance and energy consumption by two orders of magnitude through (1) a tile-based distributed-memory architecture where each processing tile holds an equal amount of data, and all memory operations are local; (2) a task-based parallel programming model where tasks are executed by the processing unit that is co-located with the target data; (3) a network design optimized for irregular traffic, where all communication is one-way, and messages do not contain routing metadata; (4) novel traffic-aware task scheduling hardware that maintains high core utilization; and (5) a data placement strategy that improves work balance.\\n  This work proposes architectural and software innovations to provide the greatest scalability to date for running graph algorithms while still being programmable for other domains.', 'url': 'http://arxiv.org/abs/2207.13219v4', 'published': '2022-07-26', 'relevance_score': 2}], 'messages': [SystemMessage(content='User inquired about the key innovations in transformer architectures, which suggests an interest in advancements in this field.', additional_kwargs={}, response_metadata={}), AIMessage(content='Refined query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Clarifier'), AIMessage(content='Found 5 papers on ArXiv for query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Researcher'), AIMessage(content='## R√©sum√©\\n‚Ä¢ Point 1 ‚Äî Localized and dilated attention improves vision transformers by keeping attention computation tractable while enlarging receptive fields: Dilated Neighborhood Attention restricts attention to local neighborhoods but uses dilation to capture wider context, enabling hierarchical vision transformers to scale with lower compute and memory than full self-attention. This yields better integration into existing vision pipelines and improved efficiency on visual tasks [Paper 1].\\n\\n‚Ä¢ Point 2 ‚Äî Attention applied to time series can replace sequential RNNs with more interpretable, globally connected models: The Momentum Transformer provides direct access to all past timesteps, models momentum-like dynamics, and offers improved predictive performance and interpretability for trading/time-series tasks compared with LSTMs and classical strategies. This shows transformer attention‚Äôs strength at capturing long-range dependencies in non‚Äëvision modalities [Paper 2].\\n\\n‚Ä¢ Point 3 ‚Äî Removing mask-guided cross-attention can speed convergence in instance-segmentation transformers: The Mask-Attention-Free Transformer for 3D instance segmentation identifies slow convergence caused by low-recall initial masks and replaces mask-attention pipelines with alternative query refinement, simplifying the architecture and accelerating training while maintaining competitive instance segmentation performance [Paper 3].\\n\\n‚Ä¢ Point 4 ‚Äî Transformer design gains from hardware- and data-aware architectural thinking: Analyses of graph neural network architectures highlight how irregular data access and algorithm structure interact with system design, and proposals like Dalorex advocate data-local execution to mitigate memory-bound bottlenecks. Together these works emphasize that scaling transformer variants (and related graph/attention models) requires co-design of algorithmic attention patterns and memory/compute architectures to realize practical efficiency gains [Paper 4, Paper 5].\\n\\n## R√©f√©rences\\n[Paper 1] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3\\n\\n[Paper 2] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3\\n\\n[Paper 3] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1\\n\\n[Paper 4] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2\\n\\n[Paper 5] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', additional_kwargs={}, response_metadata={}, name='Summarizer')], 'refined_query': 'transformer architecture innovations attention variants positional encodings', 'iteration': 1}, next=('__start__',), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-21fb-6744-8004-d5365f7b58ee'}}, metadata={'source': 'input', 'step': 4, 'parents': {}}, created_at='2025-11-16T11:13:46.985642+00:00', parent_config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-2105-6236-8003-d1c79acc7f70'}}, tasks=(PregelTask(id='c5396617-781b-80e4-1b45-d10f339f12ec', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'query': 'Can you give more detail regarding your first point?'}),), interrupts=()),\n",
       " StateSnapshot(values={'query': 'What are the key innovations in transformer architectures?', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2, 'summary': '## R√©sum√©\\n‚Ä¢ Point 1 ‚Äî Localized and dilated attention improves vision transformers by keeping attention computation tractable while enlarging receptive fields: Dilated Neighborhood Attention restricts attention to local neighborhoods but uses dilation to capture wider context, enabling hierarchical vision transformers to scale with lower compute and memory than full self-attention. This yields better integration into existing vision pipelines and improved efficiency on visual tasks [Paper 1].\\n\\n‚Ä¢ Point 2 ‚Äî Attention applied to time series can replace sequential RNNs with more interpretable, globally connected models: The Momentum Transformer provides direct access to all past timesteps, models momentum-like dynamics, and offers improved predictive performance and interpretability for trading/time-series tasks compared with LSTMs and classical strategies. This shows transformer attention‚Äôs strength at capturing long-range dependencies in non‚Äëvision modalities [Paper 2].\\n\\n‚Ä¢ Point 3 ‚Äî Removing mask-guided cross-attention can speed convergence in instance-segmentation transformers: The Mask-Attention-Free Transformer for 3D instance segmentation identifies slow convergence caused by low-recall initial masks and replaces mask-attention pipelines with alternative query refinement, simplifying the architecture and accelerating training while maintaining competitive instance segmentation performance [Paper 3].\\n\\n‚Ä¢ Point 4 ‚Äî Transformer design gains from hardware- and data-aware architectural thinking: Analyses of graph neural network architectures highlight how irregular data access and algorithm structure interact with system design, and proposals like Dalorex advocate data-local execution to mitigate memory-bound bottlenecks. Together these works emphasize that scaling transformer variants (and related graph/attention models) requires co-design of algorithmic attention patterns and memory/compute architectures to realize practical efficiency gains [Paper 4, Paper 5].\\n\\n## R√©f√©rences\\n[Paper 1] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3\\n\\n[Paper 2] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3\\n\\n[Paper 3] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1\\n\\n[Paper 4] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2\\n\\n[Paper 5] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', 'papers': [{'id': 'http://arxiv.org/abs/2209.15001v3', 'title': 'Dilated Neighborhood Attention Transformer', 'authors': ['Ali Hassani', 'Humphrey Shi'], 'summary': \"Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).\", 'url': 'http://arxiv.org/abs/2209.15001v3', 'published': '2022-09-29', 'relevance_score': 60}, {'id': 'http://arxiv.org/abs/2112.08534v3', 'title': 'Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture', 'authors': ['Kieran Wood', 'Sven Giegerich', 'Stephen Roberts', 'Stefan Zohren'], 'summary': 'We introduce the Momentum Transformer, an attention-based deep-learning architecture, which outperforms benchmark time-series momentum and mean-reversion trading strategies. Unlike state-of-the-art Long Short-Term Memory (LSTM) architectures, which are sequential in nature and tailored to local processing, an attention mechanism provides our architecture with a direct connection to all previous time-steps. Our architecture, an attention-LSTM hybrid, enables us to learn longer-term dependencies, improves performance when considering returns net of transaction costs and naturally adapts to new market regimes, such as during the SARS-CoV-2 crisis. Via the introduction of multiple attention heads, we can capture concurrent regimes, or temporal dynamics, which are occurring at different timescales. The Momentum Transformer is inherently interpretable, providing us with greater insights into our deep-learning momentum trading strategy, including the importance of different factors over time and the past time-steps which are of the greatest significance to the model.', 'url': 'http://arxiv.org/abs/2112.08534v3', 'published': '2021-12-16', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2309.01692v1', 'title': 'Mask-Attention-Free Transformer for 3D Instance Segmentation', 'authors': ['Xin Lai', 'Yuhui Yuan', 'Ruihang Chu', 'Yukang Chen', 'Han Hu', 'Jiaya Jia'], 'summary': 'Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.', 'url': 'http://arxiv.org/abs/2309.01692v1', 'published': '2023-09-04', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2009.00804v2', 'title': 'Architectural Implications of Graph Neural Networks', 'authors': ['Zhihui Zhang', 'Jingwen Leng', 'Lingxiao Ma', 'Youshan Miao', 'Chao Li', 'Minyi Guo'], 'summary': 'Graph neural networks (GNN) represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This work tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.', 'url': 'http://arxiv.org/abs/2009.00804v2', 'published': '2020-09-02', 'relevance_score': 5}, {'id': 'http://arxiv.org/abs/2207.13219v4', 'title': 'Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications', 'authors': ['Marcelo Orenes-Vera', 'Esin Tureci', 'David Wentzlaff', 'Margaret Martonosi'], 'summary': 'Applications with low data reuse and frequent irregular memory accesses, such as graph or sparse linear algebra workloads, fail to scale well due to memory bottlenecks and poor core utilization. While prior work with prefetching, decoupling, or pipelining can mitigate memory latency and improve core utilization, memory bottlenecks persist due to limited off-chip bandwidth. Approaches doing processing in-memory (PIM) with Hybrid Memory Cube (HMC) overcome bandwidth limitations but fail to achieve high core utilization due to poor task scheduling and synchronization overheads. Moreover, the high memory-per-core ratio available with HMC limits strong scaling.\\n  We introduce Dalorex, a hardware-software co-design that achieves high parallelism and energy efficiency, demonstrating strong scaling with >16,000 cores when processing graph and sparse linear algebra workloads. Over the prior work in PIM, both using 256 cores, Dalorex improves performance and energy consumption by two orders of magnitude through (1) a tile-based distributed-memory architecture where each processing tile holds an equal amount of data, and all memory operations are local; (2) a task-based parallel programming model where tasks are executed by the processing unit that is co-located with the target data; (3) a network design optimized for irregular traffic, where all communication is one-way, and messages do not contain routing metadata; (4) novel traffic-aware task scheduling hardware that maintains high core utilization; and (5) a data placement strategy that improves work balance.\\n  This work proposes architectural and software innovations to provide the greatest scalability to date for running graph algorithms while still being programmable for other domains.', 'url': 'http://arxiv.org/abs/2207.13219v4', 'published': '2022-07-26', 'relevance_score': 2}], 'messages': [SystemMessage(content='User inquired about the key innovations in transformer architectures, which suggests an interest in advancements in this field.', additional_kwargs={}, response_metadata={}), AIMessage(content='Refined query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Clarifier'), AIMessage(content='Found 5 papers on ArXiv for query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Researcher'), AIMessage(content='## R√©sum√©\\n‚Ä¢ Point 1 ‚Äî Localized and dilated attention improves vision transformers by keeping attention computation tractable while enlarging receptive fields: Dilated Neighborhood Attention restricts attention to local neighborhoods but uses dilation to capture wider context, enabling hierarchical vision transformers to scale with lower compute and memory than full self-attention. This yields better integration into existing vision pipelines and improved efficiency on visual tasks [Paper 1].\\n\\n‚Ä¢ Point 2 ‚Äî Attention applied to time series can replace sequential RNNs with more interpretable, globally connected models: The Momentum Transformer provides direct access to all past timesteps, models momentum-like dynamics, and offers improved predictive performance and interpretability for trading/time-series tasks compared with LSTMs and classical strategies. This shows transformer attention‚Äôs strength at capturing long-range dependencies in non‚Äëvision modalities [Paper 2].\\n\\n‚Ä¢ Point 3 ‚Äî Removing mask-guided cross-attention can speed convergence in instance-segmentation transformers: The Mask-Attention-Free Transformer for 3D instance segmentation identifies slow convergence caused by low-recall initial masks and replaces mask-attention pipelines with alternative query refinement, simplifying the architecture and accelerating training while maintaining competitive instance segmentation performance [Paper 3].\\n\\n‚Ä¢ Point 4 ‚Äî Transformer design gains from hardware- and data-aware architectural thinking: Analyses of graph neural network architectures highlight how irregular data access and algorithm structure interact with system design, and proposals like Dalorex advocate data-local execution to mitigate memory-bound bottlenecks. Together these works emphasize that scaling transformer variants (and related graph/attention models) requires co-design of algorithmic attention patterns and memory/compute architectures to realize practical efficiency gains [Paper 4, Paper 5].\\n\\n## R√©f√©rences\\n[Paper 1] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3\\n\\n[Paper 2] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3\\n\\n[Paper 3] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1\\n\\n[Paper 4] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2\\n\\n[Paper 5] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', additional_kwargs={}, response_metadata={}, name='Summarizer')], 'refined_query': 'transformer architecture innovations attention variants positional encodings', 'iteration': 1}, next=(), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-2105-6236-8003-d1c79acc7f70'}}, metadata={'source': 'loop', 'step': 3, 'parents': {}}, created_at='2025-11-16T11:13:46.884757+00:00', parent_config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-20ec-6c5e-8002-628c05ab18e2'}}, tasks=(), interrupts=()),\n",
       " StateSnapshot(values={'query': 'What are the key innovations in transformer architectures?', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2, 'papers': [{'id': 'http://arxiv.org/abs/2209.15001v3', 'title': 'Dilated Neighborhood Attention Transformer', 'authors': ['Ali Hassani', 'Humphrey Shi'], 'summary': \"Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).\", 'url': 'http://arxiv.org/abs/2209.15001v3', 'published': '2022-09-29', 'relevance_score': 60}, {'id': 'http://arxiv.org/abs/2112.08534v3', 'title': 'Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture', 'authors': ['Kieran Wood', 'Sven Giegerich', 'Stephen Roberts', 'Stefan Zohren'], 'summary': 'We introduce the Momentum Transformer, an attention-based deep-learning architecture, which outperforms benchmark time-series momentum and mean-reversion trading strategies. Unlike state-of-the-art Long Short-Term Memory (LSTM) architectures, which are sequential in nature and tailored to local processing, an attention mechanism provides our architecture with a direct connection to all previous time-steps. Our architecture, an attention-LSTM hybrid, enables us to learn longer-term dependencies, improves performance when considering returns net of transaction costs and naturally adapts to new market regimes, such as during the SARS-CoV-2 crisis. Via the introduction of multiple attention heads, we can capture concurrent regimes, or temporal dynamics, which are occurring at different timescales. The Momentum Transformer is inherently interpretable, providing us with greater insights into our deep-learning momentum trading strategy, including the importance of different factors over time and the past time-steps which are of the greatest significance to the model.', 'url': 'http://arxiv.org/abs/2112.08534v3', 'published': '2021-12-16', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2309.01692v1', 'title': 'Mask-Attention-Free Transformer for 3D Instance Segmentation', 'authors': ['Xin Lai', 'Yuhui Yuan', 'Ruihang Chu', 'Yukang Chen', 'Han Hu', 'Jiaya Jia'], 'summary': 'Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.', 'url': 'http://arxiv.org/abs/2309.01692v1', 'published': '2023-09-04', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2009.00804v2', 'title': 'Architectural Implications of Graph Neural Networks', 'authors': ['Zhihui Zhang', 'Jingwen Leng', 'Lingxiao Ma', 'Youshan Miao', 'Chao Li', 'Minyi Guo'], 'summary': 'Graph neural networks (GNN) represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This work tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.', 'url': 'http://arxiv.org/abs/2009.00804v2', 'published': '2020-09-02', 'relevance_score': 5}, {'id': 'http://arxiv.org/abs/2207.13219v4', 'title': 'Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications', 'authors': ['Marcelo Orenes-Vera', 'Esin Tureci', 'David Wentzlaff', 'Margaret Martonosi'], 'summary': 'Applications with low data reuse and frequent irregular memory accesses, such as graph or sparse linear algebra workloads, fail to scale well due to memory bottlenecks and poor core utilization. While prior work with prefetching, decoupling, or pipelining can mitigate memory latency and improve core utilization, memory bottlenecks persist due to limited off-chip bandwidth. Approaches doing processing in-memory (PIM) with Hybrid Memory Cube (HMC) overcome bandwidth limitations but fail to achieve high core utilization due to poor task scheduling and synchronization overheads. Moreover, the high memory-per-core ratio available with HMC limits strong scaling.\\n  We introduce Dalorex, a hardware-software co-design that achieves high parallelism and energy efficiency, demonstrating strong scaling with >16,000 cores when processing graph and sparse linear algebra workloads. Over the prior work in PIM, both using 256 cores, Dalorex improves performance and energy consumption by two orders of magnitude through (1) a tile-based distributed-memory architecture where each processing tile holds an equal amount of data, and all memory operations are local; (2) a task-based parallel programming model where tasks are executed by the processing unit that is co-located with the target data; (3) a network design optimized for irregular traffic, where all communication is one-way, and messages do not contain routing metadata; (4) novel traffic-aware task scheduling hardware that maintains high core utilization; and (5) a data placement strategy that improves work balance.\\n  This work proposes architectural and software innovations to provide the greatest scalability to date for running graph algorithms while still being programmable for other domains.', 'url': 'http://arxiv.org/abs/2207.13219v4', 'published': '2022-07-26', 'relevance_score': 2}], 'messages': [HumanMessage(content='What are the key innovations in transformer architectures?', additional_kwargs={}, response_metadata={}, name='User'), AIMessage(content='Refined query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Clarifier'), AIMessage(content='Found 5 papers on ArXiv for query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Researcher')], 'refined_query': 'transformer architecture innovations attention variants positional encodings', 'iteration': 1}, next=('summarizer',), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-20ec-6c5e-8002-628c05ab18e2'}}, metadata={'source': 'loop', 'step': 2, 'parents': {}}, created_at='2025-11-16T11:13:46.874776+00:00', parent_config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-1ac1-642e-8001-fda071d8f307'}}, tasks=(PregelTask(id='caf07422-e5a0-3bd6-b791-8dd4fb3cf16b', name='summarizer', path=('__pregel_pull', 'summarizer'), error=None, interrupts=(), state=None, result={'summary': '## R√©sum√©\\n‚Ä¢ Point 1 ‚Äî Localized and dilated attention improves vision transformers by keeping attention computation tractable while enlarging receptive fields: Dilated Neighborhood Attention restricts attention to local neighborhoods but uses dilation to capture wider context, enabling hierarchical vision transformers to scale with lower compute and memory than full self-attention. This yields better integration into existing vision pipelines and improved efficiency on visual tasks [Paper 1].\\n\\n‚Ä¢ Point 2 ‚Äî Attention applied to time series can replace sequential RNNs with more interpretable, globally connected models: The Momentum Transformer provides direct access to all past timesteps, models momentum-like dynamics, and offers improved predictive performance and interpretability for trading/time-series tasks compared with LSTMs and classical strategies. This shows transformer attention‚Äôs strength at capturing long-range dependencies in non‚Äëvision modalities [Paper 2].\\n\\n‚Ä¢ Point 3 ‚Äî Removing mask-guided cross-attention can speed convergence in instance-segmentation transformers: The Mask-Attention-Free Transformer for 3D instance segmentation identifies slow convergence caused by low-recall initial masks and replaces mask-attention pipelines with alternative query refinement, simplifying the architecture and accelerating training while maintaining competitive instance segmentation performance [Paper 3].\\n\\n‚Ä¢ Point 4 ‚Äî Transformer design gains from hardware- and data-aware architectural thinking: Analyses of graph neural network architectures highlight how irregular data access and algorithm structure interact with system design, and proposals like Dalorex advocate data-local execution to mitigate memory-bound bottlenecks. Together these works emphasize that scaling transformer variants (and related graph/attention models) requires co-design of algorithmic attention patterns and memory/compute architectures to realize practical efficiency gains [Paper 4, Paper 5].\\n\\n## R√©f√©rences\\n[Paper 1] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3\\n\\n[Paper 2] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3\\n\\n[Paper 3] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1\\n\\n[Paper 4] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2\\n\\n[Paper 5] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', 'messages': [AIMessage(content='## R√©sum√©\\n‚Ä¢ Point 1 ‚Äî Localized and dilated attention improves vision transformers by keeping attention computation tractable while enlarging receptive fields: Dilated Neighborhood Attention restricts attention to local neighborhoods but uses dilation to capture wider context, enabling hierarchical vision transformers to scale with lower compute and memory than full self-attention. This yields better integration into existing vision pipelines and improved efficiency on visual tasks [Paper 1].\\n\\n‚Ä¢ Point 2 ‚Äî Attention applied to time series can replace sequential RNNs with more interpretable, globally connected models: The Momentum Transformer provides direct access to all past timesteps, models momentum-like dynamics, and offers improved predictive performance and interpretability for trading/time-series tasks compared with LSTMs and classical strategies. This shows transformer attention‚Äôs strength at capturing long-range dependencies in non‚Äëvision modalities [Paper 2].\\n\\n‚Ä¢ Point 3 ‚Äî Removing mask-guided cross-attention can speed convergence in instance-segmentation transformers: The Mask-Attention-Free Transformer for 3D instance segmentation identifies slow convergence caused by low-recall initial masks and replaces mask-attention pipelines with alternative query refinement, simplifying the architecture and accelerating training while maintaining competitive instance segmentation performance [Paper 3].\\n\\n‚Ä¢ Point 4 ‚Äî Transformer design gains from hardware- and data-aware architectural thinking: Analyses of graph neural network architectures highlight how irregular data access and algorithm structure interact with system design, and proposals like Dalorex advocate data-local execution to mitigate memory-bound bottlenecks. Together these works emphasize that scaling transformer variants (and related graph/attention models) requires co-design of algorithmic attention patterns and memory/compute architectures to realize practical efficiency gains [Paper 4, Paper 5].\\n\\n## R√©f√©rences\\n[Paper 1] Dilated Neighborhood Attention Transformer - Ali Hassani, Humphrey Shi (2022) - http://arxiv.org/abs/2209.15001v3\\n\\n[Paper 2] Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture - Kieran Wood, Sven Giegerich, Stephen Roberts (2021) - http://arxiv.org/abs/2112.08534v3\\n\\n[Paper 3] Mask-Attention-Free Transformer for 3D Instance Segmentation - Xin Lai, Yuhui Yuan, Ruihang Chu (2023) - http://arxiv.org/abs/2309.01692v1\\n\\n[Paper 4] Architectural Implications of Graph Neural Networks - Zhihui Zhang, Jingwen Leng, Lingxiao Ma (2020) - http://arxiv.org/abs/2009.00804v2\\n\\n[Paper 5] Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications - Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff (2022) - http://arxiv.org/abs/2207.13219v4', additional_kwargs={}, response_metadata={}, name='Summarizer')]}),), interrupts=()),\n",
       " StateSnapshot(values={'query': 'What are the key innovations in transformer architectures?', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2, 'papers': [], 'messages': [HumanMessage(content='What are the key innovations in transformer architectures?', additional_kwargs={}, response_metadata={}, name='User'), AIMessage(content='Refined query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Clarifier')], 'refined_query': 'transformer architecture innovations attention variants positional encodings', 'iteration': 0}, next=('researcher',), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-1ac1-642e-8001-fda071d8f307'}}, metadata={'source': 'loop', 'step': 1, 'parents': {}}, created_at='2025-11-16T11:13:46.227811+00:00', parent_config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-185f-60dc-8000-bfda9b45d096'}}, tasks=(PregelTask(id='b683615e-a812-bbee-b9bb-2128e31d8b8f', name='researcher', path=('__pregel_pull', 'researcher'), error=None, interrupts=(), state=None, result={'papers': [{'id': 'http://arxiv.org/abs/2209.15001v3', 'title': 'Dilated Neighborhood Attention Transformer', 'authors': ['Ali Hassani', 'Humphrey Shi'], 'summary': \"Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).\", 'url': 'http://arxiv.org/abs/2209.15001v3', 'published': '2022-09-29', 'relevance_score': 60}, {'id': 'http://arxiv.org/abs/2112.08534v3', 'title': 'Trading with the Momentum Transformer: An Intelligent and Interpretable Architecture', 'authors': ['Kieran Wood', 'Sven Giegerich', 'Stephen Roberts', 'Stefan Zohren'], 'summary': 'We introduce the Momentum Transformer, an attention-based deep-learning architecture, which outperforms benchmark time-series momentum and mean-reversion trading strategies. Unlike state-of-the-art Long Short-Term Memory (LSTM) architectures, which are sequential in nature and tailored to local processing, an attention mechanism provides our architecture with a direct connection to all previous time-steps. Our architecture, an attention-LSTM hybrid, enables us to learn longer-term dependencies, improves performance when considering returns net of transaction costs and naturally adapts to new market regimes, such as during the SARS-CoV-2 crisis. Via the introduction of multiple attention heads, we can capture concurrent regimes, or temporal dynamics, which are occurring at different timescales. The Momentum Transformer is inherently interpretable, providing us with greater insights into our deep-learning momentum trading strategy, including the importance of different factors over time and the past time-steps which are of the greatest significance to the model.', 'url': 'http://arxiv.org/abs/2112.08534v3', 'published': '2021-12-16', 'relevance_score': 35}, {'id': 'http://arxiv.org/abs/2207.13219v4', 'title': 'Dalorex: A Data-Local Program Execution and Architecture for Memory-bound Applications', 'authors': ['Marcelo Orenes-Vera', 'Esin Tureci', 'David Wentzlaff', 'Margaret Martonosi'], 'summary': 'Applications with low data reuse and frequent irregular memory accesses, such as graph or sparse linear algebra workloads, fail to scale well due to memory bottlenecks and poor core utilization. While prior work with prefetching, decoupling, or pipelining can mitigate memory latency and improve core utilization, memory bottlenecks persist due to limited off-chip bandwidth. Approaches doing processing in-memory (PIM) with Hybrid Memory Cube (HMC) overcome bandwidth limitations but fail to achieve high core utilization due to poor task scheduling and synchronization overheads. Moreover, the high memory-per-core ratio available with HMC limits strong scaling.\\n  We introduce Dalorex, a hardware-software co-design that achieves high parallelism and energy efficiency, demonstrating strong scaling with >16,000 cores when processing graph and sparse linear algebra workloads. Over the prior work in PIM, both using 256 cores, Dalorex improves performance and energy consumption by two orders of magnitude through (1) a tile-based distributed-memory architecture where each processing tile holds an equal amount of data, and all memory operations are local; (2) a task-based parallel programming model where tasks are executed by the processing unit that is co-located with the target data; (3) a network design optimized for irregular traffic, where all communication is one-way, and messages do not contain routing metadata; (4) novel traffic-aware task scheduling hardware that maintains high core utilization; and (5) a data placement strategy that improves work balance.\\n  This work proposes architectural and software innovations to provide the greatest scalability to date for running graph algorithms while still being programmable for other domains.', 'url': 'http://arxiv.org/abs/2207.13219v4', 'published': '2022-07-26', 'relevance_score': 2}, {'id': 'http://arxiv.org/abs/2009.00804v2', 'title': 'Architectural Implications of Graph Neural Networks', 'authors': ['Zhihui Zhang', 'Jingwen Leng', 'Lingxiao Ma', 'Youshan Miao', 'Chao Li', 'Minyi Guo'], 'summary': 'Graph neural networks (GNN) represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This work tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.', 'url': 'http://arxiv.org/abs/2009.00804v2', 'published': '2020-09-02', 'relevance_score': 5}, {'id': 'http://arxiv.org/abs/2309.01692v1', 'title': 'Mask-Attention-Free Transformer for 3D Instance Segmentation', 'authors': ['Xin Lai', 'Yuhui Yuan', 'Ruihang Chu', 'Yukang Chen', 'Han Hu', 'Jiaya Jia'], 'summary': 'Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.', 'url': 'http://arxiv.org/abs/2309.01692v1', 'published': '2023-09-04', 'relevance_score': 35}], 'iteration': 1, 'messages': [AIMessage(content='Found 5 papers on ArXiv for query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Researcher')]}),), interrupts=()),\n",
       " StateSnapshot(values={'query': 'What are the key innovations in transformer architectures?', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2, 'papers': [], 'messages': []}, next=('clarifier',), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-185f-60dc-8000-bfda9b45d096'}}, metadata={'source': 'loop', 'step': 0, 'parents': {}}, created_at='2025-11-16T11:13:45.977868+00:00', parent_config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-185d-62dc-bfff-3612bc542a7a'}}, tasks=(PregelTask(id='456d3520-06aa-dd5c-0fdd-f0d259415c9b', name='clarifier', path=('__pregel_pull', 'clarifier'), error=None, interrupts=(), state=None, result={'refined_query': 'transformer architecture innovations attention variants positional encodings', 'iteration': 0, 'messages': [HumanMessage(content='What are the key innovations in transformer architectures?', additional_kwargs={}, response_metadata={}, name='User'), AIMessage(content='Refined query: transformer architecture innovations attention variants positional encodings', additional_kwargs={}, response_metadata={}, name='Clarifier')]}),), interrupts=()),\n",
       " StateSnapshot(values={'papers': [], 'messages': []}, next=('__start__',), config={'configurable': {'thread_id': 'demo-thread-1', 'checkpoint_ns': '', 'checkpoint_id': '1f0c2dd5-185d-62dc-bfff-3612bc542a7a'}}, metadata={'source': 'input', 'step': -1, 'parents': {}}, created_at='2025-11-16T11:13:45.977102+00:00', parent_config=None, tasks=(PregelTask(id='b08a2abb-6684-4118-8690-5fb180a02d03', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'query': 'What are the key innovations in transformer architectures?', 'llm_model': 'gpt-5-mini', 'llm_temperature': 0, 'max_papers': 5, 'max_iterations': 2}),), interrupts=())]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(graph.get_state_history(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798810c8",
   "metadata": {},
   "source": [
    "### Test 3: Memory Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a45cc2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:13:48,898 - INFO - Clarifying query: 'Explain attention mechanisms'\n",
      "2025-11-16 12:13:48,901 - INFO - Refined query: 'Attention mechanisms survey self-attention transformers'\n",
      "2025-11-16 12:13:48,902 - INFO - Searching ArXiv: 'Attention mechanisms survey self-attention transformers' (iteration 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 max_papers\n",
      "max_results tool 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:13:49,401 - INFO - Found 5 papers\n",
      "2025-11-16 12:13:49,403 - INFO - Scoring paper relevance...\n",
      "2025-11-16 12:13:49,410 - INFO -   üìÑ Primal-Attention: Self-attention through Asymmetric Kernel S... - Score: 90\n",
      "2025-11-16 12:13:49,411 - INFO -   üìÑ Toward Interpretable Music Tagging with Self-Attention... - Score: 60\n",
      "2025-11-16 12:13:49,413 - INFO -   üìÑ Attention Guided CAM: Visual Explanations of Vision Transfor... - Score: 40\n",
      "2025-11-16 12:13:49,415 - INFO -   üìÑ Beyond Self-attention: External Attention using Two Linear L... - Score: 90\n",
      "2025-11-16 12:13:49,417 - INFO -   üìÑ D√©j√† vu: A Contextualized Temporal Attention Mechanism for S... - Score: 35\n",
      "2025-11-16 12:13:49,419 - INFO - üìù Synthesizing 5 papers...\n",
      "2025-11-16 12:13:49,421 - INFO - ‚úÖ Summary generated\n",
      "2025-11-16 12:13:49,428 - INFO - Clarifying query: 'What about self-attention?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ First query completed - 4 messages in memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:13:53,453 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-16 12:13:53,485 - INFO - Refined query: 'self-attention transformers survey theory efficiency'\n",
      "2025-11-16 12:13:53,491 - INFO - Searching ArXiv: 'self-attention transformers survey theory efficiency' (iteration 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 max_papers\n",
      "max_results tool 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:13:54,293 - INFO - Found 5 papers\n",
      "2025-11-16 12:13:54,296 - INFO - Scoring paper relevance...\n",
      "2025-11-16 12:13:54,304 - INFO -   üìÑ Attention Guided CAM: Visual Explanations of Vision Transfor... - Score: 90\n",
      "2025-11-16 12:13:54,308 - INFO -   üìÑ Primal-Attention: Self-attention through Asymmetric Kernel S... - Score: 95\n",
      "2025-11-16 12:13:54,311 - INFO -   üìÑ Toward Interpretable Music Tagging with Self-Attention... - Score: 80\n",
      "2025-11-16 12:13:56,002 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-16 12:13:56,011 - INFO -   üìÑ A rich bounty of AGN in the 9 square degree Bootes survey: h... - Score: 1\n",
      "2025-11-16 12:13:58,501 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-16 12:13:58,513 - INFO -   üìÑ Variational Approach to Quantum Field Theory: Gaussian Appro... - Score: 1\n",
      "2025-11-16 12:13:58,521 - INFO - üìù Synthesizing 7 papers...\n",
      "2025-11-16 12:14:23,091 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-16 12:14:23,113 - INFO - ‚úÖ Summary generated\n",
      "2025-11-16 12:14:24,655 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Second query completed - 4 messages in memory\n",
      "\n",
      "üìú Message History:\n",
      "  1. [SystemMessage] None: The conversation highlighted the role of self-attention in neural networks, which allows models to w...\n",
      "  2. [AIMessage] Clarifier: Refined query: self-attention transformers survey theory efficiency\n",
      "  3. [AIMessage] Researcher: Found 5 papers on ArXiv for query: self-attention transformers survey theory efficiency...\n",
      "  4. [AIMessage] Summarizer: ## R√©sum√©\n",
      "‚Ä¢ Self-attention can be cast and optimized as an asymmetric kernel machine: Primal-Attenti...\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"demo-thread-3\"}}\n",
    "\n",
    "# First query\n",
    "question1 = \"Explain attention mechanisms\"\n",
    "result1 = graph.invoke({\n",
    "    \"query\": question1,\n",
    "    \"llm_model\": LLM_MODEL,\n",
    "    \"llm_temperature\": 0,\n",
    "    \"max_papers\": 5,\n",
    "    \"max_iterations\": 2,\n",
    "}, config=config)\n",
    "\n",
    "print(f\"‚úÖ First query completed - {len(result1['messages'])} messages in memory\")\n",
    "\n",
    "# Second query (same thread, memory persists automatically via checkpointer)\n",
    "question2 = \"What about self-attention?\"\n",
    "result2 = graph.invoke({\n",
    "    \"query\": question2,\n",
    "    \"llm_model\": LLM_MODEL,\n",
    "    \"llm_temperature\": 0,\n",
    "    \"max_papers\": 5,\n",
    "    \"max_iterations\": 2,\n",
    "}, config=config)\n",
    "\n",
    "print(f\"‚úÖ Second query completed - {len(result2['messages'])} messages in memory\")\n",
    "\n",
    "# Display message history\n",
    "print(\"\\nüìú Message History:\")\n",
    "for i, msg in enumerate(result2['messages'][-5:], 1):  # Show last 5 messages\n",
    "    msg_type = msg.__class__.__name__\n",
    "    name = getattr(msg, 'name', 'Unknown')\n",
    "    content_preview = msg.content[:100] + \"...\" if len(msg.content) > 80 else msg.content\n",
    "    print(f\"  {i}. [{msg_type}] {name}: {content_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d901c",
   "metadata": {},
   "source": [
    "### Test 4: Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c046b206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:14:35,936 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Do you mean a convolutional neural network (CNN) used in machine learning, or CNN as in the news net...\n",
      "‚è±Ô∏è  Time: 11.225s\n",
      "\n",
      "Second call (cache HIT):\n",
      "Response: Do you mean a convolutional neural network (CNN) used in machine learning, or CNN as in the news net...\n",
      "‚è±Ô∏è  Time: 0.002s\n",
      "\n",
      "‚úÖ Cache working! Speedup: 7218.8x faster\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.cache import InMemoryCache\n",
    "import time\n",
    "\n",
    "from langchain_core.globals import set_llm_cache\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "llm = ChatOpenAI(model=LLM_MODEL, temperature=0)\n",
    "\n",
    "question = \"What is a CNN?\"\n",
    "\n",
    "# First call - hits the LLM (slow)\n",
    "print(\"First call:\")\n",
    "start = time.time()\n",
    "response1 = llm.invoke(question)\n",
    "time1 = time.time() - start\n",
    "print(f\"Response: {response1.content[:100]}...\")\n",
    "print(f\"‚è±Ô∏è  Time: {time1:.3f}s\\n\")\n",
    "\n",
    "# Second call - returns from cache (fast!)\n",
    "print(\"Second call (cache HIT):\")\n",
    "start = time.time()\n",
    "response2 = llm.invoke(question)\n",
    "time2 = time.time() - start\n",
    "print(f\"Response: {response2.content[:100]}...\")\n",
    "print(f\"‚è±Ô∏è  Time: {time2:.3f}s\\n\")\n",
    "\n",
    "# Compare\n",
    "if time2 < time1 * 0.1:  # Cache should be 10x+ faster\n",
    "    print(f\"‚úÖ Cache working! Speedup: {time1/time2:.1f}x faster\")\n",
    "else:\n",
    "    print(f\"‚ùå Cache might not be working\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d547b2a3",
   "metadata": {},
   "source": [
    "## View Results in LangSmith Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f8c1c2",
   "metadata": {},
   "source": [
    "### Option 1: View Past Runs (Tracing)\n",
    "\n",
    "If you have `LANGCHAIN_TRACING_V2=true` in your `.env`:\n",
    "1. Go to https://smith.langchain.com\n",
    "2. Navigate to your project (e.g., \"scientific-graph-agent\")\n",
    "3. Click on any run to see:\n",
    "   - Full execution trace\n",
    "   - Each node's input/output\n",
    "   - LLM calls and tokens used\n",
    "   - Execution time per node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b2471",
   "metadata": {},
   "source": [
    "### Option 2: Interactive Studio (Live Development)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1156049a",
   "metadata": {},
   "source": [
    "To interact with your graph in real-time:\n",
    "\n",
    "```bash\n",
    "# In your terminal, run:\n",
    "langgraph dev\n",
    "\n",
    "# This will open in browser:\n",
    "# https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "```\n",
    "\n",
    "The Studio provides:\n",
    "- **Visual graph editor** - See your nodes and edges\n",
    "- **Interactive chat** - Test queries in real-time\n",
    "- **State inspector** - View state after each node\n",
    "- **Thread history** - Browse past conversations by thread_id\n",
    "- **Debugger** - Step through execution node by node\n",
    "\n",
    "**Quick Setup for Studio:**\n",
    "\n",
    "1. Ensure `langgraph.json` exists in project root\n",
    "2. Start the server: `langgraph dev`\n",
    "3. Open https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "4. Click \"New Thread\" and type your question\n",
    "5. Watch the graph execute in real-time!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scientific-graph-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
